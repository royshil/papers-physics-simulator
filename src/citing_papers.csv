paperId,name,year,num_citations
c5c2870e05eae948fc58287447def73b6b192332,A Neural Scaling Law from Lottery Ticket Ensembling,2023.0,0
8e7254ab110cbf374d1c23f9bbce022ba9d01f1c,D3: Data Diversity Design for Systematic Generalization in Visual Question Answering,2023.0,0
f9ab990ca3c0715e31854ec1087af572af8de8a6,Pretraining on the Test Set Is All You Need,2023.0,2
62d15b5a87961fd1a2b941e9fa83646db3324e2c,No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets,2023.0,0
6cd6118da9bbaa50c673118ded401ad506bb0f7c,LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework,2023.0,0
4c9eb09bfe9524574c0d4cd9614789f25f533623,CodeCoT and Beyond: Learning to Program and Test like a Developer,2023.0,0
e9724e31b870b6e333f657a7b381325918e31ddc,Scaling Laws for Imitation Learning in NetHack,2023.0,0
d96454e89f1228bc7cdbaad211f3779574aa2b2c,The semantic landscape paradigm for neural networks,2023.0,0
df2666bde176803c05c2085689b0b3ea3aa058fd,Reduce the Handicap: Performance Estimation for AI Systems Safety Certification,2023.0,0
225a242405d1629b18b7c4367a3101509c9274bb,Scaling Laws Do Not Scale,2023.0,0
72833a1a9a154b9f5b670f9940586ae03d5598fb,Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data,2023.0,2
1205d6406577389f583247b2eae4abae6215656d,Delegated Classification,2023.0,1
5db2dc69feb3d6d668151d88016f718b3f5afe5a,Leveraging human expert image annotations to improve pneumonia differentiation through human knowledge distillation,2023.0,0
3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6,SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models,2023.0,3
29c7f009df21d0112c48dec254ff80cc45fac3af,Are Emergent Abilities of Large Language Models a Mirage?,2023.0,56
3ec5f0da304a606c5989de5b00e1246ee64b3e46,kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,2023.0,13
a1058448710533ef3410e146ec9db400fa4a329d,The Quantization Model of Neural Scaling,2023.0,12
2debe7236a3792db6cfea091db8021fbddbcd062,SemDeDup: Data-efficient learning at web-scale through semantic deduplication,2023.0,19
ff4ac6c65100eb19024e187b3b6cb743a117fd60,Kernel Regression with Infinite-Width Neural Networks on Millions of Examples,2023.0,4
33a12f5c67a312adba869bad25ab1df2582c1899,Robust mmWave Beamforming by Self-Supervised Hybrid Deep Learning,2023.0,0
dab6c070ea92e33e4d70b38923a301b05052a968,A Meta-Learning Approach to Predicting Performance and Data Requirements,2023.0,0
a7a40b35b6f37c554f1c5c2038892ed70c693a64,Learning to Grow Pretrained Models for Efficient Transformer Training,2023.0,9
57e849d0de13ed5f91d086936296721d4ff75a75,LLaMA: Open and Efficient Foundation Language Models,2023.0,1715
7af28fd91d91441ebbd029c002cb58d7de286210,Scaling Laws for Multilingual Neural Machine Translation,2023.0,7
7d0221f3d8282a84375790768aafafb63a69cbf5,Data pruning and neural scaling laws: fundamental limitations of score-based algorithms,2023.0,2
b60e04fa6c497ea8fae3ff63e34a107054756442,Cliff-Learning,2023.0,0
5445e246bb2ea0d71026f299604afe4cffa39a0d,Evaluating Self-Supervised Learning via Risk Decomposition,2023.0,1
9752ee1b191b8e5f69f667ae729c8dd32c6b9535,Power Laws for Hyperparameter Optimization,2023.0,0
53535d38fe259a3aa7c911edd8048d764e09e8e1,The case for 4-bit precision: k-bit Inference Scaling Laws,2022.0,39
f5b3cb14e0947c62b470d2072483481f14258738,A Solvable Model of Neural Scaling Laws,2022.0,17
4b8ae99910c2a0226e01a6199da8e5fb56ee1e2a,Broken Neural Scaling Laws,2022.0,25
4129fb99f5c8785452d60ddaf29d178525e29d0a,Measures of Information Reflect Memorization Patterns,2022.0,2
b2f60d9aecb1e52c32e5ee2a0efa52c4443f2e03,Active Learning from the Web,2022.0,0
6bd90b6343367806abab262256f1b2eeccdeebf3,Data Budgeting for Machine Learning,2022.0,1
9c4a22ef4e6a8e365a7edf2d3d7a0e34a28585ab,Optimizing Data Collection for Machine Learning,2022.0,8
b39d803f9e5f816744f10425fb9419828d03c301,Scaling Laws for a Multi-Agent Reinforcement Learning Model,2022.0,10
f94aa0d4c5c1dee834d6da0365bf0acf12f18850,Scaling Laws For Deep Learning Based Image Reconstruction,2022.0,8
99934ef57a75f49afe68736cbc7dbf480687b552,Efficient Quantized Sparse Matrix Operations on Tensor Cores,2022.0,4
fb9f9e98d35340875905730e1a80221fec818944,Revisiting Neural Scaling Laws in Language and Vision,2022.0,22
fbad88bb09b45ba78d3b3df644c639f21760b20a,Impact of dataset size and long-term ECoG-based BCI usage on deep learning decoders performance,2022.0,0
45122c8f76a4e2fd0163d1f0522db37e97ea4721,Beyond neural scaling laws: beating power law scaling via data pruning,2022.0,126
5f709f6279b9eb0edfee203413d5b30b5f65c818,Limitations of the NTK for Understanding Generalization in Deep Learning,2022.0,20
8215f4e7dc7ea6f588bcbc9b0f4383672545594f,On Data Scaling in Masked Image Modeling,2022.0,20
d3403aa9b57f69f85a61a84a83c0c5f2f284c97e,How Much More Data Do I Need? Estimating Requirements for Downstream Tasks,2022.0,8
497d5e7861e5e89ab599f8936ccdae10162776d8,"Evaluating the Diversity, Equity, and Inclusion of NLP Technology: A Case Study for Indian Languages",2022.0,0
6e10343767ab09dde83cf99ea3442907402a9810,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,2022.0,26
8b293973061026d9d0eed90e71e30928e029171e,Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models,2022.0,63
0c2e2769cd4d34bd459fe6129daaaf4755af9987,Investigating classification learning curves for automatically generated and labelled plant images,2022.0,0
5032e61905f744bc753ab206ea9cbedf44bb4efa,Data augmentation and multimodal learning for predicting drug response in patient-derived xenografts from gene expressions and histology images,2022.0,3
0286b2736a114198b25fb5553c671c33aed5d477,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,2022.0,345
0b0d7d87c58d41b92d907347b778032be5966f60,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,2022.0,75
04ff95e0edc3759fc5d18a1b929b3ccf79b032b2,Deconstructing Distributions: A Pointwise Framework of Learning,2022.0,11
9cbc044e315cdefe9a255119037ac7c23e9abdd5,Predictability and Surprise in Large Generative Models,2022.0,87
e404bdfaa858b3c25540aa5d2c5dfe20c16ead37,Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments,2022.0,6
927a5203363fc9c8ba48599dc749cf0cc647444b,Compute Trends Across Three Eras of Machine Learning,2022.0,95
37ddb9305c8c9120c21a2fae5a851ce8e4384a9c,Data Scaling Laws in NMT: The Effect of Noise and Architecture,2022.0,21
c2536182c010c41941e8a031071a1880c34cec60,Unified Scaling Laws for Routed Language Models,2022.0,64
3cee0b563cd8872378eae52c55acbef896e56b76,Error scaling laws for kernel classification under source and capacity conditions,2022.0,6
ff3e25bfe48d795b4c563b5f64359a1343a29661,Auto-Compressing Subset Pruning for Semantic Image Segmentation,2022.0,1
3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e,A General Language Assistant as a Laboratory for Alignment,2021.0,176
2ca09ee92154b4480230654989a3ecec6b29ef10,Turing-Universal Learners with Optimal Scaling Laws,2021.0,1
ccbd1c215944aecc8da2984e36619939c6bf8096,Is the Number of Trainable Parameters All That Actually Matters?,2021.0,2
05f5cf4ee6cdfde52a6632a642090dfb1ad17836,Deep Learning and Its Application to Function Approximation for MR in Medicine: An Overview,2021.0,1
de1fdaf92488f2f33ddc0272628c8543778d0da9,Scaling Laws for Neural Machine Translation,2021.0,54
b1e509bfee5f08dffc1e1b247c63a0260ed09fcc,A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?,2021.0,8
39bf40fec9bb8a68198471d86bbd8b5a763d47be,Dataset Distillation with Infinitely Wide Convolutional Networks,2021.0,91
28a54945a3f7f4e607b2c4b9a791ab508fa19f0b,Redundant representations help generalization in wide neural networks,2021.0,2
28141ca1ce5a2c536358d3934a23045b837e2d0c,Self-Supervision is All You Need for Solving Rubik's Cube,2021.0,0
2a9541011d490977204eb11217a952ae15be7193,Benchmarking down-scaled (not so large) pre-trained language models,2021.0,1
6f59cb30a6070df32c2c30888e56359ab0d94402,"Can ""Conscious Data Contribution"" Help Users to Exert ""Data Leverage"" Against Technology Companies?",2021.0,6
afbb0a31cc96c779fc363b65998a50f7b159e383,Scaling Scaling Laws with Board Games,2021.0,15
21ec9c0f869bdb33b06c7dbc8880169db0397d08,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,2021.0,89
0635138addc322a51c0a3df0ef3232f58490840d,The Shape of Learning Curves: A Review,2021.0,57
3836de2cb7300624e656c33aa1dd287fc138a69d,Is it enough to optimize CNN architectures on ImageNet?,2021.0,11
9efe8dbde586d6248ecfc69f08b918012e2ac478,Revisiting ResNets: Improved Training and Scaling Strategies,2021.0,191
6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b,Explaining Neural Scaling Laws,2021.0,94
8512718bafa447f9b433da9e809215dfc28b6b28,Towards More Fine-grained and Reliable NLP Performance Prediction,2021.0,24
07e420f5b350269aab6694cb19a49799874dadb3,Learning Curve Theory,2021.0,29
4383a975c09b72ba2f1a77cd779bb6965dbfb2fb,Scaling Laws for Transfer,2021.0,110
4454a763c891afb3fb8fa6567a367d05b1938e97,Meta-learning with negative learning rates,2021.0,14
eae695ffd7ea8db1fdf9971544f6ddcd41a2d99e,"Analysis of the Scalability of a Deep-Learning Network for Steganography ""Into the Wild""",2020.0,7
d3edc20ed4a07195f3663abc0ead4220266fd75b,*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task,2020.0,15
673702d188c843b3e495e0037e30bf811fde5ebb,Generalization bounds for deep learning,2020.0,28
b1b54df9b9fc206341877c2bc6f6d291618e0405,Learning curves for drug response prediction in cancer cell lines,2020.0,11
4f55f6fbbd53679a71bd943da0c79d6b9e64fc28,Power-law scaling to assist with key challenges in artificial intelligence,2020.0,9
71446ab82087505d7849b1b6f1fce4dcda751c1d,Design Patterns for Resource-Constrained Automated Deep-Learning Methods,2020.0,3
3efbcfeeb0ea1051a71101d3318da4411081f0b8,Scaling Laws for Autoregressive Generative Modeling,2020.0,186
95cb2118e74e68727d2b9429606ff95afe09a7bd,Domain Divergences: A Survey and Empirical Analysis,2020.0,25
8d90f9474b7432a1aabab113dbeca38ead42117a,Learning Curves for Analysis of Deep Networks,2020.0,15
21cc6e612da081b4d28fab75d09b2e0e6e2cf5f2,Automatic Feasibility Study via Data Quality Analysis for ML: A Case-Study on Label Noise,2020.0,1
647a0e74203cc134c500588616ba60f9717de782,The Deep Bootstrap: Good Online Learners are Good Offline Generalizers,2020.0,11
e0b8b6bf45f845702a684e078eefcc440cad6c83,On Power Laws in Deep Ensembles,2020.0,32
3836ccb33191799e748e8e96f85a813eaf650ff8,Data Movement Is All You Need: A Case Study on Optimizing Transformers,2020.0,70
45eed86953cfe055342b68d13b913789d24e1ad9,"Is SGD a Bayesian sampler? Well, almost",2020.0,28
6ea7201aad5d146ba481051d26b884d19a34af15,On the Predictability of Pruning Across Scales,2020.0,28
6b85b63579a916f705a8e10a49bd8d849d91b1fc,Language Models are Few-Shot Learners,2020.0,9998
deedb9b61a01d686b28e6034770fccc142e77fab,Predicting Performance for Natural Language Processing Tasks,2020.0,41
2c5a8950cf0a13e229ad19093ba064495fda8de7,A Neural Scaling Law from the Dimension of the Data Manifold,2020.0,33
5290d7921f0266c8b50b79fc8a0b7d22868f4f60,The Cost of Training NLP Models: A Concise Overview,2020.0,141
850464c9006261bd632c4203f3e630db09a32faf,Comparing Rewinding and Fine-tuning in Neural Network Pruning,2020.0,283
e6c561d02500b2596a230b341a8eb8b921ca5bf2,Scaling Laws for Neural Language Models,2020.0,1437
0495d9df8eb84dcdab4e5536179823cd26279949,Big Transfer (BiT): General Visual Representation Learning,2019.0,887
a88d957759d2b1fd4f161e45356f18bfb53d6897,Deep Power Laws for Hyperparameter Optimization,2023.0,0
f17fb84d509fc39a105552b5446097e84bb22a65,Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning,2023.0,0
bfef17a7dffac471d945a4ba101b146ccea24dd4,SCALE THROUGH SEMANTIC DEDUPLICATION,2023.0,0
36a329ecbd38c84b9706ccce933f60533338a530,Data Efficient Neural Scaling Law via Model Reusing,2023.0,1
3881801d73a223cac1fd317a5c47df26be87af57,Empirical Limitations of the NTK for Understanding Scaling Laws in Deep Learning,2023.0,1
b17c2f0c2dc3f98e85322889dc144e79d183fcff,Scaling Laws from the Data Manifold Dimension,2022.0,12
50d28a037f68b4788b8c0a748ac4c6a5c1ef0e2b,S CALING L AWS FOR N EURAL M ACHINE T RANSLA TION A,2022.0,0
2d2b68c201986f47d39911a926d1bfd98f8439f5,Ease.ML/Snoopy: Towards Automatic Feasibility Studies for ML viaQuantitative Understanding of “DataQuality for ML”,2022.0,1
da2315d26c2412a634b2eb3998ce7ce4f2a9369f,ImageNet as a Representative Basis for Deriving Generally Effective CNN Architectures,2022.0,2
8db7f1f679edfc4826eb137421b7ff5d3d0852f2,"Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A Case Study for Indian Languages",2022.0,7
62cf47127412fa8a3ad783d23aa3fc4ab36a7d44,A Scaling Law for Syn2real Transfer: How Much Is Your Pre-training Effective?,2022.0,0
76e2b3b6e1da49764d342ea922290410162125ca,Measures of Information Reﬂect Memorization Patterns,2022.0,0
6eed98e7d04e4968da9f28a117ec39f2624465fe,Representation mitosis in wide neural networks,2021.0,0
680f09c0640e6157d0811f387802c67dea4e35ad,Model Performance Scaling with Multiple Data Sources,2021.0,13
8e839692c9b4edcbc64780cb94c2cda2584085b1,A Scaling Law for Synthetic-to-Real Transfer: A Measure of Pre-Training,2021.0,1
905c6d4708972fb535627a646b3695c8bef4fc24,"Statistical Numerical PDE : Fast Rate, Neural Scaling Law and When it’s Optimal",2021.0,0
990d042905b8a41f601d3fdd975bc4051a852bcf,Data Movement Is All You Need: A Case Study of Transformer Networks,2020.0,4
32a6870be73105faa2faab41b550f087639ea281,Survival Loss: A Neuron Death Regularizer,2020.0,0
71f8b19c00498adf0a3d6c991a9b69f78bae3a55,P REDICTING THE IMPACT OF DATASET COMPOSITION ON MODEL PERFORMANCE,2020.0,1
9a5711594a565dc489f2b882a8bb5c3152b2d087,Snowcat: Efficient Kernel Concurrency Testing using a Learned Coverage Predictor,2023.0,0
f73cd1c9eba950c04fbd81e1f024392978059d59,Scaling Laws for Associative Memories,2023.0,0
8923aec569a13f94148e3e90a94c68730f6ad03d,Searching for High-Value Molecules Using Reinforcement Learning and Transformers,2023.0,0
74e0f3400b6ef05a5eb7bdac1b6d0758cbf5b1f1,NOLA: Networks as Linear Combination of Low Rank Random Basis,2023.0,0
b6e773e8a69ae6c13c5fab08a6d97b554318ea19,Multiple Physics Pretraining for Physical Surrogate Models,2023.0,0
95f2bdbc0fa4f93fa2917e9a313b29183b6e65de,Video Transformers under Occlusion: How Physics and Background Attributes Impact Large Models for Robotic Manipulation,2023.0,0
a2bf0f9d41c550bc7cf69706b322f9c9838d3b00,Large Language Models for Test-Free Fault Localization,2023.0,0
66380c4a519b7c3468d5a71fd1921d37a0890110,VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores,2023.0,0
c35c41dc9c93eff38ec8913cb4d639b06a915d33,Linear attention is (maybe) all you need (to understand transformer optimization),2023.0,0
4e3254d90560d19b42419e9b4c9367cf3674dcad,Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers,2023.0,0
9486179c5ab6e0cff0ef919c34080a882bb9fe9f,AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ,2023.0,0
bfeda6c7aa7899a80adb01894555b09d24756a59,Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023.0,0
d7a30e5a0688943adefa05e19fc7fd3193630a30,On the Stability of Iterative Retraining of Generative Models on their own Data,2023.0,0
98478ac589e5b40a20630ff54bb4eec4ab4c5f6b,GAIA-1: A Generative World Model for Autonomous Driving,2023.0,0
f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55,Data Filtering Networks,2023.0,0
12db3efff4cc9e16822dd64bb1cad66f3f034f3b,L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models,2023.0,0
02bc90e9fb4a681b048c6652720afe439d16e6cd,Scaling Experiments in Self-Supervised Cross-Table Representation Learning,2023.0,0
9f9cdced51568c623ec447bf0ea9709b383b5a0f,Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks,2023.0,0
0d9506ed31d2bebb3e102325417ac20419a14c22,LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud,2023.0,0
039b24eb29bfd65145bf66549735bbd57bb4d4f0,Neural scaling laws for phenotypic drug discovery,2023.0,0
9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b,Transformer-VQ: Linear-Time Transformers via Vector Quantization,2023.0,0
8d727ce66eeef021462c14ab7afbbc1110495b01,A Benchmark for Learning to Translate a New Language from One Grammar Book,2023.0,0
ad91394aaa1dad451e1ea52acb73b525c9574642,Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit,2023.0,0
ac35e13321b3fd108b1a427964872514ea3c3eb7,The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,2023.0,0
9099ee08e59cc33ed1c88d4708cf5c931bf46dc4,LawBench: Benchmarking Legal Knowledge of Large Language Models,2023.0,0
048c43c9e373c19c27395c9e8a51370e52635543,Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories,2023.0,0
4d88bcb406e1eb604bdba1d659f2d8493d6a9a85,ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens,2023.0,0
d7afdc5b3d11471cb8b6d6a6e4ddd708126cf4c3,FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets,2023.0,0
e157ab212c441f61686d955b75ed697eca69f367,Effective Long-Context Scaling of Foundation Models,2023.0,1
2709c0e161eca5eb4c572ae46802946c60999d97,Supersonic: Learning to Generate Source Code Optimisations in C/C++,2023.0,0
73bd1ee5193d1f9fa5913e15fcf111db9f75bc75,Large Language Model Alignment: A Survey,2023.0,0
fc709c4e746f5fef146c19a8ac7873db26a57d74,General purpose large language models match human performance on gastroenterology board exam self-assessments.,2023.0,0
56b5b15ab8f6b8aefa5ff497f07ec083a83638d5,Small-scale proxies for large-scale Transformer training instabilities,2023.0,0
03ec8d1d8f91dd727371512f9079cad31e202f66,(Predictable) Performance Bias in Unsupervised Anomaly Detection,2023.0,0
32479758dc9ff9820828a12aa7f3d066f187dc1c,LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,2023.0,0
a808907ca9ab19f1ed10fe81f399d666fbfad50d,Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces,2023.0,0
3f1b5af721e7085ccedc4a27e1272d412e396ee8,Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity,2023.0,0
aaf866c645cd9bcb35ae8567ce0eef77d466abf8,"De novo peptide sequencing with InstaNovo: Accurate, database-free peptide identification for large scale proteomics experiments",2023.0,0
d0ddfc304c6a3490008835a944d8d0065ece77e2,Cluster-based pruning techniques for audio data,2023.0,0
91158ac126e1394ac056d98b87bd6994d503f0ef,Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models,2023.0,0
7b689adb8c156d6158660f90d1c86888ee281f63,DreamLLM: Synergistic Multimodal Comprehension and Creation,2023.0,1
77b046c5d568b329a927cfc895ea2e6c8f43ff43,The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute,2023.0,0
c9896820b341e339d07e830f42898ab79b48c997,Explosive growth from AI automation: A review of the arguments,2023.0,0
c17c3c31e104e04a2f33f87d1a38909082df81c9,Prompt engineering of GPT-4 for chemical research: what can/cannot be done?,2023.0,0
c96297261467b5daa2d01227496a70d444602434,Baichuan 2: Open Large-scale Language Models,2023.0,6
bc9f29881c1d93d225f0a74fa700531202c7043a,OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch,2023.0,0
437a386f3fe4b8c7449a37e2364412a26e0a478c,PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models,2023.0,0
21091f8133ab034baacb92fdb958e14989eb427f,Language Modeling Is Compression,2023.0,1
ac70fb2c74aa87420878c441c3d24969947e0294,Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training,2023.0,0
292097a93d1cd97c317bbe0bd663d3c9669042d5,Initial policy considerations for generative artificial intelligence,2023.0,0
e716e6e0b3dd5124268780dc9bed521a07f371b8,Contrastive Decoding Improves Reasoning in Large Language Models,2023.0,1
62b4e06f5249d22e4a153ec4a2dc934c6a014372,OWL: A Large Language Model for IT Operations,2023.0,0
5038d82c10fc6784d96fca8d3dc5f97c4e479efd,BROW: Better featuRes fOr Whole slide image based on self-distillation,2023.0,0
9b98ca94b7733feee1cff5c57596611ad35fa7aa,Scaling Laws for Sparsely-Connected Foundation Models,2023.0,0
e892a225c417fbac7545c3e31b45d1c42dc9c933,Chain-of-Thought Reasoning is a Policy Improvement Operator,2023.0,0
d3e6702c319cbd5314f0adea6e408562cb104e77,Uncovering Neural Scaling Laws in Molecular Representation Learning,2023.0,0
68ed29e5d398e6030cddcc575f1977973c8b0791,"A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time",2023.0,0
dc4e366166a505320a5423393c2869591ec70200,Complexity Scaling for Speech Denoising,2023.0,0
7d7734879954700a0654bf9ddd3ba5dccad7b0ac,ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3),2023.0,0
c413a339d7784574ed43debea494ef405ee09d81,"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",2023.0,1
6483a6f2038cd8583ad5b6678602bc904459a7f7,EarthPT: a foundation model for Earth Observation,2023.0,0
87ed9604338cca1c075b8988b43864781b32a0d0,The Grand Illusion: The Myth of Software Portability and Implications for ML Progress,2023.0,0
9bb3deca32af8d632e0d916c587cca6c185a6576,Uncovering mesa-optimization algorithms in Transformers,2023.0,0
5aae7d84f8eaa55f3386cee41d94769e7ab01e9d,Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning,2023.0,2
0726d711e4098f8ab73c6dc6af141cc2b66186a6,"Future-proof: Monitoring the development, deployment, and impacts of Artificial Intelligence",2023.0,0
b3c406596c1223e1d91f763cd2f107b0eea77b72,Generalization error bounds for iterative learning algorithms with bounded updates,2023.0,0
110804428354df709b3693f9efc81946a9036ebf,"Neurons in Large Language Models: Dead, N-gram, Positional",2023.0,1
3146b57943bf0f052b2fe3ebd58a8babb3aef343,Meta predictive learning model of natural languages,2023.0,0
00e889fcfaf4396a20f37f681cf8b14f3e878879,LLMCad: Fast and Scalable On-device Large Language Model Inference,2023.0,0
6fa243245110dde83317770a05398c01056f8251,Public opinion evaluation on social media platforms: a case study of High Speed 2 (HS2) rail infrastructure project,2023.0,0
2aa0488985e500fdfa55bc11ee64ca45aa232955,"Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck",2023.0,0
f8afa4bd5b05f52ffa304f56aed7a5792a42ef1f,FLM-101B: An Open LLM and How to Train It with $100K Budget,2023.0,2
a570d3d8f1b66a8ab3fff7876dc9bba3bcdc789b,Aligning Large Language Models for Clinical Tasks,2023.0,0
9f5409401b78c587fad84e3065d1232e2d1ca817,Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness,2023.0,0
d315ca681e95b73f2a6a6115d1e218dec9720d6f,QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm,2023.0,1
77b603850094ff749c9040772f8169a75145d506,Explaining grokking through circuit efficiency,2023.0,1
4bb8aae62c6f6023705d27504e48cdc9f01e6044,RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,2023.0,11
135ae2ea7a2c966815e85a232469a0a14b4d8d67,Taken out of context: On measuring situational awareness in LLMs,2023.0,3
a9caf21a845cb0b1b1d453c052188de118006093,SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills,2023.0,0
3fd67db1afb95d46a67bd302f36aefe1624afc9f,The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants,2023.0,3
48144ca4e20b183cfe9b410a3e9819da31df350a,Using Deep Learning for Flexible and Scalable Earthquake Forecasting,2023.0,0
5c577988ccebfea96de86678d04fd94fad367d2e,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,2023.0,2
0ad412319d4e186c26f423ccc94d560bcaffd997,Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping,2023.0,0
432d85f5d2165db6e7ffbe6bc7ad43c5fdeabf66,Generative Model for Models: Rapid DNN Customization for Diverse Tasks and Resource Constraints,2023.0,0
1516da95ff7eb6785b05ff11601342361667b0ba,International Governance of Civilian AI: A Jurisdictional Certification Approach,2023.0,2
48eb70ed72f985dc9d300042aeeac77084e3465f,Large Language Models to generate meaningful feature model instances,2023.0,0
19b43ff57e5d8f8a99da4110fbc30b4ecc39a527,Spoken Language Intelligence of Large Language Models for Language Learning,2023.0,0
a5cec58c2525b6f5b011d2170bf37a67b42d1fbb,Large Graph Models: A Perspective,2023.0,0
50a124ba06f63bb6d462a02a3f442af8b15bd0f5,The Poison of Alignment,2023.0,1
841015849a9bde6d95b60143dad2b9f429a08ae5,AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning,2023.0,2
374ebdc8240a35820cb7ab8bfca37e180e21b605,Sparks of Large Audio Models: A Survey and Outlook,2023.0,1
6ab61706e21c5360f3206f20031c994ae794a5c1,Considerations for health care institutions training large language models on electronic health records,2023.0,0
9cbbb250a565228ba328038ee7944b89cff53e84,Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning,2023.0,0
11cf88dce827bd67cbfa60400306318022e736d5,D4: Improving LLM Pretraining via Document De-Duplication and Diversification,2023.0,1
f8b90d640158f61c4553518a8554a73b540e07e7,From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models,2023.0,1
3ed178316be914658a80e561bf00576577f34389,Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference,2023.0,1
7fedf859b24ac14b8016542750dd8c4c695d5151,Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature,2023.0,0
99ec8a1e221e1ab86e9df308656639144729f628,A mathematical theory of relational generalization in transitive inference,2023.0,0
681f9009e22c947007b53455e9f8f22e29209010,Towards an Understanding of Large Language Models in Software Engineering Tasks,2023.0,2
aa05929a10a7891a5081b5bfb67fb9ef35041640,GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training,2023.0,0
d62daf809266e02a3e3be4bec160579ff3839cc9,An Investigation on Hardware-Aware Vision Transformer Scaling,2023.0,0
8cf37154fc183d16e4c17c86309855248662b709,SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding,2023.0,0
b169cbff7d5a11afac18f929d5c69ea0933a4da1,GradientCoin: A Peer-to-Peer Decentralized Large Language Models,2023.0,2
3826d8ef406de77f05c8774fb37d9431c5ceda95,Unreflected Acceptance - Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education,2023.0,1
7bd6a6d55c55f5b66ab04f2f5fa6659bad928c68,Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants,2023.0,0
a402c9b48238fb9755d8117f7c57eed039906939,CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning,2023.0,0
79395f4ef32b925381f7ec9a824b05bdb982fd33,Large Language Models as Zero-Shot Conversational Recommenders,2023.0,3
3ac3c10e1317fe8419f794cf30ce3227e95e1f54,Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes,2023.0,0
7054a39213d8ab0b96fe632867df5df915af4f7f,Latent State Models of Training Dynamics,2023.0,1
f92e80ce8c78d733a7a2895ba3fd63707911b61d,Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training,2023.0,0
d3741ebfe500da26a8a08b8f4c9325ab26015202,Two Phases of Scaling Laws for Nearest Neighbor Classifiers,2023.0,0
451a657dabf80ebc43f6a3be518250b2cd5dfe1a,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,2023.0,1
54d1c51b5b4c70eba1caeb77397a1f330c089b9b,Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice,2023.0,0
64e802ea8e9dbe247c31fb06184c04dbf9e55e4e,EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce,2023.0,0
96f6ad72733599db609332987ec6b65e30f11d07,"Platypus: Quick, Cheap, and Powerful Refinement of LLMs",2023.0,4
0b220041eb83c23b7b10d32a5d08c0309d528071,Large Language Models for Information Retrieval: A Survey,2023.0,1
9144b4010332136da0584f202db624ce81d1bcba,"Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",2023.0,0
c51192d7440807dc98cc4374fb5d919390d70b0b,OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization,2023.0,47
88cba3a919aa10c18f42ccbf2bab753c17b3d947,Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling,2023.0,0
d55d9cc276f283a080ed003b097eb93a8202419c,Composable Function-preserving Expansions for Transformer Architectures,2023.0,0
4cc2d056365b8b5a58ce57f4b0bf7b20cfa2b6b7,Universal Fuzzing via Large Language Models,2023.0,3
f5ec0b22930faf15c3a7912bda367e8a9f4c8bd4,On the Unexpected Abilities of Large Language Models,2023.0,0
2f181fae61d469e651a8cc14eaa3ca639a62359a,Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK,2023.0,0
8ab27e590fccc2a2a1fcd6b820ff3e2d3d36e36e,Effective Enforceability of EU Competition Law Under AI Development Scenarios: a Framework for Anticipatory Governance,2023.0,0
26d4cb3aae776fd140e1197465e3e484a87dbae1,Topological Interpretations of GPT-3,2023.0,1
0d77314166c54a17a98e1317f9ba1e48dcfe9e83,Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023),2023.0,1
d78282bd8539a50dfaaff5690aaedffbc833e924,TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties,2023.0,0
049ad4be9cdfd92f02c36f8fb056c5c7a98ff750,PromptSum: Parameter-Efficient Controllable Abstractive Summarization,2023.0,0
a37aee483e392678e6c376f6f9ab70ebda2952c5,Efficient and accurate sequence generation with small-scale protein language models,2023.0,0
0a04d0f9ffc0d30157dea059abbf344c681908ed,DNA language models are powerful predictors of genome-wide variant effects,2023.0,4
f56f19fcae3dfbaa157fd4660595052a1d809dea,Advances in machine-learning-based sampling motivated by lattice quantum chromodynamics,2023.0,2
91206346edbe28abb606d7b3425cd455d4019d4f,Scaling Relationship on Learning Mathematical Reasoning with Large Language Models,2023.0,13
f6a503bd80a640ad7cb7e038e9e1b5618f8c24ec,The Capability of Large Language Models to Measure Psychiatric Functioning,2023.0,1
1e2eba005ccd8ab7a668a525c5b43245853bdaf1,Reasoning in Large Language Models Through Symbolic Math Word Problems,2023.0,2
1e26b42669b060a3850e4766dea0db6e3c85cdec,Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey,2023.0,1
eacc01a7886e01c7e79bdebec242d3c1a2608dd0,The Paradigm Shifts in Artificial Intelligence,2023.0,0
2edccb8fa562ed52cd49ea6fc67ed32db6218247,From Sparse to Soft Mixtures of Experts,2023.0,1
c6026b642f7967484a353fc34815e184c0eb727d,Jack and Masters of all Trades: One-Pass Learning Sets of Model Sets From Large Pre-Trained Models,2023.0,0
6ba360aee802b18d8ecc303e8855b832b1c29dc7,Applicability of scaling laws to vision encoding models,2023.0,0
447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e,Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models,2023.0,1
f4f090cd549c5961894b7f94a499ca4d3c4b0842,Linguistic Explanations of Black Box Deep Learning Detectors on Simulated Aerial Drone Imagery,2023.0,0
02718f60311b790a67788d6517574cf1d1a3f73f,Consciousness beyond the human case,2023.0,0
7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4,When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities,2023.0,6
f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d,Scaling Sentence Embeddings with Large Language Models,2023.0,0
d851a5096c74a4fd7a41be38acbaad518615af20,The physics of optical computing,2023.0,0
5404f4e28645861ebed175e580e2b95a6fff9ca9,UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models,2023.0,0
819c9542d80da3c0ab2889090eefe75b181386d9,A Theory for Emergence of Complex Skills in Language Models,2023.0,3
88695b5bb6462872ce1dd946cff00dd6ebabf2d9,Scaling TransNormer to 175 Billion Parameters,2023.0,1
4d21debb0f5fec315181e0912b5105c6ce4fc67f,Backdoor Attacks for In-Context Learning with Language Models,2023.0,2
d6234d686b22db52ebb6c41f74706bb99df0faaf,On Physical Origins of Learning,2023.0,0
f1e77517b8921ad2570af3f35f36ec11958a26ef,Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI),2023.0,1
304f8b4edea01fdb5a2f7f8b998c83188deeccff,Towards Generalist Biomedical AI,2023.0,8
b400be3bfa817d9cd5f94f9fac8efc5a3f257153,Educational data augmentation in physics education research using ChatGPT,2023.0,2
5e6f5d1dbb4b2b26755cd40466a4441e5b6f262e,ICON: A Linguistically-Motivated Large-Scale Benchmark Indonesian Constituency Treebank,2023.0,0
c29dbfbc17fa190b787a2662d49f08a38c8bd166,ARB: Advanced Reasoning Benchmark for Large Language Models,2023.0,8
3d04f5bb0599ce02d8fa47420f72f500758c4660,How to Scale Your EMA,2023.0,0
e7634ee6523860fa893f6a82faaeb29fcb8cabf6,ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer,2023.0,0
685df9dfc68655096e02db7b9c372cd169e07b99,Optimized Network Architectures for Large Language Model Training with Billions of Parameters,2023.0,0
6795e4ea791d05863c8496cced66ba13520f53cf,Deep learning subgrid-scale parametrisations for short-term forecasting of sea-ice dynamics with a Maxwell elasto-brittle rheology,2023.0,3
e06595ebb2fe4d73fe42e566b57d7a109df75615,Instruction-following Evaluation through Verbalizer Manipulation,2023.0,3
435925f84dbe507fc7a16ac82b7824c8419d089c,Exploring the Landscape of Natural Language Processing Research,2023.0,2
87e5bb672d578c0f2bc654ba53d476186fd4b813,Exploring Usability Issues in Instruction-Based and Schema-Based Authoring of Task-Oriented Dialogue Agents,2023.0,1
7aad760762c4a10dfbc2d3391eb8bdb28c80b236,Federated Large Language Model: A Position Paper,2023.0,3
80b4a44ab0303ace08afc1110381866461048b23,Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey,2023.0,0
104b0bb1da562d53cbda87aec79ef6a2827d191a,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023.0,473
8282c6b141c335fc144818f20d86e68800691a01,"Reproducibility, Replicability, and Insights into Dense Multi-Representation Retrieval Models: from ColBERT to Col*",2023.0,0
f5e670c22d1125de557aaa79f721fcfb557fcb36,Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning,2023.0,0
24a46662853bf85d316d655f2c8790aa67815d02,Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning,2023.0,0
e3dc6559baad16f2edb8b03add561834e2e9439c,An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration,2023.0,0
94450a1c1fcfe2c57f2d345867e0394a48cf481b,A newcomer's guide to deep learning for inverse design in nano-photonics,2023.0,2
a68dc9208aae7578e8ee384caa8ccbcf34e539e8,"Mini-Giants: ""Small"" Language Models and Open Source Win-Win",2023.0,0
94ce1d5924e05e8d75e43ce70044293ddcef850a,Large language models in medicine,2023.0,29
291955cc13853de19c58337d6a68816d00c2fa74,CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,2023.0,0
97d8400f4e02e3831e00acbdedc612adfc9a3a90,How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery,2023.0,2
c064c79e3026f81e5043cd5b0f4264b4d43336e6,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,2023.0,5
3c8f5d5e04f5c46898632a9d3e631cf8bedddeb0,Implicit regularization in AI meets generalized hardness of approximation in optimization - Sharp results for diagonal linear networks,2023.0,1
42029832864c30c42a77538939f176f572b324a6,SecureFalcon: The Next Cyber Reasoning System for Cyber Security,2023.0,2
99b0c3a18050889c591e1db6d51ca01298638437,Transformers in Reinforcement Learning: A Survey,2023.0,0
0e41ae9360a962430650d5bb174de223aa8deea5,Navigating the Complexity of Generative AI Adoption in Software Engineering,2023.0,1
0b0d22adc201913c7ff186504db129cc51d9971c,No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models,2023.0,5
3e664adb009dce373129a3563e4b2cb08731bc76,PolyLM: An Open Source Polyglot Large Language Model,2023.0,5
2e4ad5efadaaa317d7f8f148e8c7d10fce97ba59,Scale Alone Does not Improve Mechanistic Interpretability in Vision Models,2023.0,2
e94e5f89f92018f2ff0a6b77e4373fb77a9d1c17,Stack More Layers Differently: High-Rank Training Through Low-Rank Updates,2023.0,3
4c4effecc6b59d442f1ae7dd76bdf1118fe8421c,Large Language Models,2023.0,42
1b90e9e9734bed6b379ae87d688cb3b887baf597,Objaverse-XL: A Universe of 10M+ 3D Objects,2023.0,6
2111fb505e95de968d7a031aadbc5d5582851f70,Writer adaptation for offline text recognition: An exploration of neural network-based methods,2023.0,0
68fd6cc9b41291d625b41761149016be6485c0b3,ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey,2023.0,0
ef4b604fca0c62dcd0d5caf7ca24ad74e285632d,MultiQG-TI: Towards Question Generation from Multi-modal Sources,2023.0,0
2d23de7d30419336633dd94115382818bcd68e8b,When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment,2023.0,0
f581b2f1f2998ab4193e53bec7672f6f8948fc8a,"Choice Architecture, Privacy Valuations, and Selection Bias in Consumer Data",2023.0,0
67470539bcd9c51016294c3e325b608554dc90a4,T-MARS: Improving Visual Representations by Circumventing Text Feature Learning,2023.0,4
494b043fce4da2ecc7f87bc96f7c29a5278cca61,Frontier AI Regulation: Managing Emerging Risks to Public Safety,2023.0,12
8eba1bb64ecfafb097e565f167b61a6a450ae733,Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources,2023.0,0
4ad771a10145e5e3b7e74bf6e98b165d7258889f,What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?,2023.0,7
ed391d9143bb81715c051d866e2b51a4d4c9c772,Won’t Get Fooled Again: Answering Questions with False Premises,2023.0,1
c5ef32afd5dcdf396fa61ce6a92e5233c1aebfd1,Absorbing Phase Transitions in Artificial Deep Neural Networks,2023.0,0
3e5741ee9cfd23c79d2af2e209ebb1b57da96e2a,Improving Automatic Parallel Training via Balanced Memory Workload Optimization,2023.0,1
82b67d45ad067c55ae7616e20f1a4a1533b9037c,Postmodern Fermi Liquids,2023.0,0
1e3ef48abeef882e12f9553a1baf8944f3782c88,Several categories of Large Language Models (LLMs): A Short Survey,2023.0,2
72543b71571576e1a86ef55aadf3f17458670d0f,"LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens",2023.0,13
425675cc3a4cfadf3b750e1563090a0ed378ddc2,"Abstractions, Scenarios, and Prompt Definitions for Process Mining with LLMs: A Case Study",2023.0,2
85d75d62e8c6955ce9d4bf2cd98cf2496bc7e800,Improving Language Plasticity via Pretraining with Active Forgetting,2023.0,0
36ccf3967e4cb30caeb9419c113eecde0074aa7e,Minimum Levels of Interpretability for Artificial Moral Agents,2023.0,0
275fb93244b5a465d7e30fc6111e3403b47557be,scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI,2023.0,10
1b173d9b8b0a2529259b6fa16376aff11c1ac08f,BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer,2023.0,5
621902ab679a6e746760198bae556590813b1b4e,What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?,2023.0,9
8dd74ae5a0e580f69b08706d79eb1abcbbef132a,"An extended clinical EEG dataset with 15,300 automatically labelled recordings for pathology decoding",2023.0,2
bab5e35001757719d0f8338f94dde2860dae784a,How Large Language Models Will Disrupt Data Management,2023.0,0
942130a875ccfe55a4c60c27c636f693e25cb13d,Personality Traits in Large Language Models,2023.0,9
72160b3c0f73c968fcb903db71817d1bed695f4d,"Look, Remember and Reason: Visual Reasoning with Grounded Rationales",2023.0,1
edccb296307f1ea187c403072159fc00b96cb888,λ-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces,2023.0,0
2adc13eb55c92e026c4cefc89a47a0ee0ac95111,SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding,2023.0,0
a75166c6afaa6923dfc00ffaf388dc8f5194a9ce,Stitched ViTs are Flexible Vision Backbones,2023.0,0
43bf1f91cb31726725a01aea268a69ae4d84523f,The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit,2023.0,5
e11111dfda2a1f7aa9ecb8720032739233fb72f4,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,2023.0,3
a20c6ce80872dbc6a8e6403b2a366973061a9f89,Scaling Laws for Discriminative Speech Recognition Rescoring Models,2023.0,1
f5dfba34121f0f2cfc3074d70194d7ac6e3bb4fe,Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction,2023.0,0
6f4e5c46cea178819901e55852339b7d85d7e8e5,Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition,2023.0,0
8ea59ec82cf28be5ccc428edc18ff3b7bedf4e1d,Recurrence and repetition times in the case of a stretched exponential growth,2023.0,0
18b556c3d8b2b714a1eaca033a715c4f6c7493c8,The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets,2023.0,1
8aaeaec441c9f8bb6407b6633caa998bb35bc537,Exploring Data Redundancy in Real-world Image Classification through Data Selection,2023.0,0
2f0c2b38da784c81e983c7cc1e7bbabb47237c55,Lower Bounds for Multiclass Classification with Overparameterized Linear Models,2023.0,0
8c88c693d5dc2802d3b76b85740e1f04fdaaf801,Large sequence models for sequential decision-making: a survey,2023.0,0
84960c4d1c4fed017413ef869f7a94cc043c1fc2,Action Q-Transformer: Visual Explanation in Deep Reinforcement Learning with Encoder-Decoder Model using Action Query,2023.0,0
4b2b865d5cca6eb9a113a0074d38cc3d292d2fdc,Scaling MLPs: A Tale of Inductive Bias,2023.0,2
295ab302cb09c20c12ae5363cc9b9b5e0d75b8e6,On Hate Scaling Laws For Data-Swamps,2023.0,2
c520d8a888355f7abb7728b2e2510fe7bc63f814,Large Scale Foundation Model on Single-cell Transcriptomics,2023.0,6
128f921e01d3c71d644346353bf72f830654ca49,OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents,2023.0,9
f60137ff115c4741abb40c909ab94dfee92becd9,Eight challenges in developing theory of intelligence,2023.0,1
2922768fd451ecdb45f48c1a83eb57f54a91221b,Textbooks Are All You Need,2023.0,24
b18c62d515072ccc35709772388b91bac1045514,Deep Fusion: Efficient Network Training via Pre-trained Initializations,2023.0,1
02e699561624adae5ea3550e8ebee988a2557286,Self-learning Monte Carlo with equivariant Transformer,2023.0,1
dfd1927e00519848b124449373168d00eb140bde,DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization,2023.0,0
6946275ebf6fe106aa8c16d7ae75fab965dc470e,Road Barlow Twins: Redundancy Reduction for Road Environment Descriptors and Motion Prediction,2023.0,0
1b0378d52d8988ffd5ecfb507e23420985171306,A Comparative Analysis of Automatic Speech Recognition Errors in Small Group Classroom Discourse,2023.0,1
c1104befbe6b4ded6274ccca28750b20982bbcb5,Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication,2023.0,1
1c7d3a359f4029e5384d784ce118ab53e71877b6,DropCompute: simple and more robust distributed synchronous training via compute variance reduction,2023.0,0
ed3101df8bd7b0addfb9ea8713ddb590a15461a2,Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness,2023.0,3
cc1723cf8a6cecb2295360ba01a01da90452403e,Structured Thoughts Automaton: First Formalized Execution Model for Auto-Regressive Language Models,2023.0,0
b109a1b6254ca0f9467c98285a1d9a1f710f6b2a,Deep Generative Models for Decision-Making and Control,2023.0,1
80aa5cc89de1c044e9218890e2726b791a413d56,Modularity Trumps Invariance for Compositional Robustness,2023.0,0
31d65e179b1d00484154b3525d93846dd82f23d8,Inverse Scaling: When Bigger Isn't Better,2023.0,12
c117158a0cd31a3fbf44b6adc6fffca715d63c6b,Solving Large-scale Spatial Problems with Convolutional Neural Networks,2023.0,1
051549d8ef56937b2f4d113afdcf8c7586d3770b,Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models,2023.0,2
f5359f596e0306599b4aa4157e6fe03567b35c01,Knowledge Distillation of Large Language Models,2023.0,4
7eddac2807fa24901625b680bf7f2eec650e555a,Machine learning in computational histopathology: Challenges and opportunities,2023.0,1
9afa0c3227fd0ec3a76928784e59c4205cbace24,"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",2023.0,3
7c8c2067c5d6f39daa58e5271e25be00836c2cf5,Generic Attacks against Cryptographic Hardware through Long-Range Deep Learning,2023.0,0
fdeebf187c390becf9890ccd37e0f189fa3929fd,Adversarial Attacks on the Interpretation of Neuron Activation Maximization,2023.0,1
6294f078e79828cac21e717813e8f3d02b18a97c,The importance of resource awareness in artificial intelligence for healthcare,2023.0,2
c23707cdd0966ea4bb31755e2d59fefa0970a707,End-to-End Neural Network Compression via $\frac{\ell_1}{\ell_2}$ Regularized Latency Surrogates,2023.0,0
82da02137bae421a3f7a89c3bf2ab662037f4dfa,Embodied Executable Policy Learning with Language-based Scene Summarization,2023.0,1
0f10fc57c7a4aae8181feeca69ad232a05ead7cb,Value function estimation using conditional diffusion models for control,2023.0,0
beca17564ef9a03d42ce9db4e303689fba3ffcc1,Towards Autonomous Testing Agents via Conversational Large Language Models,2023.0,4
c6d3262df3bf9b25a6890e1d59cd1287796b97b9,RRWKV: Capturing Long-range Dependencies in RWKV,2023.0,0
32f541216112de78037d8e0f95ddc152eb6f05fa,K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization,2023.0,0
378a545c3a1cf6c4aada8f9ee8820c0d8008220a,Benchmarking Foundation Models with Language-Model-as-an-Examiner,2023.0,12
bda25425e952fef6ecba708c3337425c6ccaa105,Privately generating tabular data using language models,2023.0,0
1645e93f34ce34c0ff248a7349bf757a416c5312,Health system-scale language models are all-purpose prediction engines,2023.0,19
d11ae7f22045a2217fb2ef169037fba216153c63,Stabilizing Contrastive RL: Techniques for Offline Goal Reaching,2023.0,2
122f2d18a7b621576254ec12079941ecf0632360,GEO-Bench: Toward Foundation Models for Earth Monitoring,2023.0,0
c44bdf4ff085f577a7cdd9a1cebf4af0b4eb0c82,Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex,2023.0,2
bf4810017b54e50354cccffd8966121c7166cb17,Iterative Translation Refinement with Large Language Models,2023.0,3
0ce17ce506657527d0e3ef3679c8be369c785d2b,Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models,2023.0,1
066a9da13badca3791832f50f47103f31d681189,On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research,2023.0,0
3ccc28903fadce0ddbd5c54f1422d78acc8ec932,TPNoC: An Efficient Topology Reconfigurable NoC Generator,2023.0,0
cf59882f25cacb1b688388ca46e6d306dd9fbd41,Efficient GPT Model Pre-training using Tensor Train Matrix Representation,2023.0,0
51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df,SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,2023.0,9
70ba08769e83d5869f89ee9f5f19fab443ab7e4e,Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models,2023.0,1
9cc5df8dffcd99a65b31076aed511d3d63d21c5e,Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception,2023.0,0
588180debec5be162364ad86d280390cb1693852,Large-scale Language Model Rescoring on Long-form Data,2023.0,2
2286562f2b185ce476a23f218f2de83b0561fbe9,bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark,2023.0,0
3fae5fb60ba9eeead5f1b2681ac6b09fe9cc4926,A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models,2023.0,1
0d9484f20e966c3faccc219b71cb1aa21e877569,OMNI: Open-endedness via Models of human Notions of Interestingness,2023.0,3
751563cf0c32fe4dfa43d3416c916f8eb053e5f3,GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration,2023.0,1
b045e0048d6362f5649cc4d0fb6aadb776e5a5c3,Multi-Objective Population Based Training,2023.0,0
8ea24b1dbb3e690ebc64543c03f0552a6c1fb49d,Knowledge of cultural moral norms in large language models,2023.0,4
5a90a8f4ec612cef6b1bb9cf4eae897385d33c2d,An Overview on Generative AI at Scale with Edge-Cloud Computing,2023.0,3
b15848be370a0ed38be2e75c3be807cc77b4e51e,Wearable based monitoring and self-supervised contrastive learning detect clinical complications during treatment of Hematologic malignancies,2023.0,0
d525996d538248685dccb038f85197619df14d93,Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning,2023.0,3
844b22bb025f485d85d00f1f61555a8ff0131658,STEVE-1: A Generative Model for Text-to-Behavior in Minecraft,2023.0,2
c8c6408437ac4bce0e00192ff0c339206350f598,Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior,2023.0,2
6cd94eee6bb0d10e095af1a297919fc73c636297,Training-free Neural Architecture Search for RNNs and Transformers,2023.0,0
e4a95f595b5d60a0858725996b9355f7275492cf,Hierarchical Attention Encoder Decoder,2023.0,0
7a1e71cb1310c4a873e7a4e54d1a6dab0553adce,"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",2023.0,86
c398de8d4a18ec49b8f2eaaf3b0473186b99e1e1,Reimagining Retrieval Augmented Language Models for Answering Queries,2023.0,1
7e55408ab2b3ffd7dca5109375058bac96f6e89c,Latency Matters: Real-Time Action Forecasting Transformer,2023.0,3
43d1320630e4e5ddc08e703b2ab5c32f3a121d1e,Data-Free Model Pruning at Initialization via Expanders,2023.0,0
3167db922c8532213d909890a43c77e8cc9f9098,Explanations as Features: LLM-Based Features for Text-Attributed Graphs,2023.0,9
d9ffb44ee3c8ec0b6692df8a90451384c1edd89b,Likelihood-Based Diffusion Language Models,2023.0,2
0a44b9cc5496e0cb26082f880cf1ed52ebc42d4a,Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,2023.0,7
17dfa45f14fcc1b861bc06b7f0b4678d870628d9,Intriguing Properties of Quantization at Scale,2023.0,3
97dd366154f62d07372b5e09e0ef006a4544b724,Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods,2023.0,1
32ff7e5ea4ef146cc63fdee23af1cc47e89af095,NetHack is Hard to Hack,2023.0,3
28902dc5b6dc0289962c8b88a04ffe03503388ea,Dynamic Sparsity Is Channel-Level Sparsity Learner,2023.0,2
7dddeee6552bfc313e3bb9f0109f3ac5402c561e,Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity,2023.0,0
4d7c4c371d9742d3f2d9c338bfb898fc856e7349,Faith and Fate: Limits of Transformers on Compositionality,2023.0,21
04270591b6006a83b0a8970ef80bcbfc26a835d9,Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models,2023.0,4
54c94a33c4140c532d4cd7e01713a71a1499fa40,A Rainbow in Deep Network Black Boxes,2023.0,2
4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5,Brainformers: Trading Simplicity for Efficiency,2023.0,2
e9f966dfac520ddf0ab053f93ec94f6b9fbe6412,Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks,2023.0,0
5875e7cc6d8941f078e32b51b8d36fc08f9c1774,Feature-Learning Networks Are Consistent Across Widths At Realistic Scales,2023.0,1
1a527ac2736239019a9aedd3494443d5a22b57ad,Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models,2023.0,3
49e294319811fa4d122dcbd127e15188825d8c8e,Federated Conformal Predictors for Distributed Uncertainty Quantification,2023.0,1
24811cadf16519910f643b6084107164e6ca4219,Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,2023.0,1
9aaa71d9311b44e2228eac213c24c37bb9ca64d1,"Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale",2023.0,2
6309a53b7321a6e71ddb248b45c37d3e25643ffb,Manifold Regularization for Memory-Efficient Training of Deep Neural Networks,2023.0,0
31a7d8c4a5ab6bab522494b57270249105c8748e,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks",2023.0,9
efabf7727dc3212676f03f1384809ce019db109d,Future-conditioned Unsupervised Pretraining for Decision Transformer,2023.0,0
51b169701290cd129e0781fc9f3a9918604c89b5,Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,2023.0,3
89ea9c3ebba8e518328e0f9598d3d3f79aa14c46,"Exploiting large neuroimaging datasets to create connectome-constrained approaches for more robust, efficient, and adaptable artificial intelligence",2023.0,0
1dc98c796078210f2734161b7af6d42ff8569338,"LanYUAN, a GPT large model using Curriculum Learning and Sparse Attention",2023.0,0
024d966e6f75e3672d4200e51bc99af6b141b70e,Scaling Data-Constrained Language Models,2023.0,17
e8d6dc483b439c1e5ab839e86794ba301dabed88,Automated Tensor Model Parallelism with Overlapped Communication for Efficient Foundation Model Training,2023.0,0
0fbf7ea1a3bd1754ed9aa12ed25906b731ece589,Training Data Extraction From Pre-trained Language Models: A Survey,2023.0,2
c193eb176985a81ae64f63c5e50b2f11cfb7c4e6,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,2023.0,2
e904967df6fb2530d79d2772e8f56b2543a01648,Genomic language model predicts protein co-regulation and function,2023.0,0
b57509b85eb466d2c011796a5227500dcce3b05a,A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning,2023.0,4
4631398b0d61061b9ca9489d76ded4dd05bcf1ec,"The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",2023.0,7
762cccb8e0897579dd3882488b13a1100b6ab6e2,How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench,2023.0,0
1deb8eb83cc3bb30db98bc21917b815e758b15d4,Delving Deeper into Data Scaling in Masked Image Modeling,2023.0,0
6d31db7c53853de62cacec26facdb4300d6b5092,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,2023.0,2
ec371ee01b81d831def16881b3c44c93045c2132,Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions,2023.0,0
bdb0cc86a8dd8f3a1558b7f3a1d001eea521c6c1,OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach,2023.0,1
5d2dbbcccf3a47a73f746296408b500f399dc8d0,Emergent inabilities? Inverse scaling over the course of pretraining,2023.0,0
a460d28507b63b7265461cd62badd3dc095f600f,Boosting Cross-lingual Transferability in Multilingual Models via In-Context Learning,2023.0,0
5df78a86dc942d3c53aec5ed2e8ac141cb41aa61,Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,2023.0,6
19c63eade265d8a47d160098d97194b3b83d3770,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,2023.0,12
6dd44624ac912fb50c21c691806ee52d27e73abb,Large Language Models are Few-Shot Health Learners,2023.0,8
1041bdde891d58247c0adbbaa5778e0b64dbf059,Think Before You Act: Decision Transformers with Internal Working Memory,2023.0,3
9141480721653789597b6e537ee0eeab401f3e60,PromptNER: Prompting For Named Entity Recognition,2023.0,6
e4ceaac6047fa4931d11d06ace81b72bf5ffbde5,Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models,2023.0,0
39af21a31aea2824d23bcb8b2812537c6597d1b5,When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale,2023.0,0
7fb5b154b7cb6b6761e25b82f67138ccb5f116f6,The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning,2023.0,3
bb8f7fbec020675d269ccfa0e6e603f02b664c0d,PaD: Program-aided Distillation Specializes Large Models in Reasoning,2023.0,5
b555fbe8d4e30fc1312c615ab7dfe6ac030dbcff,Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction,2023.0,0
e251587936562309fb0373b62c753a29b0952809,Multilingual Large Language Models Are Not (Yet) Code-Switchers,2023.0,5
6bf981314d81ca838d2cc55fc6f6265717792b67,Generating Data for Symbolic Language with Large Language Models,2023.0,4
ecc7b6708ea4c61b8fe16cf9b4cef4d392c670a8,Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model,2023.0,0
e9a0b58b498e4d8b16706f04452e854caca1444a,Revisiting Acceptability Judgements,2023.0,0
33b68ea3ce551e33d634660f20ce43fbff0b5fca,Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training,2023.0,12
552572fa975344350b8af09009c647b4d4598f8c,Deep Learning based Forecasting: a case study from the online fashion industry,2023.0,0
a9d61f1688247bb8d92c393395129c52b7c0d0f8,"Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors",2023.0,10
569b6242853dd6299c3f5cdbc040dec53ead8033,Training Transitive and Commutative Multimodal Transformers with LoReTTa,2023.0,0
66d2021641c2003d8614c898bbddb653ec349b22,Rethinking Semi-supervised Learning with Language Models,2023.0,5
f16e25394420bb15361539864fd4a9029ff0bf58,Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design,2023.0,2
1567bcac0ab09269c9d0ff33c9a406132417fab9,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",2023.0,13
a6df4b0c0cee5865a29bb7b9d4d424821de0681f,Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance,2023.0,4
1e4c49c9c93678dec95326ce25715fd2a1e64192,Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model,2023.0,1
026b3396a63ed5772329708b7580d633bb86bec9,RWKV: Reinventing RNNs for the Transformer Era,2023.0,34
07ca3f17e63e9415be9fe830cc14df507d271330,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,2023.0,6
fdb10253a1c32f4c51a6499f0e3bc3fdb11a51da,Learning Interpretable Style Embeddings via Prompting LLMs,2023.0,1
daf9e24adbba3d1aead91cbac26502d3043db069,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,2023.0,6
851b7a7110eba40cba9c85635ec7fe53f82e1a58,Communication-minimizing Asynchronous Tensor Parallelism,2023.0,0
1d98e3de197c56ff89754fb4423418d8df5e931f,Automated Few-shot Classification with Instruction-Finetuned Language Models,2023.0,0
e9b3e82b1c9eb4136df28e94f24cd823431be93b,Lifelong Language Pretraining with Distribution-Specialized Experts,2023.0,2
672491163a327f80e08ce3ef4751e94c78631822,"Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",2023.0,3
8376e50e81329b3db5049a90851cc0418d071e3d,Scaling laws for language encoding models in fMRI,2023.0,4
5692501c10d0c1762842f92c66fcf0bffe2c0342,Multimodal Web Navigation with Instruction-Finetuned Foundation Models,2023.0,5
506d709b4a11ebf1b2e2f142f4b6f0d58abac450,AutoTrial: Prompting Language Models for Clinical Trial Design,2023.0,2
4f0c7f4df04f07609bdb67944af2a529d5a4517b,A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,2023.0,7
c07eebebee2b2d8519dc730dbcd979f8dfd7b562,Reducing Sequence Length by Predicting Edit Operations with Large Language Models,2023.0,2
018e943ba0452b05edd903c3eaf746068ebca138,LeTI: Learning to Generate from Textual Interactions,2023.0,5
ef0a8963022aa11ac9e1042acc086052b8ec1678,Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model,2023.0,4
a4b1c935b9bf55fbb003e2c3f49c7bb28906c789,PaLM 2 Technical Report,2023.0,233
3fb0731538c59f8520a309996a0567b58965f0fe,Pre-Training to Learn in Context,2023.0,3
5c7aaee5651221893ea0e67c363cab4c4be53b83,Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,2023.0,5
d50ddfa3f02b8fa5d0a7bfd396ddaf9afc895cb1,Revisiting the Minimalist Approach to Offline Reinforcement Learning,2023.0,1
2743ce22ace6362be2a01900106bf4b62280e22c,LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development,2023.0,3
412e266cddfd87c79087a88ba1e4d11b89a45a13,MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,2023.0,11
28085f480ce456a376ebace9b899e3bc93dbc048,TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,2023.0,22
742a4098ddc25fd3c53d4ebdfc845d1e5d2bbff7,How Good are Commercial Large Language Models on African Languages?,2023.0,0
b5982d5ef4eed477bb3dacfe43c5c5b5fe173694,WebCPM: Interactive Web Search for Chinese Long-form Question Answering,2023.0,7
ac1d4a24e58871d0173515709b9ef0c5d64c3294,Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,2023.0,15
661e8ac4908a9d2a85835245ea99b6a314cc4a60,Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns,2023.0,16
1fbaf2d8b69ef6e42a38c233f5d01bea70bad5b7,Fast Distributed Inference Serving for Large Language Models,2023.0,2
6cec825e32b1790a69893a5b2506818241506217,A Glimpse in ChatGPT Capabilities and its impact for AI research,2023.0,4
516631db8d75b3f223ae66260a3e048d6d8eae72,LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM,2023.0,0
ea8197cb357af6f89e8b6e5548897236a24719b1,What is the best recipe for character-level encoder-only modelling?,2023.0,0
b7ec5ff2ebe38c2d0191ad0a728e1726d200f645,StarCoder: may the source be with you!,2023.0,98
9e41cd86a5ebdcbecfef6fd7395e736e2453e495,When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution,2023.0,5
9d872153d5905e4555cbfd59a1955d3c89879432,The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation,2023.0,2
e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2,Augmented Large Language Models with Parametric Knowledge Guiding,2023.0,5
a026ff41d370292caa13b2c2ce540f1e19bb79fc,Leveraging Synthetic Targets for Machine Translation,2023.0,0
82b17686abba18dfc821a262dab5fbb081aa2388,Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,2023.0,2
04000ad50c18192a322f8fc031ce4225aa3cede4,Conformal Nucleus Sampling,2023.0,5
e71a0140d9d690a6236c07451c7f03272b534562,BranchNorm: Robustly Scaling Extremely Deep Transformers,2023.0,0
29312bc12a22c423dd0968a18cd9e422881e29c6,Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity,2023.0,1
d4d7966a7b4dbf4cd764e443392d22826618a293,Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs,2023.0,0
450b5490cc653478c272be50aa986798df828a20,Uncovering ChatGPT’s Capabilities in Recommender Systems,2023.0,21
c982c02c56a3d06e7c46ab98211bab0181b1da7e,Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy,2023.0,1
3a5ee836d71e1360015990e5a67f545d00e6ca10,Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations,2023.0,1
5e13bf41c138fb0518a1bdebe4d2cbe699812d8f,Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner,2023.0,7
5aec1050fd62971f2b40b597ef7526fbc820beab,Automated Program Repair in the Era of Large Pre-trained Language Models,2023.0,26
13e0f0bf9d6868d6825e13d8f9f25ee04285cd29,Poisoning Language Models During Instruction Tuning,2023.0,14
8cf819f6ee33909484ece40d79944c9c37f01a89,"A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development",2023.0,31
d9f7a191c1febd6a4d2d3e2530d8811cb0686898,Concepts and methods for transcriptome-wide prediction of chemical messenger RNA modifications with machine learning,2023.0,0
96c9524916bc38c14e1569c8a21e6497dac709cf,Language Models for Multimessenger Astronomy,2023.0,0
1b31c83ecb6da7964e8294cb5bd8cb6d3b380e34,Exploiting Input Tensor Dynamics in Activation Checkpointing for Efficient Training on GPU,2023.0,0
b533c638bcae456fbecc9716516122e923b51a32,The Emotions of the Crowd: Learning Image Sentiment from Tweets via Cross-modal Distillation,2023.0,0
47975c425e03911ad2ac9e618bd9ae1b23e4c852,Hyperparameter Optimization through Neural Network Partitioning,2023.0,3
186e96fe036927182ec963b63f9dd7f8ff650158,"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",2023.0,14
4128eb532a044cc67f21df82d4b78b62f9c7f6ee,Towards Automated Circuit Discovery for Mechanistic Interpretability,2023.0,22
b4eacdaa5a0d765b1804ccc6289dce84a86840d6,Revisiting Network Value: Sublinear Knowledge Law,2023.0,0
389ec3e8902a5dcfcde1adec735854e93f845937,LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions,2023.0,18
43111716c14c9e509dceffdd60196ad3f5912a36,DataComp: In search of the next generation of multimodal datasets,2023.0,33
04ee9597be4d6d2457214334e495e591000b5542,PMC-LLaMA: Towards Building Open-source Language Models for Medicine,2023.0,21
4f0c057ebc71dc8804d503495f1a35c9cfb9b86f,Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models,2023.0,0
131c6f328c11706de2c43cd16e0b7c5d5e610b6a,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,2023.0,69
3caaea71b338a1a6d33263be06e5fa8d35438093,Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates,2023.0,0
ca3037fed8ed14dea92985b9f288b05185f867d0,Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks,2023.0,7
8bc617c9139648d7a92991d70c671230bac7b2e2,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",2023.0,42
097adbdd0afb2c4f4bdb694a2a8228a248ed1e36,Application of Segment Anything Model for Civil Infrastructure Defect Assessment,2023.0,5
deb8f26509ae320fc975b32922416cb156c61bbd,Emergent and Predictable Memorization in Large Language Models,2023.0,14
a1d990d92bf199525aea68b84a56c62eafc7f27b,Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search,2023.0,0
261549439aebdda72b648ecc462448fd24857ac1,Progressive-Hint Prompting Improves Reasoning in Large Language Models,2023.0,33
682346d2a78fa57e8e9649052f3fea01f5924f1e,Robust flight navigation out of distribution with liquid neural networks,2023.0,2
0ffd57884d7957f6b5634b9fa24843dc3759668f,Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study,2023.0,30
05e003a34148d4663734d3f39deefa0979d2a0e6,GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information,2023.0,14
d0a93ad1af752853f73d89288e8fafb0c9906801,"ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models",2023.0,8
d4d461288e76c7b6a36ffd6b66c26815bc7bd8e6,STen: Productive and Efficient Sparsity in PyTorch,2023.0,0
69b86c9379a5990f6e9faefc6c537aad62c30993,Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales,2023.0,1
ebb1acf1eb3705510de5a19065092404427428a5,STU-Net: Scalable and Transferable Medical Image Segmentation Models Empowered by Large-Scale Supervised Pre-training,2023.0,8
a43a3fadc9190e61b34f59a913f1716e443519e4,On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,2023.0,27
4812ea10550820dee56ce6e38795ce04e0a351b5,Exploring the state of the art in legal QA systems,2023.0,2
93752cae0d4ecd2d09d6660feb3c1860af973f18,Can Large Language Models Transform Computational Social Science?,2023.0,40
b66fe3aa4a795060c2f0f001c502eb126d4d2876,RRHF: Rank Responses to Align Language Models with Human Feedback without tears,2023.0,42
281a7a99c16ce8f53bfbfb7aeb460dbd28648d28,Toxicity in ChatGPT: Analyzing Persona-assigned Language Models,2023.0,41
31296e09c844eb684136ee1b4930da3b1396bba6,Balancing Privacy and Performance for Private Federated Learning Algorithms,2023.0,0
052e9e31544869957996c1a44084a9c41d29bcfe,Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,2023.0,37
38179848e2d6a3ad373b1793848816111428ac36,OpenAGI: When LLM Meets Domain Experts,2023.0,16
f274ce903eca789be99fd6e721dc6e213f8debeb,Learning a Universal Human Prior for Dexterous Manipulation from Human Preference,2023.0,2
dbbc5003af690799fa4fe6330fb795311cde106f,FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement,2023.0,7
9af359ce4b885759e5b775b36e9332289efefb33,Comparative single cell epigenomic analysis of gene regulatory programs in the rodent and primate neocortex,2023.0,1
13581a46d32822e44cbeb1acdba4a59cef2b2ec1,On Efficient Training of Large-Scale Deep Learning Models: A Literature Review,2023.0,10
0665b9c0637c416699eab8b205bbce6564f99328,On the Pareto Front of Multilingual Neural Machine Translation,2023.0,0
ece77610adfb0fb162dd22ef694f2777393c319a,Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,2023.0,26
79a574788b99f3a065cdc972c94497a921af5239,Inductive biases in deep learning models for weather prediction,2023.0,2
7470a1702c8c86e6f28d32cfa315381150102f5b,Segment Anything,2023.0,571
b2fafd5a5a6376c1afea627ffea571d5c6acf134,Predictive Coding as a Neuromorphic Alternative to Backpropagation: A Critical Evaluation,2023.0,1
8f4e198467de15fdbb305d0982ff6f15565ab601,To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency,2023.0,1
5861df95084cf739a6ca3185d6523dd702bd1f10,Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT,2023.0,3
be55e8ec4213868db08f2c3168ae666001bea4b8,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,2023.0,137
23a183676b28269e7a427c41da7329b6326a9f17,Eight Things to Know about Large Language Models,2023.0,32
e06b097b0b3865d627b3bcc0d8454242bfb3ffab,Artificial Intelligence for Scientific Discovery at High-Performance Computing Scales,2023.0,0
5a23a489bb9e742edacc8b8e778b06e1594365d3,Generative AI at Work,2023.0,27
35e7f3c5f70389e445f4ef615bff1ef4a8b16c84,AI SoC Design Challenges in the Foundation Model Era,2023.0,0
2b8d28149a43b9659a6da2c56014ec4206a912b4,Ticket automation: An insight into current research with applications to multi-level classification scenarios,2023.0,1
1d29334cfbe9a1a943082058876f0c22d44c62fd,A Survey of Large Language Models,2023.0,393
06c6eca6bf50d10f7e8a9d9a29f9457526a0d7a5,Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare,2023.0,5
83edcfbb206ddad38a971d605da09390604248ea,BloombergGPT: A Large Language Model for Finance,2023.0,107
11265dd3ba850f41c52d90c7a88c8e8f8688364d,Accuracy and Architecture Studies of Residual Neural Network Method for Ordinary Differential Equations,2023.0,0
138fa5f64fe54376022998fe553b6156a93ff19e,Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need,2023.0,2
1bba2a9c6db3b356b8ae6ef2efff5645e4d96c2b,Text-to-Image Diffusion Models are Zero-Shot Classifiers,2023.0,12
7b52b03775c72503684d48411f562ae29870dd3a,Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture,2023.0,0
cfb5ef050cad787196d60a55a9583107d02e58e3,BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning,2023.0,3
fbf6a107dea17d2c5e0b714ad5541f5bef8317d3,Guided Transfer Learning,2023.0,0
424132ec245c3173685751ac1101c3be6cc55a67,xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data,2023.0,1
15145a009d3532b8cfb663805115f5f229396079,A Simple Explanation for the Phase Transition in Large Language Models with List Decoding,2023.0,0
38b722315873e4519c47cd27b578a14989a0a453,Exploring the Benefits of Visual Prompting in Differential Privacy,2023.0,3
285dae5c2f2ef55c70971094a1ddd45afe720eee,Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense,2023.0,5
23c3f8139124c65533566bf14cc66f8fbf29bc37,Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture,2023.0,0
40eec924e30d235df94dab154541b888095f63b1,Machine learning and Bayesian inference in nuclear fusion research: an overview,2023.0,4
3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1,EVA-02: A Visual Representation for Neon Genesis,2023.0,17
b9efcc0930946333bbf4dcf59a5c7a06a5c68389,What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring,2023.0,8
12d16f426edc6ab248fb476007bd1646282d4d68,Language Model Behavior: A Comprehensive Survey,2023.0,10
348a1efa54376fa39053e5e25d52bd0eb6a0ba68,Capabilities of GPT-4 on Medical Challenge Problems,2023.0,165
362cbfd0d05e139cd6cf049754098a6e1520b910,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,2023.0,18
78627329741d84f7590b5c1fa8297a074dcc0f4a,"Bridging the Global Divide in AI Regulation: A Proposal for a Contextual, Coherent, and Commensurable Framework",2023.0,0
5501d00310b06e00351295529498cc684187148d,GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models,2023.0,107
0c996bb80acce751973940175862962cf0b3a2fe,CoLT5: Faster Long-Range Transformers with Conditional Computation,2023.0,16
6b2395ebcdc3f284d30fefe5d9877aa3caa7b53f,Reclaiming the Digital Commons: A Public Data Trust for Training Data,2023.0,2
f59559bbc4640a63f5e3b9ede8df763acb82353f,Towards the Scalable Evaluation of Cooperativeness in Language Models,2023.0,1
c926bd6ebdbcf0bfabb55c1f0171055bc2dbe516,Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs,2023.0,8
7a3f4935d55b8c2cc7fb44d502f128886ccb75c2,Architext: Language-Driven Generative Architecture Design,2023.0,3
443c1bef6a7dc3db941375ae76451c884ceffb8a,A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training,2023.0,0
4a8da6b76b6d4396e407973d236c7b7d282d7259,Complement Sparsification: Low-Overhead Model Pruning for Federated Learning,2023.0,0
2c5460afa19ad6fc2568b7e210115acacc14a40c,An Overview on Language Models: Recent Developments and Outlook,2023.0,9
e770bdf8f00beb180a6b6509dd9a09b1bdbaed68,"ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the LexGLUE benchmark",2023.0,8
ee2452dd435c1587678f1f831c8db25501f5ea9f,Word meaning is both categorical and continuous.,2023.0,0
f661bdc053ba13df80eda479791306e1178db235,Magnushammer: A Transformer-based Approach to Premise Selection,2023.0,4
11f558df6f4e3c9da5a4d3a2cedeb54e288cb637,On the Risks of Stealing the Decoding Algorithms of Language Models,2023.0,2
09cc98aaf9cffc174846ffc69fc076c2871df19a,Exploring Efficient-Tuned Learning Audio Representation Method from BriVL,2023.0,0
24cf9a7b11fa4cf945773f3d2b7e0e0f8d2f615d,"Extending the Pre-Training of BLOOM for Improved Support of Traditional Chinese: Models, Methods and Results",2023.0,1
36cd3d3a9e8a64f322476153513ece1fd617acfc,nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models,2023.0,4
41cb6257f262822ff517e2ff5226c283b674caa4,Performance of ChatGPT on the MCAT: The Road to Personalized and Equitable Premedical Learning,2023.0,4
16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6,The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,2023.0,64
d7e00702bbb5a0cccc97033f0405b634ae9e2d3c,Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent,2023.0,3
542905f5fc96bce7572f6ded7f56aedfa62270c1,Understanding plasticity in neural networks,2023.0,8
1462a0e5b7db47301bb0995db56426e1f4a0ac7d,Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers,2023.0,6
1f040c3a8d49f8e54169a0e07013692c7d58de4b,How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,2023.0,18
02eb7a537825c56865eee239bada016f9e5b0901,Learning curves for deep structured Gaussian feature models,2023.0,0
aeeadd43a95ac7af93f6fbf7b165655157a30055,Large Language Models: The Next Frontier for Variable Discovery within Metamorphic Testing?,2023.0,1
430aa6966c15c4a20a4fb2d8383e136b9cb6cde7,Almanac: Retrieval-Augmented Language Models for Clinical Medicine,2023.0,7
6e036e28e7af03bfcdd98ffa254df6644f7657c5,GNOT: A General Neural Operator Transformer for Operator Learning,2023.0,10
932b9fd1e2aaf3c56841304b7a49e30c804f6234,Inseq: An Interpretability Toolkit for Sequence Generation Models,2023.0,13
77e5d0a68afcffb27191572590deced60feb9d5d,Retrieved Sequence Augmentation for Protein Representation Learning,2023.0,1
c23c2c8df918e4f327dbc27c9471a1485360933e,MUX-PLMs: Pre-training Language Models with Data Multiplexing,2023.0,2
6c282567e9c452f81416214933b9b1e45ab3add4,The Dormant Neuron Phenomenon in Deep Reinforcement Learning,2023.0,6
afeef45429c9c349133c88e9069cf979c89c4279,What makes a language easy to deep-learn?,2023.0,0
5be35511f774e09877e4c6ce7a4fd81891db188b,"Phase diagram of training dynamics in deep neural networks: effect of learning rate, depth, and width",2023.0,1
4c4b1bd9f36ccc545cf051efa9d03db2ed045770,Reward Learning as Doubly Nonparametric Bandits: Optimal Design and Scaling Laws,2023.0,0
3ac2d89388a816786234aa9f8ef2de9a635b0a69,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",2023.0,24
9a8645efed4a653023a287d0102f0a41afe5216d,Industrial Policy for Advanced AI: Compute Pricing and the Safety Tax,2023.0,1
2cf43a61d0937ad25f23eaef7c90253ab799b3c7,Poisoning Web-Scale Training Datasets is Practical,2023.0,30
25d4ffc9fb1b320137ea51612ad4fdb1fdfcee19,Harms from Increasingly Agentic Algorithmic Systems,2023.0,14
139ebdcaa3d5f60a3c5289bb61d5865141886d51,Optical Transformers,2023.0,3
9d877509f3fb8fbbf3e3d54eeef3c84bc0e1e3b2,A Simplistic Model of Neural Scaling Laws: Multiperiodic Santa Fe Processes,2023.0,3
2d227f3a0203c2491bcf7c77ae594cea3a7caf89,Cluster-Guided Label Generation in Extreme Multi-Label Classification,2023.0,1
89184ab496b2a1ae31e068e628479b4cd8f4b9d2,Do We Still Need Clinical Language Models?,2023.0,32
eebe95c31aba4b862e1bc3d312ca60094bd01f61,"With Shared Microexponents, A Little Shifting Goes a Long Way",2023.0,2
22e2f488ecd88bd2adf79092d0d390d8f7b06a0f,Auditing large language models: a three-layered approach,2023.0,36
6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552,The Capacity for Moral Self-Correction in Large Language Models,2023.0,42
288c66ee0afaaaa44831bdbce2893453ef74ceda,Over-parametrization via Lifting for Low-rank Matrix Sensing: Conversion of Spurious Solutions to Strict Saddle Points,2023.0,1
629bc57782bb4326a3eb5f89314e350729c5f417,AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models,2023.0,11
05b9bbd578c3f1d52af0fb3c9ac355541ca1c3e8,In Search for a Generalizable Method for Source Free Domain Adaptation,2023.0,6
5b3bbac233a8fd08675fdd511c5cd461ecb47736,ASR Bundestag: A Large-Scale political debate dataset in German,2023.0,0
71ef07a18d1e2127bbe0c3e4db1d16bf386ade9a,Level Generation Through Large Language Models,2023.0,10
101958920c32d67b6509cd849e6644b28d7e473f,Binarized Neural Machine Translation,2023.0,1
feea0452e03b78f7c85f40e5daa1bd08b61bb44b,Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models,2023.0,0
a19c67d9d1d416731930dff76d8c79ff5b8247c9,Population-size-Aware Policy Optimization for Mean-Field Games,2023.0,1
e0401ca2d4fd6d0ed55130a4a24b33ed90111479,Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories,2023.0,3
0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,2023.0,27
7a2a5b8c63bb537678c8be2d654132f52f09398a,Toward Large Kernel Models,2023.0,1
712573cc74633ec2283724e868328fd2d319c091,"Ten Lessons We Have Learned in the New ""Sparseland"": A Short Handbook for Sparse Neural Network Researchers",2023.0,13
7080caab6265c6432a3eeb1328ab64a6e3bd4a08,Multipath agents for modular multitask ML systems,2023.0,1
e962f95e03a50ff2f3a0fe7840daebac04578c46,Structure-informed Language Models Are Protein Designers,2023.0,12
a8f9be0f611af3aea17baa639d556d601be80477,Exploratory Inference Chain: Exploratorily Chaining Multi-hop Inferences with Large Language Models for Question-Answering,2023.0,0
43cefce076df7ee54505fd78a8a97129c0f6d36b,MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism,2023.0,2
94d84d1403a23a8a8486a151f52a126beb16875c,Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks,2023.0,0
f7140c45b6dd9574e9ef16633b4c4dfa7ba71940,Scaling laws for single-agent reinforcement learning,2023.0,6
b369b05f5386ce22dc7fa33c6f912d5c1cd27f14,The Power of External Memory in Increasing Predictive Model Capacity,2023.0,0
24576dcca716c82f66b8cc3c85ecfae18be41edd,Adaptive Computation with Elastic Input Sequence,2023.0,2
fbd49b25bdab98c171af49962a41139c73dacbde,Specializing Smaller Language Models towards Multi-Step Reasoning,2023.0,47
13ae50c61d87c3b9b5f72c64104bb0440483c53f,Alternating Updates for Efficient Transformers,2023.0,0
528bfc811a3b4213566134afe2c880f867be5065,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",2023.0,13
27be451186b3a1b0367591e5aa328eb9065e4e1f,A Closer Look at Few-shot Classification Again,2023.0,1
5278b81db686b4d36143941bff1c683bea963a63,SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient,2023.0,8
43016361a50fc150696a0d73928b5cde1443ae38,Policy-Value Alignment and Robustness in Search-based Multi-Agent Learning,2023.0,0
5882dd04d95c9c88cdec389059fcf44d56cbb789,Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation,2023.0,2
58d6ec0dec4952e93be7cf72c1cebb0216eac9df,Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers,2023.0,2
e1c9da44c7a5d66c802c0f42816a19720842e357,An Experimental Study on Pretraining Transformers from Scratch for IR,2023.0,5
cd44fd81820fc36c6ebac4e6a9a9d848dd854d2b,Safety of self-assembled neuromorphic hardware,2023.0,0
abba9a6f99d877fdd1b8412ddfcc26fdac6163dc,SMART: Self-supervised Multi-task pretrAining with contRol Transformers,2023.0,8
bad6fa523ecf782c837a2eecaaffa4e1f7477c24,Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction,2023.0,7
874deb5f06f35e52ae13a921b23611eec4abd1da,ClimaX: A foundation model for weather and climate,2023.0,44
7ed237af793f43c442b3e8e1bc9ace906a276b2a,Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?,2023.0,4
bfe6fd05f09647b001c7eb6e333a95c881c88344,Human-Timescale Adaptation in an Open-Ended Task Space,2023.0,32
f39d601fdcfc4c029bb33be2898856e6d4673bdb,AutoDDL: Automatic Distributed Deep Learning with Asymptotically Optimal Communication,2023.0,0
c879413103f8950bdd414c7f60a39bd7748c9be8,Prompting Large Language Model for Machine Translation: A Case Study,2023.0,35
9a9e68d400069f023f7dc9b982226c95159a509d,Dissociating language and thought in large language models: a cognitive perspective,2023.0,89
7c0013dff50fe4dbc6af3677324a5852716f121a,"Mephisto: A Framework for Portable, Reproducible, and Iterative Crowdsourcing",2023.0,3
1673b72a4ebb56ae3775131937c40afae3853e9b,Data Distillation: A Survey,2023.0,9
c7fc4c09a18bf0c26d04fc69f5567bfd3ac0c8f6,Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,2023.0,0
468992bf970c37bd1fef58b78a6c2fcd8c018868,Scaling Laws for Generative Mixed-Modal Language Models,2023.0,28
3a82ca5b41eb1e50e78d48c1cf229d2053e774df,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,2023.0,8
6975638e6a7ee1e403b3d76242b3ec7f127d91f6,Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data,2023.0,0
776f3b645743e6daee06f18078598b974812c497,Bayesian Interpolation with Deep Linear Networks,2022.0,6
4b308ba40e67b0b4b25c6fde17195d5a456a2f41,Cramming: Training a Language Model on a Single GPU in One Day,2022.0,19
6052486bc9144dc1730c12bf35323af3792a1fd0,Large language models encode clinical knowledge,2022.0,252
b118283afc5d8652de52cd13a5e287d76c5ec91f,Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,2022.0,13
8ca527976a1e803e10fae88a7ef37c4254af1d0b,Finetuning for Sarcasm Detection with a Pruned Dataset,2022.0,0
77d3fb31d3d0a8929aa5e46aac9910e793b94557,The role of machine learning in HIV risk prediction,2022.0,0
658a8195ca7dc839bf254d4b2eb67c50384c5c6e,Language models generalize beyond natural proteins,2022.0,32
06edda0310b4ec7c5012d012349252a3a77521b6,Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners,2022.0,8
54a4517022703a27e1670d3b84214521882f0108,Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning,2022.0,1
7a5cd8a1bf99c9cc58bd7a818be446c29e9e1cbb,SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning,2022.0,4
8bf77f3f14d20b36a0a4b96693e0a6480f17aac1,HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation,2022.0,7
44c376d9195c10bafd052835c8cc1508b5a8a96a,Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation,2022.0,1
1e03fe3f5d1c1f0e413e368fe42109774ac975e4,Evaluation for Change,2022.0,0
f3b15f3399d26409ce519e522cb0e07d1651731b,Multilingual Sequence-to-Sequence Models for Hebrew NLP,2022.0,1
5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e,Training Trajectories of Language Models Across Scales,2022.0,12
83562af413d730b9321efe8bea24058514ac940b,I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation,2022.0,12
30f60e8a9b2186bf278c9e62299b9d8b3cef44e4,Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?,2022.0,2
3692f4df9d11af68f9b9c9a526667db3f99e552c,Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models,2022.0,10
cef330bacf014d60daabbd489647b2006af130ca,Discovering Language Model Behaviors with Model-Written Evaluations,2022.0,76
736973165f98105fec3729b7db414ae4d80fcbeb,Scalable Diffusion Models with Transformers,2022.0,100
34bc28087e1d6f047e2736791f79d769293f447c,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,2022.0,12
f1b475c6bde80880dd8edfc223c1f20c087201b7,Ethical Issues in Automatic Dialogue Generation for Non-Player Characters in Digital Games,2022.0,0
89d1997bb400534ddae24dfc8456378f108e18a7,Data management in training AI chatbot with personality based on granular computing,2022.0,1
c77404122600594ff2f45ec50d3e36a7eacd30a3,Offline Reinforcement Learning for Visual Navigation,2022.0,7
857ba15bba68934b8a23de8c2c3564bb86a59f22,Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation,2022.0,1
e8aa5f51aaf29344174f90d7edca49cc153a6b00,Economic impacts of AI-augmented R&D,2022.0,1
16de2006e2960ba410772c6b6d460b83c0a5cc4b,Reproducible Scaling Laws for Contrastive Language-Image Learning,2022.0,90
7d5175db1b99552491063d2d9581b0b51e1d2932,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",2022.0,6
8255b71fae79c92d1b3d72caa1e563c80dc36a0b,Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging,2022.0,2
be157d55b4afd5be9c81619d75aa4897f5e201e4,Elixir: Train a Large Language Model on a Small GPU Cluster,2022.0,2
93fdf5cf598aefb0335f001039e83494dc721c3a,General-Purpose In-Context Learning by Meta-Learning Transformers,2022.0,25
dcc062eca9f7665100ac8389258c6975bdde8a27,DP-RAFT: A Differentially Private Recipe for Accelerated Fine-Tuning,2022.0,8
d2edf22af2239f754ea7fa0e044be254161eee70,Deep Incubation: Training Large Models by Divide-and-Conquering,2022.0,3
7557105c9aa6a26db4f8e73fabb25e8134013fb5,Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning,2022.0,1
17a272c5ac1ef01126b2848ae05d0c06162d212b,Pretrained Diffusion Models for Unified Human Motion Synthesis,2022.0,11
2e0caf5cc32390692eac3f9c7bdf5de3b19f7e04,Languages You Know Influence Those You Learn: Impact of Language Characteristics on Multi-Lingual Text-to-Text Transfer,2022.0,2
a2935aa093b7a8c5a562bf406e0cd673aa8135fa,An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws,2022.0,0
7821e7639ffaeea175422f35fae2eb1c095ed1a6,Protein Language Models and Structure Prediction: Connection and Progression,2022.0,15
e99422aa7044c8a42d9b69c833addd7c49800e45,How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective,2022.0,3
2cadb8141d86cacde9b4fc5ca72cc82874022eeb,RAMP: A Flat Nanosecond Optical Network and MPI Operations for Distributed Deep Learning Systems,2022.0,2
9bdad7f737c1303f199384c5b65dc67da7e5c4d8,Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes,2022.0,15
d7a9b3c750c7e5f0c9af3864a796b7fcdc07f030,GPT-Neo for commonsense reasoning-a theoretical and practical lens,2022.0,2
fb49e38135302a1c16d644c0f746cef7d5f10ee4,Understanding BLOOM: An empirical study on diverse NLP tasks,2022.0,0
08c53d66c03b2c00a70fcc2952f90b08cbe94d3b,Receptive Field Refinement for Convolutional Neural Networks Reliably Improves Predictive Performance,2022.0,2
e2df6ae1b3485449364ce2a5356ab09600fc3632,Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism,2022.0,10
4eef626112610d8767d5ded9b288ce0752bdd7aa,Powderworld: A Platform for Understanding Generalization via Rich Task Distributions,2022.0,1
62a45ab7b676f3877d41f66f6c9ddf1ec44a1c5f,GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,2022.0,11
9f4351600c72d5dac0251cd49984f691dca2fcd1,Word-Level Representation From Bytes For Language Modeling,2022.0,0
0b2c1f698b8e7b2d1b15945a53e4245833a048d6,Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of Modulated Cross-Entropy in Natural Language Inference,2022.0,3
be25cd72db91d40d305d9bf870e23a16fda813be,Coreference Resolution through a seq2seq Transition-Based System,2022.0,3
598d9b235f5ab148fc757240d9bc39a47b8eaf72,Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks,2022.0,132
438e9fece0f177cd5655e490c580ae70c757b09b,Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models,2022.0,4
4f4e98cc9133e1814ac2eee9fc4693bf80d1d0d4,Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback,2022.0,12
637337f4a84156ea9eee0ca69a4604e5aa43afee,A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach,2022.0,4
e9f5a58209b34c5c1b1063bd7c7fecea4864334b,Metadata Might Make Language Models Better,2022.0,0
5e52d654fd31f04c1bd884cd5480e6af8c95ad50,Efficient Transformers with Dynamic Token Pooling,2022.0,8
7d645a3fd276918374fd9483fd675c28e46506d1,Galactica: A Large Language Model for Science,2022.0,217
074f9767bf81f04448ce6d997b7e29dc92b10fe7,General intelligence requires rethinking exploration,2022.0,4
4d17732d90440682b0500f4e209c6cc4fac20e0e,Teaching Algorithmic Reasoning via In-context Learning,2022.0,43
78281482c1fdad8e167bab39cc9955c73d58ae8f,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,2022.0,125
e51578b433b7cf7cb92febd4fcd8b78035f08ff9,Breadth-First Pipeline Parallelism,2022.0,0
679d0eede253ae27e40ea1fff5eac592ff7a9562,Striving for data-model efficiency: Identifying data externalities on group performance,2022.0,1
26c80bd65baa90f5b18157de4951f4eb0b62ab69,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,2022.0,110
379e42895f6d40ab9e9559609f505aba89145a5d,Efficiently Scaling Transformer Inference,2022.0,45
964bd39b546f0f6625ff3b9ef1083f797807ef2e,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,2022.0,663
c90a33f1f0049d524e9b5b3174d35611fd9a8096,Pretraining in Deep Reinforcement Learning: A Survey,2022.0,11
bcec7d17e68aceb91d020dd796ece075694f77c6,COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,2022.0,10
d2995cbd50528beac1419e5943c8fd6e4c91ddb3,Harmonizing the object recognition strategies of deep neural networks with humans,2022.0,22
b364def1f651dc7a5d312c6edd5d79b0de8197bb,"Astronomia ex machina: a history, primer and outlook on neural networks in astronomy",2022.0,3
340441f2565b314200dfad29aca208e23077dd58,Rethinking the transfer learning for FCN based polyp segmentation in colonoscopy,2022.0,1
4610ffb1b016acaa82a2065ffd1a3adbae1ce722,Large Language Models Are Human-Level Prompt Engineers,2022.0,159
761699845aa27675c39f027462ef6b03658be2da,Fine-Tuning Language Models via Epistemic Neural Networks,2022.0,2
4809452eb4ed547adaf44a004d47ee910265ba34,Inverse scaling can become U-shaped,2022.0,26
d17bddf9dc329bb9ff7883642699b84055db06fc,PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation,2022.0,4
e24f4b28167b05fbf7d29000490fc0a4e4c109c7,eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers,2022.0,244
b1cd1c4119bddea1b2a2b06779d9aeee12882b9a,There Are Fewer Facts Than Words: Communication With A Growing Complexity,2022.0,0
7df49275071c0ea7b2e7a30df6bd0edd26e6edec,Cloud-Intelligenz und kollektives Lernen für das automatisierte und vernetzte Fahren,2022.0,0
5a5e9bc060c4b6e3aa600b0bea5c5427ab5c94e0,Cloud Intelligence and Collective Learning for Automated and Connected Driving,2022.0,1
de1c66454220eeb93f92b66bacad9d4f7dc8b2ab,Developing a GPT-3-Based Automated Victim for Advance Fee Fraud Disruption,2022.0,2
1a6a7fe065e42365515ccf7d0b5d1225b8088464,STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training,2022.0,3
7ffb3a27a2a4da5c35472bd3a3a4dee8d40a6d86,Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models,2022.0,11
bb15f3727f827a3cb88b5d3ca48415c09b40a88f,What Language Model to Train if You Have One Million GPU Hours?,2022.0,44
e6afdf59d67c84ec1902f709b78e87d4aa8ad189,COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,2022.0,18
f143659c51896efe0aa59c339e591a7bf4bd5e53,"Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks",2022.0,0
e70724ae65ca650008f81ec846a63e1bcfb4e33e,Scaling Laws Beyond Backpropagation,2022.0,1
519b4ff448ee48ba7e08d146a29ade6f019a98d4,Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning,2022.0,26
8a4e2828777c9b3703e8e2b68ac27d9af496261a,"Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models",2022.0,6
f031ba42cf82f106200bb03fbb91dd5671a59b9c,Practical Program Repair in the Era of Large Pre-trained Language Models,2022.0,31
12c20b14f0652c2953dee2e4fc7da59e46744ff4,Learning Better Intent Representations for Financial Open Intent Classification,2022.0,2
31ee24ceedd6dec76836cff99762a2b82acb292f,Precision Machine Learning,2022.0,8
8b1aa3a20638363be3a6d941e907df704c738dfc,The Robustness Limits of SoTA Vision Models to Natural Variation,2022.0,5
311fd5f6f114ae51f8cbd95a0da69d7b556d25f1,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,2022.0,55
4583738cfaf1fd90f33de6cfb0f4bb2693f54fa6,Characterizing Verbatim Short-Term Memory in Neural Language Models,2022.0,2
ed38c6b157c11476939c426ec6871c926f2f3524,Leveraging Large Language Models for Multiple Choice Question Answering,2022.0,31
8f309ff0faa96376253722f9a9cbbb0b7a3f1891,Performance-Efficiency Trade-Offs in Adapting Language Models to Text Classification Tasks,2022.0,0
c10195a628f31315ec56c219e219b25d81bb2b7f,Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale,2022.0,2
c2a2b801e9091c33d50efd8758d3bcc2b05368ff,Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination,2022.0,5
1bb6d5761903c7ac978188ae36e2648905e95dc5,Transcending Scaling Laws with 0.1% Extra Compute,2022.0,31
4bef9d46209ac8988ea5ab83547149760d4af65e,Automatic Document Selection for Efficient Encoder Pretraining,2022.0,2
5484d228bfc50efbac6e86677bc2ec2ee4ede1a6,Scaling Instruction-Finetuned Language Models,2022.0,678
18f7ff46b033abd09a7f3f81237500514cc56556,Measuring the dimensionality of behavior,2022.0,1
4431a39f677fe59b07b3f0cfde7b10f7208cf46c,N-Best Hypotheses Reranking for Text-to-SQL Systems,2022.0,3
fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b,Scaling Laws for Reward Model Overoptimization,2022.0,57
5c02d55fe14e2baf4b6b59a476ee6a20698397ef,"Language Models Understand Us, Poorly",2022.0,2
05bb7f4e44131a1f6a1c3d6b3cefc385a04204cc,A baseline revisited: Pushing the limits of multi-segment models for context-aware translation,2022.0,7
61b54e8702a9e204c722653bdd43de56df92ca13,Optimisation & Generalisation in Networks of Neurons,2022.0,1
663a41c866d49ce052801fbc88947d39764cad29,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,2022.0,155
711d5e8ddbb840ad31a9ffa3d38590603ba69a92,Prompting GPT-3 To Be Reliable,2022.0,55
82cd40e926300b6b18c34ced2edeb07e84d9d6c7,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,2022.0,8
e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9,LAION-5B: An open large-scale dataset for training next generation image-text models,2022.0,683
0cb346ccbd338517789db9abafe9fd93549f7893,A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing,2022.0,4
e02dce6ee032a13b1653f69b034a35676e7d4dc2,Can language representation models think in bets?,2022.0,4
30d0a5a5f8e776c844a37afe3b1ace0b21b24859,Retrospectives on the Embodied AI Workshop,2022.0,25
84e693cfde91eb7acf598b223313365c8dd89485,Spontaneous Emerging Preference in Two-tower Language Model,2022.0,1
9166caa474031b62bacad8a920db8308e6a15120,An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification,2022.0,11
e086d318769c3be0173b572d5e070c226bfb3443,ASRS-CMFS vs. RoBERTa: Comparing Two Pre-Trained Language Models to Predict Anomalies in Aviation Occurrence Reports with a Low Volume of In-Domain Data Available,2022.0,4
b941c4bea1a71066f6f32275641aea1efc99b21b,Meta-Principled Family of Hyperparameter Scaling Strategies,2022.0,4
229efae9f027e39249ca013eba5580dc620e953d,FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings,2022.0,36
6dd4743ee5430157c981a8dfe9a7434d99be2e8b,Understanding HTML with Large Language Models,2022.0,16
f158e70e9ead719e8a524eaf8ec79270574f2eda,How Large Language Models are Transforming Machine-Paraphrase Plagiarism,2022.0,6
bf0d43bf6aac1eed8abe56f0b26d902a1f259fc0,Automatic Discovery of Composite SPMD Partitioning Strategies in PartIR,2022.0,0
3de645f0c1993cd3f3374ad747640a1aa6658a82,Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?,2022.0,12
6a0321b05af7ed549309a05be5f9e335396d3a3d,Generalization Properties of Retrieval-based Models,2022.0,3
6d5555348f453bac901c5b57e8a4eeb3074b4071,Learning to Reason With Relational Abstractions,2022.0,2
fb49e88c6bd676516898e911e42b4f8479e6f1bf,Ask Me Anything: A simple strategy for prompting language models,2022.0,60
257457233bd12a76096f1023935f08b6b0a14465,Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,2022.0,5
9be68112ba98a733fb57f93e753cb3e0fd6f63d9,Fine-Tuning with Differential Privacy Necessitates an Additional Hyperparameter Search,2022.0,13
36a4d5977b6eeb5656f7a8c185159c050411c958,Antibody Representation Learning for Drug Discovery,2022.0,5
55e3fe05598be7c3dd357d51166869f6571b824f,Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,2022.0,1
49b8782d0e715f50c3cd9718180abd2e43e80b7e,Memory in humans and deep language models: Linking hypotheses for model augmentation,2022.0,0
c88cafa3e980765a64febe369ceb7c2aa7261d2a,Complexity-Based Prompting for Multi-Step Reasoning,2022.0,100
7698498dcb14db063154f4c955fc041114d1960d,Single-sequence protein structure prediction using a language model and deep learning,2022.0,101
461630c5126fb457b2396ed5238fff0977e94b5a,DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability,2022.0,5
20b1b783d500d65505c87d7b72b84f2c2ae2ecf9,A Combinatorial Perspective on the Optimization of Shallow ReLU Networks,2022.0,0
891edceb78a274b0c2494d8176bc4d6f6e3f9cbc,Calibrating Sequence likelihood Improves Conditional Language Generation,2022.0,18
8fbd7ddf1ea30c991f3b1152a245df77caa18e16,Learning by Distilling Context,2022.0,20
13d78bde4dc7059ab941871048ffa91d556584c8,Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods,2022.0,5
b65b7f480a61d3dd31d8117b349cabc87c8ccf6c,Bidirectional Language Models Are Also Few-shot Learners,2022.0,14
34e1c62586f0c86af60a6ff2c3e1121c1ebd779a,Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,2022.0,14
8f61a198bf9f97e445267f8b48d459e4bac9a14c,A Multi-Agent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems,2022.0,2
b6408c7dc8ce8c386197990d90ccb528419db25b,Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning,2022.0,2
cbd6478c6aaec64481c6e3463725d8269b268748,The Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling,2022.0,4
d42c50b154dbc41cf869c702f9e4c40fd0cea4da,OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning,2022.0,5
1a133d1fa8a680b4a4944cc6d5c20bfdf963aed5,Local Grammar-Based Coding Revisited,2022.0,0
f67db2aa37b2093fe40c1bb48fbe3626e565c2f4,SpeedLimit: Neural Architecture Search for Quantized Transformer Models,2022.0,0
bf72cfa87a5c55e430abf6d2a3d9b66eb9e1a717,Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,2022.0,9
25b6c9a11d9078d2cc45e02e284195320ce61f0f,Variational Open-Domain Question Answering,2022.0,2
e194f641c554cb00cdd4bd6993f14c2dff8c3c03,Efficient Few-Shot Learning Without Prompts,2022.0,61
b2542a738b75ee9b7ce1a13d8b78f9095d212412,Generate rather than Retrieve: Large Language Models are Strong Context Generators,2022.0,73
205eab69e430b4da93ecf3fd9115f0919b448040,WeLM: A Well-Read Pre-trained Language Model for Chinese,2022.0,6
cf49af82a5f1ba5f2beba9f290e684b7b51b64e6,Extremely Simple Activation Shaping for Out-of-Distribution Detection,2022.0,17
cbeb03802b00024bf3d43b63814241e5261abe93,Relaxed Attention for Transformer Models,2022.0,2
33fd110d1e4ca5f91d1b7ca7ff24ce1e9335359e,Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics,2022.0,14
818861b82fae7e41a6e4d599c63ecc9a18822a7d,"Can we do that simpler? Simple, Efficient, High-Quality Evaluation Metrics for NLG",2022.0,1
f5d73457d05d4b4da3905fde99d104e4b744a032,Textual Datasets For Portuguese-Brazilian Language Models,2022.0,1
ddf0de2a4e0f707446c7463ea63cfdc08d59d4ca,Rapid classification of local seismic events using machine learning,2022.0,0
fe828d8dc632780c10b49ed67cc74a526076b966,Ising models of deep neural networks,2022.0,0
ed9b3de9d3b99bcd004f853c3696dd4916bb794d,Current sequence-based models capture gene expression determinants in promoters but mostly ignore distal enhancers,2022.0,20
5860267c5b1761fa5ff8dbcde980edff4c3b2f2a,Part-Based Models Improve Adversarial Robustness,2022.0,5
ceece2488eef8fa427cf4fee60e6cfb1155ff15e,A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems,2022.0,6
28630034bb29760df01ab033b743e30b37f336ae,PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022.0,204
0d4af5dd9b1212c9f6a78b18935fc1c1187c2d73,Language Chameleon: Transformation analysis between languages using Cross-lingual Post-training based on Pre-trained language models,2022.0,0
f71778a1cb584cb1272c292a152d4bcd0a8ae6e3,PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization,2022.0,0
9a5df5b7f2b42ffccb6b52c20547850cadfa0fe2,Enabling Connectivity for Automated Mobility: A Novel MQTT-based Interface Evaluated in a 5G Case Study on Edge-Cloud Lidar Object Detection,2022.0,2
0b5f3259d393e0e25469c92d7fa44f8cec35d170,Mimose: An Input-Aware Checkpointing Planner for Efficient Training on GPU,2022.0,3
0afc96eca8b94d19a4c98fdd78e5fd9c68f6859a,Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision,2022.0,2
6e02a7eedad079451b9a8dd358268727cf599c6e,Do Large Language Models know what humans know?,2022.0,14
ca086f4c09cf8de705830ac2b70951737fab93ca,A Review of Sparse Expert Models in Deep Learning,2022.0,41
1ad7a323cebbd186a6f3c9a1f0caf0af94ce91bd,HammingMesh: A Network Topology for Large-Scale Deep Learning,2022.0,5
dbeee6ac35ed73eb6ffb16c6201502369b6ae42b,Masked Sinogram Model with Transformer for ill-Posed Computed Tomography Reconstruction: a Preliminary Study,2022.0,0
6c673f92ac808d9ccbaf3fc35e8da9cd66caf847,Petals: Collaborative Inference and Fine-tuning of Large Models,2022.0,13
c2efc25fa1a2b0192b95dc92d9dccf90e602c74c,Efficient Methods for Natural Language Processing: A Survey,2022.0,29
18a831422a4b4c89bbf1cc4baaa2cfcbf29daaf1,Exploring and evaluating personalized models for code generation,2022.0,1
966ccb741d4e8d92db931852f2d9480fb5c497a0,Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment,2022.0,12
e66384bfc81d0c9357ce5e692a1b42a30bbe48e1,Adaptive and Efficient GPU Time Sharing for Hyperparameter Tuning in Cloud,2022.0,1
cdbfd6201296db1f7e82421c0de34f12fd5294e9,On Reality and the Limits of Language Data: Aligning LLMs with Human Norms,2022.0,1
74d700b7df1f3fff7a145e47250310035d845c4d,Enhancements to Neural Language Model for Generating System Configuration Code: A Study with Maven Dependency,2022.0,0
acba7ea5905901944027b1bab14d5c6059edf062,Federated Select: A Primitive for Communication- and Memory-Efficient Federated Learning,2022.0,7
608547338e096bfdf1db170ada341c28f1b7462a,Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training,2022.0,2
93f9d29445a1236c0b1ab45026c2e308b9b74c15,Understanding Scaling Laws for Recommendation Models,2022.0,8
69e9fcc1b85732d822cc0f02fb2942c0fdd74dba,Self-Supervised Learning for Scene Classification in Remote Sensing: Current State of the Art and Perspectives,2022.0,11
e221c769957a8eef2836a64cede88401129d86e9,Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP,2022.0,30
2d3b6058370b804009fd7866098f4fca2d1894ca,Reducing Retraining by Recycling Parameter-Efficient Prompts,2022.0,5
3b39efe6c91ae432dd35bb79431edb8a6719f906,Investigating Efficiently Extending Transformers for Long Input Summarization,2022.0,20
ac774b892a67fd0eccdf92e94e2bcb1402cc037a,Now What Sequence? Pre-trained Ensembles for Bayesian Optimization of Protein Sequences,2022.0,6
398e4061dde8f5c80606869cebfa2031de7b5b74,Few-shot Learning with Retrieval Augmented Language Models,2022.0,174
2a672342035defd8d75b54e08597ef124c6a0172,Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models,2022.0,1
05816d1476719e04084ee03f6a1bbfb68cdfa8e6,Joint Data Collection and Resource Allocation for Distributed Machine Learning at the Edge,2022.0,4
076deb54a5d776cd21eabf2c40cdd839f53d6d77,giMLPs: Gate with Inhibition Mechanism in MLPs,2022.0,0
ec4c8d99eb1c028c43af6d8bbf727392d351cb59,Efficient Training of Language Models to Fill in the Middle,2022.0,58
2c709ef6186bd607494a3344c903552ea500e449,Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks,2022.0,38
0d975a8bbc1e0495cb95df8666d42111b546ab34,Retrieval-Augmented Transformer for Image Captioning,2022.0,20
39638f73ecd4080592d9fd9270fd7aa25df02f63,Dive into Big Model Training,2022.0,1
cf25fc0d063eb653de708267e95e0f1d6f5ef4e0,Training philosopher engineers for better AI,2022.0,0
6edccbd83a9aae204785d4821f97855677c33866,Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,2022.0,41
0369ea9a84d230007ccae2d772ee7b1e3d0f7c36,Comparative Study of Repertoire Classification Methods Reveals Data Efficiency of k -mer Feature Extraction,2022.0,3
0de905634bd3d42a612ffab7ddd7a814e7e655bb,Benchmarking Transformers-based models on French Spoken Language Understanding tasks,2022.0,1
3215fb4ca5ffa36333245c35b133d452824e7837,Towards Learning Causal Representations of Technical Word Embeddings for Smart Troubleshooting,2022.0,0
d697b440dd0e65a05fe027e4c0ea85f62dcba033,Can large language models reason about medical questions?,2022.0,81
00bd301992aaf5b27beb1f6a4edded32a3c635e8,Generative Models of Brain Dynamics,2022.0,9
1d91bc3979e87380c31ac8aa152919228a639a04,Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,2022.0,11
8abc6b3246ea821efa862735359313bd220f39e8,u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,2022.0,11
44f6612c238297304331d6fe6aa4b4f909f1c6f0,Wayformer: Motion Forecasting via Simple & Efficient Attention Networks,2022.0,52
f75425ef1884a7bc8d367788aef111b242cd540b,Embedding Recycling for Language Models,2022.0,3
5607f81dd171defdb8e76506fd3ea8869d92e415,Where is the mind within the brain? Transient selection of subnetworks by metabotropic receptors and G protein-gated ion channels,2022.0,2
5b984766416a4c881f7acdfcd3ddf983b6ff9333,"Synergy and Symmetry in Deep Learning: Interactions between the Data, Model, and Inference Algorithm",2022.0,7
605ce1aa8886eecb53bdb29fca60f1fe5ff7fd8c,Mechanisms that Incentivize Data Sharing in Federated Learning,2022.0,15
cb6ed5fe96f6c04ddaace1d141226091a8192601,Training Transformers Together,2022.0,4
c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617,Neural Networks and the Chomsky Hierarchy,2022.0,35
041edc8b14bdd0e5627377956fd0e6c6c011146a,Machine Learning Model Sizes and the Parameter Gap,2022.0,9
4ce6d229d5f44239c948fd56ad744c012aae22e0,Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,2022.0,10
14efaa989b8db89ad907e6a15253d482cbba77d9,Linguistically inspired roadmap for building biologically reliable protein language models,2022.0,8
051c2cfb399d71dfc91459cad5be404ee4568f9d,When Does Differentially Private Learning Not Suffer in High Dimensions?,2022.0,26
70e8b89f4dde78113a1c3f3a2359a6af702e38f0,Evaluation Methods for Representation Learning: A Survey,2022.0,2
2a1493eee51f07be435dd02d0de9ca48a57fa79f,Knowledge Distillation of Transformer-based Language Models Revisited,2022.0,3
8dd49f94b213834e580eeea18098f402b1be1226,Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers,2022.0,7
26133033149afb4b45e5d0a4bd1dc712a236810e,ProGen2: Exploring the Boundaries of Protein Language Models,2022.0,68
b9c004a61ba7db8e4e576b95048c16bbb61b103d,Bayesian Optimization Over Iterative Learners with Structured Responses: A Budget-aware Planning Approach,2022.0,1
65fc1f1c567801fee3788974e753cdbf934f07e9,Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos,2022.0,97
374bbb716aa007a65ac03f0220d3027fa724874d,AI Challenges for Society and Ethics,2022.0,3
5eeb828685e44ca5b8ebafb34a9fa4d51c9186df,LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models,2022.0,8
dac3a172b504f4e33c029655e9befb3386e5f63a,Emergent Abilities of Large Language Models,2022.0,744
daa3af99c6421a60e4dc06cb27fc97a60a1aa54b,Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems,2022.0,13
9b155c244ed21edc44594ffd97b4e06fc1d8908b,Etude comparative de modèles Transformers en compréhension de la parole en Français,2022.0,0
0edbcf40a5d8f001b1542c4adfaf3303c7216cb9,Mediators: Conversational Agents Explaining NLP Model Behavior,2022.0,9
29acc890e521f7a6415666ab9eb3432c49b4587a,Self-critiquing models for assisting human evaluators,2022.0,68
8409243143193a7533f1a6cef05181b1d1849d20,Ancestor-to-Creole Transfer is Not a Walk in the Park,2022.0,3
44791ef3cf02861b9ce085af632f84546d329c57,Trajectory-dependent Generalization Bounds for Deep Neural Networks via Fractional Brownian Motion,2022.0,1
5275611d030f4280edecec60834a2a13abdb03da,Improving the Accuracy of Progress Indication for Constructing Deep Learning Models,2022.0,2
2e700ff36108119f5ed19a53bd2eaa22b42ec3d8,Tutel: Adaptive Mixture-of-Experts at Scale,2022.0,21
59f91478c1f63b0fb5628bbd1af8267792708443,Intra-agent speech permits zero-shot task acquisition,2022.0,8
65a4723b9980d99b1fdd1e03eba69953cd6af958,Graph Attention Neural Network Distributed Model Training,2022.0,0
7574ac431d9b1f3ef5996746817a4ae7cad94ad3,Learning to Ask Like a Physician,2022.0,7
7ffb212356df9980347b3d3b9910dfba75a5d0c7,"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code",2022.0,30
511e6559df79b5b7cc3fa69ae31ef1c3badce048,When does return-conditioned supervised learning work for offline reinforcement learning?,2022.0,27
d852c5928774e110c8cb700fa2e457904eb4b0fe,Dynaformer: A Deep Learning Model for Ageing-aware Battery Discharge Prediction,2022.0,6
9431759eb9845eab9731febf802754b3d4c6d381,Does Federated Dropout actually work?,2022.0,8
1be5064847c7a5e9a7e516e4c21aa8f4f8ab2eb5,Multi-Game Decision Transformers,2022.0,89
1ab3a007cf4819cc740a21a3140cb0ef3d7c132c,Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression,2022.0,9
962fad98095f5a5e34ba5354557337ec8637a10d,A Blessing of Dimensionality in Membership Inference through Regularization,2022.0,4
5c4f8de98525eebd762773093d149ba459cef290,Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation,2022.0,77
ea9c21c66f970a6bcc6750fc7d6ff6fa3c7fd301,Can Foundation Models Help Us Achieve Perfect Secrecy?,2022.0,3
832aa77be74d7d3a8b33b9abc0f48b8b7babac12,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,2022.0,58
c9866e126cb70229e2017087f84b5db446f25a20,An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems,2022.0,15
4a4581003f56e8cb581ad6f383c037964765d3d5,Active Programming by Example with a Natural Language Prior,2022.0,2
011095a0082e5e301f9bf30267b193c1c9e7e370,Perturbation Augmentation for Fairer NLP,2022.0,31
744e68099831f743aec5e0224a79fec784a61ff1,Semi-Parametric Inducing Point Networks and Neural Processes,2022.0,0
7618f17179bb316002cb6cc472d61382776af6b7,Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT,2022.0,3
9ebd20a621ea9f458d4919c8e247932ddc7664fd,The Diminishing Returns of Masked Language Models to Science,2022.0,4
bc981df87edaae95104007f48872b30d16d07d60,CNNs Avoid the Curse of Dimensionality by Learning on Patches,2022.0,2
4b516216d7d150a081fd74993bddf36b6b22c118,Chain of Thought Imitation with Procedure Cloning,2022.0,8
a005a6160efa2fd055581b8222b41f71f966ea50,Life after BERT: What do Other Muppets Understand about Language?,2022.0,3
aa4d9972af3264d032dbee58501ed4ac49477103,Scaling Laws and Interpretability of Learning from Repeated Data,2022.0,28
58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b,Can Foundation Models Wrangle Your Data?,2022.0,46
ffabceeeab1e6298b3633ce5a0117fe4f4fd0270,SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System,2022.0,8
50090a9c51e276e8c6331e508f161a2516224f65,Single-shot optical neural network,2022.0,11
4853183081cb85284859ff0cc97273116a57a345,The games we play: critical complexity improves machine learning,2022.0,5
5922f437512158970c417f4413bface021df5f78,A Generalist Agent,2022.0,360
7cdaa08890895e1ad92afb5fad429690ad7b1dac,Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning,2022.0,172
6c790c1a4233f5201b052b149a3c6c76261aee68,RITA: a Study on Scaling Up Generative Protein Sequence Models,2022.0,31
b21670e8061a06ab97e7d6052c9345a326e84ff8,UL2: Unifying Language Learning Paradigms,2022.0,92
4ce22cb5747db637e242e2dc74536a13b185e4aa,Optimizing Mixture of Experts using Dynamic Recompilations,2022.0,2
c7d7113e0d7d306953fbda7409a21e5948ee6fbf,An Exploration of Consistency Learning with Data Augmentation,2022.0,1
90c7ea8273612b2fa12a294f171863e30d879505,Adaptable Adapters,2022.0,9
7ab0d76a2a0ba7491f06a6cd2d264b8f50037305,Jack and Masters of All Trades: One-Pass Learning of a Set of Model Sets from Foundation Models,2022.0,2
561c74d1b27efc2fbfccfae6985ae70e97c68687,Triangular Dropout: Variable Network Width without Retraining,2022.0,1
e47da75675b9a3fe02ef1efadca39bc8cdfcdc17,Designing Effective Sparse Expert Models,2022.0,54
ee18ec08a0cdfd23e8cb85bb9c2421b88453e314,Artificial Intelligence Based on Machine Learning in Pharmacovigilance: A Scoping Review,2022.0,14
6a684417089eb6f07ccb1ac5391c63ebacef821d,Can OpenAI's Codex Fix Bugs?: An evaluation on QuixBugs,2022.0,37
ba377c911b1d601cbe6a7f10f89e5b6175adcd98,TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting,2022.0,10
5cbdaa4a011044fa9d341f8b743d301dcc84ceec,Continual Learning with Foundation Models: An Empirical Study of Latent Replay,2022.0,16
f7820b52e3cdff6625e6bd0430a8d48ca66cca3f,Self-Programming Artificial Intelligence Using Code-Generating Language Models,2022.0,1
26218bdcc3945c7edae7aa2adbfba4cd820a2df3,Flamingo: a Visual Language Model for Few-Shot Learning,2022.0,880
1fafaccebc4a74898a74c606f846318c4c2c7536,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,2022.0,35
9f96d8b71551e735171283976ce2160e0b88e824,Can deep learning match the efficiency of human visual long-term memory in storing object details?,2022.0,0
3b9b1aba877ecd3f7e508cbc78a41b623349902b,Translation between Molecules and Natural Language,2022.0,39
0221d3f45899e226ba7840ca9b19117de3a394bc,Semi-Parametric Neural Image Synthesis,2022.0,15
e33dbc93f454b856100332b2f5427e027b356bc4,DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation,2022.0,2
0f071ec7a34b25d60b5f122e5e755213771bf7f9,EHR foundation models improve robustness in the presence of temporal distribution shift,2022.0,9
2fd1f995210eabaee50f21d684b1c341d40448e3,Active Learning Helps Pretrained Models Learn the Intended Task,2022.0,19
e37018d3cfab9cfc29a7b78404e6c86ea18a907e,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,2022.0,302
b6ec1e8f18185b4b3d46201359a440404575460c,METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals,2022.0,21
5288b9f3a9f575543f44c39e1d3b78b3ca4c99da,InCoder: A Generative Model for Code Infilling and Synthesis,2022.0,212
408efdd599b2b27ecb95a4d799869c9ff568fb31,ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension,2022.0,44
c69f9a5185b4c29525bedb2dcc79d20b42c14cc6,TRUE: Re-evaluating Factual Consistency Evaluation,2022.0,66
094ff971d6a8b8ff870946c9b3ce5aa173617bfb,PaLM: Scaling Language Modeling with Pathways,2022.0,2038
b83eb34b088b31118974f33109ce53a32209d73a,"Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review",2022.0,12
341bdbcfc3febef7691a97c216ad394653211095,Can language models learn from explanations in context?,2022.0,129
8326dba15f6b8ee6e43c23eea3265a05e59e8135,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,2022.0,34
02be347cd94a5c3596003d98ccaf66831b048df5,Scaling Language Model Size in Cross-Device Federated Learning,2022.0,11
8342b592fe238f3d230e4959b06fd10153c45db1,Training Compute-Optimal Large Language Models,2022.0,681
54b8bc5be8bbffae333dd73f2cb9d93a492d438e,HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System,2022.0,11
1b261d347181af289e950354db6e0095e84e28b2,Vision Transformer Compression with Structured Pruning and Low Rank Approximation,2022.0,2
43fee02f5e63dcc85c58d74966951e89b2331489,"Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking",2022.0,0
2c6230fd6c474790d3a3199b8491bef0d54f8fd3,WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks,2022.0,14
3cc1ba2e3cd14857f843d9c3b955fa1fb333527a,EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training,2022.0,34
03013e291fb3192b286147f5bdb5770e434f91b2,Do Language Models Plagiarize?,2022.0,16
3938e017cf711cec6f91d072c5f7a0795c3f11c0,Prospectively validated disease-agnostic predictive medicine with hybrid AI,2022.0,0
daade6d1d7aa018e42d3acabff1e83380d5d0f07,Does Corpus Quality Really Matter for Low-Resource Languages?,2022.0,9
86febec5ac647fe4e5aab1284f913684ccf70845,"Improving Macroeconomic Model Validity and Forecasting Performance with Pooled Country Data using Structural, Reduced Form, and Neural Network Model",2022.0,0
3330175709ceb69cb2358053e72b9afc49723657,Towards Personalized Intelligence at Scale,2022.0,0
bc7984bfcfae537dbe633eeeb8d69c42a994c724,ELLE: Efficient Lifelong Pre-training for Emerging Data,2022.0,29
1098ca3dbda5778c2bf6c9e8cbb9bc7a02249e10,Staged Training for Transformer Language Models,2022.0,12
ad9effaa6c12330f24e34bd3fa5d1496acc113af,More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize,2022.0,26
436278ce3b85265dc5cface29e63c714fe979d23,LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models,2022.0,2
7fad3f050e8a3fdb00213c919b433b904fff8dfe,Distributed Methods with Absolute Compression and Error Compensation,2022.0,3
024dbd9cf7f9c605cc3a99b35b578ce24993d32c,Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale,2022.0,24
68de81265a2ef91f838b00a9bd39e2ab53cb63fb,COMPASS: Contrastive Multimodal Pretraining for Autonomous Systems,2022.0,6
a86d04a60bfdd196f217b2f4a01ef08a8c83896d,DataMUX: Data Multiplexing for Neural Networks,2022.0,5
6bcdf260d7927d8fe4ff030c20ee1db974d0c969,Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?,2022.0,17
bbc57e1b3cf90e09b64377f13de455793bc81ad5,Mixture-of-Experts with Expert Choice Routing,2022.0,47
a45582e75f326fdb668e164d6d06a365e4bcc6b5,Geometric Regularization from Overparameterization,2022.0,0
1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3,ST-MoE: Designing Stable and Transferable Sparse Expert Models,2022.0,32
9f1753227e7dbbc3b79dec494982e96f1604b98e,Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness,2022.0,17
92f7056e93c4a78eeaaa6c644ebd1edacdff1ee4,MUCE: a multilingual use case model extractor using GPT-3,2022.0,4
e8f170e3eee1fce6c9e9d6ca7b7f56f97a40e02b,CAREER: Transfer Learning for Economic Prediction of Labor Sequence Data,2022.0,0
44d6c201a4056e260e9844bdbb01461ea9b1a011,A data-driven approach for learning to control computers,2022.0,24
802a5d24c78f713e282b003d99b4afd924bd7568,A Survey on Dynamic Neural Networks for Natural Language Processing,2022.0,13
ebd042387a2dde05ab18b23820030841bb671966,Lie Point Symmetry Data Augmentation for Neural PDE Solvers,2022.0,21
9d40837175577bb0009b138269b422f6d5820d00,Transformer Memory as a Differentiable Search Index,2022.0,89
a8d671773daa3bf07329ef76f0be110d6b7a646d,Pruning Adapters with Lottery Ticket,2022.0,1
62d17b6f6ad77fd71ef9954c7784700d5e316f1f,What Does it Mean for a Language Model to Preserve Privacy?,2022.0,56
e468f74ebffa8bbdd99bf8d0233822a1d2a9b430,Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets,2022.0,20
5cbe278b65a81602a864184bbca37de91448a5f5,Competition-level code generation with AlphaCode,2022.0,439
627fa6970101e2b532745dc9e11cf1850f9fc205,Adaptive Fine-Tuning of Transformer-Based Language Models for Named Entity Recognition,2022.0,0
916a06a6d51aa93de27aac2f3e14faed08dd6706,Formal Mathematics Statement Curriculum Learning,2022.0,38
f74b387136d1d57a791c6b7c9d823577a47516aa,Harmony: Overcoming the hurdles of GPU memory capacity to train massive DNN models on commodity servers,2022.0,5
f70f9f9c0187fc830081124f984560057891dad7,Nonlinear Initialization Methods for Low-Rank Neural Networks,2022.0,1
0d564d688e4e25bf640adf46387f0baf31beefbb,Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification,2022.0,2
1415479215dcfec5e2ee0d33f1e1565ae2c65bb9,Examining Scaling and Transfer of Language Model Architectures for Machine Translation,2022.0,8
55ad5e818cfed72317576027fb33a9609210d592,Training and Evaluating a Jupyter Notebook Data Science Assistant,2022.0,18
24efea3318ec208063ede7f8422696743b2f924d,ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise,2022.0,0
1b6e810ce0afd0dd093f789d2b2742d047e316d5,Chain of Thought Prompting Elicits Reasoning in Large Language Models,2022.0,1831
aff09088b8953a35d20938556eb9070e4d693d39,A Large and Diverse Arabic Corpus for Language Modeling,2022.0,0
b3848d32f7294ec708627897833c4097eb4d8778,LaMDA: Language Models for Dialog Applications,2022.0,767
c783e1fb3ce8514f981925ee590c00884660ee4e,CM3: A Causal Masked Multimodal Model of the Internet,2022.0,86
7d1e512888a2fa4e838c12a02ae7fce867d322a8,DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,2022.0,81
0c181f508ec9de8e48f62523ba8a9bcb1f51f83a,Pretrained Language Models for Text Generation: A Survey,2022.0,0
10b9ca173c665e3f2c322c2d5ce9b9d433fe4629,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,2022.0,38
400d619cbabeb669115bb7281a889ab869829ef5,MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound,2022.0,100
6dabb2efbe33ef7f7bfa39be57496a0a7e80568e,Complexity from Adaptive-Symmetries Breaking: Global Minima in the Statistical Mechanics of Deep Neural Networks,2022.0,0
cf4f4b69b76dc58dc8c0d443ab88ceb461eec719,EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate,2021.0,6
b6270cc18c5a992c30275364b3ebf2f5f6643d12,Generative Models of Brain Dynamics -- A review,2021.0,4
42eddd30bad406ffbd7472bbc467e1446c65b3d6,"Toward a Computational Neuroethology of Vocal Communication: From Bioacoustics to Neurophysiology, Emerging Tools and Future Directions",2021.0,9
c0a98fca99e5f2d34330a15ab8ad4b9d692146d0,The growing cost of deep learning for source code,2021.0,16
0b483b550b21ec42d693fc04a372dbb10dd07019,Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge,2021.0,3
4c70bdfe7bbdf4899a50e25f6fdaff0d7ee97ac1,Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer,2021.0,13
5b0cfef3ebb8f709a4cf2e4718e8440cc5890bab,Efficient Hierarchical Domain Adaptation for Pretrained Language Models,2021.0,24
5ebc677cb284606192b827ffa7a8578542896686,Can Multilinguality benefit Non-autoregressive Machine Translation?,2021.0,1
3dfb1f50f2a34a699c339dabaa6f9b3a977973de,LongT5: Efficient Text-To-Text Transformer for Long Sequences,2021.0,119
32881086e4f0d97a81b0f9c89b59384040b1d02d,LMTurk: Few-Shot Learners as Crowdsourcing Workers,2021.0,8
503fc0edd883e8ddaa6c542eb47c45669c276864,Epigenomic language models powered by Cerebras,2021.0,5
80d3f14c95a1c187b5ba17b3a326a611874a3996,Computational bioacoustics with deep learning: a review and roadmap,2021.0,83
f57bdceddfffd82951f7942f8f936bf8c3ead446,Eigenspace Restructuring: a Principle of Space and Frequency in Neural Networks,2021.0,16
002c256d30d6be4b23d365a8de8ae0e67e4c9641,Improving language models by retrieving from trillions of tokens,2021.0,356
d1a082fb5e84420c68ecb4bcdfa029d7771f2d02,A Transferable Approach for Partitioning Machine Learning Models on Multi-Chip-Modules,2021.0,2
5a980aa843e40de5f91a243cbf680af273c797ba,Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning,2021.0,35
64c8350a66ada14ecc0baa3446c75884cc27e3dd,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,2021.0,38
ee042a3e299a32c413532e64603de8d3ddb6aa87,Automap: Towards Ergonomic Automated Parallelism for ML Models,2021.0,9
e5e8d196e1b857bd1b23ace648e695e41a45863b,Co-domain Symmetry for Complex-Valued Deep Learning,2021.0,7
82edbb92d0f6952224c5aa0aff264b44b8fb4e98,Toward Foundation Models for Earth Monitoring: Proposal for a Climate Change Benchmark,2021.0,12
0290d7d28892c2f04404985e37cda69322e6a74b,Investigation of Training Label Error Impact on RNN-T,2021.0,4
90b21dbad8969b74d704eed15a3d98722a88e464,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,2021.0,39
ce608300992225fd1dba6229adbee51341dd2332,Emotions as Abstract Evaluation Criteria in Biological and Artificial Intelligences,2021.0,0
3ea60cbce6c9065661d207fccf021c5d58a83f01,Scaling Up Vision-Language Pretraining for Image Captioning,2021.0,132
da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed,Sparse is Enough in Scaling Transformers,2021.0,48
02bba282dfbcee7fb549d87ce6d23ce9ac784f25,DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning,2021.0,0
cbf98ebe967e0f3f3236e7932f37013b98244e94,ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning,2021.0,137
04db9b694280134f09af5fa787a306907edba29d,How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,2021.0,40
06b20a1c6883464fcb2855adc146874fe7937c41,Merging Models with Fisher-Weighted Averaging,2021.0,67
be0fbb810583930c071d0b9b2c5187fe260783f5,Swin Transformer V2: Scaling Up Capacity and Resolution,2021.0,593
9a258f42e333ed5ff79037724eb01747ede0bb49,Few-Shot Self-Rationalization with Natural Language Prompts,2021.0,54
7567744a0e23174166575e8d98590967684696b4,Scaling Law for Recommendation Models: Towards General-purpose User Representations,2021.0,16
21e76679cc64c119347997f21c70e33b540d9d3e,Tensor network to learn the wavefunction of data,2021.0,5
9e4f5629355d2fe5febae6ff78d239cff053ea02,Information geometry for multiparameter models: new perspectives on the origin of simplicity,2021.0,16
6c15605b4b77f970975757a875d349ba240f4caf,Scaling ASR Improves Zero and Few Shot Learning,2021.0,15
c109bf9c97536b6ba3caac73cf94d195f16336c0,DP-REC: Private & Communication-Efficient Federated Learning,2021.0,8
1444536496d8064f33e10b38b5820fecfab5b367,Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs,2021.0,29
b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df,LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs,2021.0,500
f74ec348bcfb0c3cf65a1e8ddb3e64374a8c7e9b,Learning Size and Shape of Calabi-Yau Spaces,2021.0,25
c23d9d44e8bc68408cea9f305d1f24d915bc0d0d,Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey,2021.0,119
96716a1fe0815b3c3f05c9421dc39515b44e8dd6,Comprehensive analysis of embeddings and pre-training in NLP,2021.0,11
c09ebcb1ca6ad1eced57340f3e81e456416ed185,A Systematic Investigation of Commonsense Knowledge in Large Language Models,2021.0,16
e88dcda819baa14d7392c0d5d1b9d6996e87016b,Towards Language Modelling in the Speech Domain Using Sub-word Linguistic Units,2021.0,1
3186b9dd1331b647cf3304d185c248ea7ec9ad1b,OneFlow: Redesign the Distributed Deep Learning Framework from Scratch,2021.0,18
d6045d2ccc9c09ca1671348de86d07da6bc28eea,Training Verifiers to Solve Math Word Problems,2021.0,516
865bc9d894a55f3e77ea29f11691f8bf550571d5,Emerging trends: A gentle introduction to fine-tuning,2021.0,15
66d735987a31d666a6459566ae026c40ab9a1c3a,The Efficiency Misnomer,2021.0,66
aace5f303546288dc1c5c3dbaadc12a92d78e891,Practical Galaxy Morphology Tools from Deep Supervised Representation Learning,2021.0,11
9019f5da4af44b66b7e0d19e6177baf6d5071dc8,No News is Good News: A Critique of the One Billion Word Benchmark,2021.0,3
f309fd989473d73ff9a4d7b3baf20148214b4e53,Identifying and Benchmarking Natural Out-of-Context Prediction Problems,2021.0,2
d3ac65b10af091759863b8e2c488036bf52a2ce6,DAG Card is the new Model Card,2021.0,5
9cb47b4063de9ebba86881fbe8774362e7580623,"Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model",2021.0,7
74ee70e3ecbb2a7123a14de75bee7d3d8514f1cb,Wide Neural Networks Forget Less Catastrophically,2021.0,30
1520b70861275c13331069acbae555984950acac,Intelligent host engineering for metabolic flux optimisation in biotechnology,2021.0,6
f18ace4af609cf284325b65735aa97e8e0499172,Behavioral Experiments for Understanding Catastrophic Forgetting,2021.0,3
a0b777b25cdf0fc992568ca52a5c7bebf1ee987f,Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research,2021.0,7
e968a3c9590481cd13f2f86a7ac8839e3cf3455f,Improving Compositional Generalization with Self-Training for Data-to-Text Generation,2021.0,13
92bc69500c16fce47bcfd06ada14ffc4a7e8ddca,PAGnol: An Extra-Large French Generative Model,2021.0,3
11ccb9b509d84e845901ee097e7d0a6419fdc182,EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models,2021.0,3
6bd91a3183ddb844641acb9f3fe9faec6a9ff617,Meta-learning via Language Model In-context Tuning,2021.0,61
c5fbf9a62a91e3182f65e3746d3263387effa4a7,The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail,2021.0,31
d83c7aa12420d5d035f43d7737bfa6893e9e2c61,Exploring Universal Intrinsic Task Subspace via Prompt Tuning,2021.0,6
683de10cda9cd3b709983db2da67f1f059fa516a,Towards More Effective and Economic Sparsely-Activated Model,2021.0,5
f9c0f78521e5bf4a38f2fd9c417dfc9c54950af1,Automated Essay Scoring Using Transformer Models,2021.0,13
9c392c7d79a28f6b6825c0193f1d1695ad1e73b5,Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers,2021.0,6
6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd,Large Language Models Can Be Strong Differentially Private Learners,2021.0,154
55d1133ec3bd8851dc0172cf7454063872a11898,Unsupervised Neural Machine Translation with Generative Language Models Only,2021.0,19
e588a46a86e6548f4bb57b0ce2b53b98889759bb,The Evolutionary Dynamics of the Artificial Intelligence Ecosystem,2021.0,24
101904869d6f2a4fe56838b3eb96f0fb4c4f9ca6,Temporal Convolutions for Multi-Step Quadrotor Motion Prediction,2021.0,1
24e775b20adf21e9b5b95c6a9b7a5c164d055849,M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,2021.0,26
bfa385a782d7fce8b1951afa8fdee56d070e8453,A General Method for Transferring Explicit Knowledge into Language Model Pretraining,2021.0,1
79f95d13d8430b2e31cb8a8104c0455c6995a259,The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks,2021.0,10
11fe37ab6faf6bf85ad2f5746c154dec5412bd04,8-bit Optimizers via Block-wise Quantization,2021.0,67
c206a6e7f51f5e1b6bfc479a174b66ad88ada2db,Exploring the Limits of Large Scale Pre-training,2021.0,71
dd0e499557c933477e9de29151370d874e88b4b6,Language Modeling using LMUs: 10x Better Data Efficiency or Improved Scaling Compared to Transformers,2021.0,4
fd33e77884e69f6bc099990fc2790248af2749d9,LexGLUE: A Benchmark Dataset for Legal Language Understanding in English,2021.0,92
0ad6c5ac3cc46b2caa3a1927ce70d9c31b05800e,Combating Fake News with Transformers: A Comparative Analysis of Stance Detection and Subjectivity Analysis,2021.0,5
90148343ba48aed5eb363337888fa981ef39a489,MotifBoost: k-mer based data-efficient immune repertoire classification method,2021.0,1
31000c79187b9159c53aa53e3a73a008ca3844b4,Using Named Entity Recognition to Identify Substances Used in the Self-medication of Opioid Withdrawal: Natural Language Processing Study of Reddit Data,2021.0,4
08460ecff91b8a54358b9c1709d7dc6a77417f62,Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing,2021.0,4
6ca9b34fbabbdb33288508906b79beabd0b2ef3e,Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules,2021.0,0
2d4f66046bb436864cd6bf589e3a931c405f9f44,Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers,2021.0,70
d52977458284acce166bb1291a9d79143aa59070,Scalable and Efficient MoE Training for Multitask Multilingual Models,2021.0,47
4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1,The Trade-offs of Domain Adaptation for Neural Language Models,2021.0,10
992129aa96c97fa3b2ced0ddbc4c7d07bfaaf821,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,2021.0,43
4a8964ea0de47010fb458021b68fa3ef5c4b77b2,Primer: Searching for Efficient Transformers for Language Modeling,2021.0,52
4e0709e83f62f7011ec98dd9182aa3cde81aa66d,Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering,2021.0,20
586cafd248d01c1a1c67a57b3cef9f807173110c,Performance-Efficiency Trade-Offs in Unsupervised Pre-Training for Speech Recognition,2021.0,46
9cac09098aa611bd9a94d080d2401840632ab16f,LM-Critic: Language Models for Unsupervised Grammatical Error Correction,2021.0,21
ff69d758764157e612f92f97a987838312c568a9,Compute and Energy Consumption Trends in Deep Learning Inference,2021.0,34
a6d8d04962f84ae6225e72723869a002b9fc8036,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021.0,68
73ebd89b20bb1c1a93e79e4b0175e0825b0aecda,Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach,2021.0,7
77d956cdab4508d569ae5741549b78e715fd0749,TruthfulQA: Measuring How Models Mimic Human Falsehoods,2021.0,271
9289826beb6206eeaf500105f7329d6d5a495d8a,Robust fine-tuning of zero-shot models,2021.0,260
732402ff28d1b814ef24dd49d428be333b187d8c,De Novo Prediction of RNA 3D Structures with Deep Learning,2021.0,0
2f98ab0aab23200b3c1803279c14924bcc7a5025,A Refutation of Finite-State Language Models through Zipf’s Law for Factual Knowledge,2021.0,3
88afeaf0a4208477e845170daa8a189cc0a13a73,On the Multilingual Capabilities of Very Large-Scale English Language Models,2021.0,7
752985595ce02327b76bb11e0afafb9d31522215,Scaling Effect of Self-Supervised Speech Models,2021.0,4
27e63284b1adb0280bfb2963fda048108ad560e8,Why and How Governments Should Monitor AI Development,2021.0,16
6b827b800091fa7b1171b265846bf226cd92d3e2,Overview of Machine Learning Process Modelling,2021.0,4
5b13fae7218cf5e7f7b0aaf88ba366d35c9a39c6,Design and Scaffolded Training of an Efficient DNN Operator for Computer Vision on the Edge,2021.0,1
0622f6f8b0ea543671cc1bdb090353017f3558c8,Learning C to x86 Translation: An Experiment in Neural Compilation,2021.0,4
5046dfc4a3b77b120c1ccfc7ada2eba63fe5e1d9,Comparative analysis of molecular fingerprints in prediction of drug combination effects,2021.0,30
6d178f85c8ca9698b02d0f4f1f4a8f2fc4895411,The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models,2021.0,8
4183d028c7b7e54f55e63698232e4d0a6df535bc,Towards Structured Dynamic Sparse Pre-Training of BERT,2021.0,12
6c761cfdb031701072582e434d8f64d436255da6,AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing,2021.0,111
917c63f2186119166b3379f5d2816bb1a2f39b09,DEMix Layers: Disentangling Domains for Modular Language Modeling,2021.0,67
898b14509593d235414df054527b7702e35c3099,Post-hoc Interpretability for Neural NLP: A Survey,2021.0,75
1081df12b1500f48718ad9cdf3b0b90f44a31fc9,Curriculum learning for language modeling,2021.0,10
4c67b079e9154b85dd8486050ef2d98fd8c626d7,Single-sequence protein structure prediction using language models from deep learning,2021.0,47
2bc9e9b5161df6ea2015b209c8586f5220fd78d0,Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers,2021.0,8
c1b219918c2ee00fca9a1628ecea8a5c7f9d15b2,"Simple, fast, and flexible framework for matrix completion with infinite width neural networks",2021.0,15
9933a5af7895354087baf6c96b64dc8a8973eaed,Perceiver IO: A General Architecture for Structured Inputs & Outputs,2021.0,314
edd7fca50558906e6cc44053c1d6944c152c6f4d,The Value of Data and Its Impact on Competition,2021.0,8
528d1f017cf87144aecb43834b2712a2300a9190,Over-Parameterization and Generalization in Audio Classification,2021.0,1
1106f88502c8681c774a63fd1553fb98525fe2fa,Epistemic Neural Networks,2021.0,49
1e3c328c60a5b7237be77595aa7e60f9cc1dabb9,A factorisation-aware matrix element emulator,2021.0,15
58350665d7880fa1c85f6889b3a44cc96fa67d56,Codified audio language modeling learns useful representations for music information retrieval,2021.0,40
acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269,Evaluating Large Language Models Trained on Code,2021.0,1489
44ca883b0f1385d4cf832fe3ed65a0f00b40bb5f,"Prioritized training on points that are learnable, worth learning, and not yet learned",2021.0,0
319b84be7a843250bc81d7086f79a4126d550277,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021.0,178
7220984a462b650a0f1e12a0954cf7cd273434aa,A Generalized Lottery Ticket Hypothesis,2021.0,4
20a01009d38d083a49e01ef46005363135453661,The great Transformer: Examining the role of large language models in the political economy of AI,2021.0,24
03c06a7e960311758e05f89a3a5e5c3913971704,JUWELS Booster - A Supercomputer for Large-Scale AI Research,2021.0,7
4b88931fe3f022f508ba4d01aa7994603a3ee359,Adversarial examples within the training distribution: A widespread challenge,2021.0,1
cc33ddc931cb3f709f7f6e9d4165cf8d98874f67,Towards universal neural network potential for material discovery applicable to arbitrary combination of 45 elements,2021.0,47
8afdd8b9dbdd1073b2a177688768ae8ac5060498,MassGenie: A Transformer-Based Deep Learning Method for Identifying Small Molecules from Their Mass Spectra,2021.0,28
8125a482edcff959e7245a1da0da9ff6d5ed9742,Deep limitations? Examining expert disagreement over deep learning,2021.0,22
43e35f6a5930caf964b891c7f57f93f111fbe251,Transflower,2021.0,35
f6c4d57c710b461ece658a0b8e7427e862fef116,NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs,2021.0,41
64902a5077ee68011cd467398dbb66511e8e891a,It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning,2021.0,17
f76a85b34dee7d3a20937a9d016bcbed2e947f2f,Distributed Deep Learning in Open Collaborations,2021.0,19
90e99c33ce72b81445dad9b10cff931201a9d5c7,On Anytime Learning at Macroscale,2021.0,15
67bbcc013a70b09652032623787dba6ab80740a7,"Learning the protein language: Evolution, structure, and function.",2021.0,133
feba0c47bf12a02c3a725174bb53df78658a72a8,"Pre-Trained Models: Past, Present and Future",2021.0,316
41fe7f4b3ebf9616419101faa8c5f2ee43a118b4,Revisiting Model Stitching to Compare Neural Representations,2021.0,51
de8f92c8a7ebde8bffb968a536f79e5fb7cd225e,Scaling Laws for Acoustic Models,2021.0,13
d103f20f0a30eb055d0ae44b3b5b8ab44b0c7913,Neural Symbolic Regression that Scales,2021.0,64
883fd1cad73f37a41196f6eeb92a07e85af8ec8c,"Disentangling the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect",2021.0,13
9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73,GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures,2021.0,3
a6337d9ebb0b7de84588806110157806f9c0383b,GraphiT: Encoding Graph Structure in Transformers,2021.0,70
0f1382cb004b4834cc3ca7824a61d0d6b86a5763,Pretraining Representations for Data-Efficient Reinforcement Learning,2021.0,66
2a805d0e1b067444a554c5169d189fa1f649f411,Scaling Vision Transformers,2021.0,563
0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe,Hash Layers For Large Sparse Models,2021.0,90
5aab57cc0530560d82c74c055f664280619d7e81,PROST: Physical Reasoning about Objects through Space and Time,2021.0,22
9c4dd36ad206ca8be96ae4000568e899f4acfa91,Top-KAST: Top-K Always Sparse Training,2021.0,68
70db308d3635bf20bfbc66e177e6744362e4a9cb,ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression,2021.0,8
3127973b8ca73c67c3ba207c4a2b59dba8e5d258,Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models,2021.0,0
aee7fd72f33bc8060ff32eb88f88b904802a9243,Lower Perplexity is Not Always Human-Like,2021.0,30
c0f77574d63a1d608009ad096e8f0fc791eb1297,A Generalizable Approach to Learning Optimizers,2021.0,18
10e37aea3266cfa33849a530db7eec4d4b52966b,What Matters for Adversarial Imitation Learning?,2021.0,46
32feca141fce06c6588b4014d27953a3fc25f19b,PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World,2021.0,42
ec5798563138f80de1a4ab2f54ceddd894d7522f,Doing more with less: training large DNN models on commodity servers for the masses,2021.0,4
a2a97c944a5a987435cbe8249693b63a3a2c2745,"Effect of pre-training scale on intra- and inter-domain, full and few-shot transfer learning for natural and X-Ray chest images",2021.0,3
7777a31341fa4bfbd25b96a5320681af8dccf3af,Exploring Sparse Expert Models and Beyond,2021.0,22
74e24fa1628780d269ba7bff7237f7ac1fccea7e,A generative nonparametric Bayesian model for whole genomes,2021.0,6
981995fd64611f475179b280f4e9c241051ac185,Knowledge Inheritance for Pre-trained Language Models,2021.0,25
1006d191e9eb5b4dbc35fc0bb389328ddc75cba7,ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models,2021.0,208
4862141c0283502fe30d0c3b2f01c87b30fd15dd,Search Spaces for Neural Model Training,2021.0,4
b9dd0fec4df68bec7673c260f34083a1bec5895b,One4all User Representation for Recommender Systems in E-commerce,2021.0,22
28459083ba624020c8f1c1ed7c3a075f48b4e709,KLUE: Korean Language Understanding Evaluation,2021.0,93
e3a3e85c5a32af29e13b3561f6cf070de70651de,Pay Attention to MLPs,2021.0,343
bfe6d34ceece0a79bee5791b35e1314018b62201,Compressed Communication for Distributed Training: Adaptive Methods and System,2021.0,6
8307e0a2c64092fce2d44e73c6b865afe4264d55,Investigation of text data augmentation for transformer training via translation technique,2021.0,1
e3b8ed7c6d98ddb5ef7a41ce80dbbfbf60e8e5a6,High-Resolution Complex Scene Synthesis with Transformers,2021.0,18
6a8cb4fb5a20c7e5733a9bd50cd5feaad6c11360,Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level,2021.0,26
d13a0c8d49cb268d8d245925baee0316c1fe1875,Which transformer architecture fits my data? A vocabulary bottleneck in self-attention,2021.0,15
5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec,What’s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus,2021.0,51
dc70b180329a6ffead5e48093fb5a551955047c4,HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish,2021.0,37
39692df948172a319ca33434f27c213d99d941a2,AI Risk Skepticism,2021.0,4
a65b7ff72deb358bef29900a53c814771275c37c,Scaling End-to-End Models for Large-Scale Multilingual ASR,2021.0,48
ba68248488a92114d5136263d284e109395abb5e,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,2021.0,4
79b8ef3905a42b771248719495a2117271906445,Carbon Emissions and Large Neural Network Training,2021.0,302
478bc0a2b9e5230c1f97eb90934122b682095b11,"Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?",2021.0,12
1adadbfa95e43a70fcd17e6ce947a0652b86bfc3,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,2021.0,156
3a863672b072a37ac8d2eb6b780234f6b748a2d9,Generating bug-fixes using pretrained transformers,2021.0,36
01df9e724fb0149ad350ff6870b6110a587cb626,Comparative analysis of molecular representations in prediction of drug combination effects,2021.0,1
6263f69b0154fb090fc76f792c1a4aa182e24294,Language Models are Few-Shot Butlers,2021.0,16
4506c9cd34a0b6f09fe23a3d9be0bc0962d14540,ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning,2021.0,144
d8ea98c57f1f4cf4d42b26fd8e3694772e9ca495,Robust Optimization for Multilingual Translation with Imbalanced Data,2021.0,13
7694aae9766d5f1fe74d900cd82aee898cb6e8e9,How to Train BERT with an Academic Budget,2021.0,61
da8a4416db1da4887572ed47240a72b6f177a472,Online and Offline Reinforcement Learning by Planning with a Learned Model,2021.0,70
4b734d4fb14acbc5135eb382e1388840df84e9f2,Lookup-Table Recurrent Language Models for Long Tail Speech Recognition,2021.0,4
981dbdf6f87f13f3f3047a925c519fc39a35202b,Revisiting Simple Neural Probabilistic Language Models,2021.0,6
52c06a43eeddd2a0666b7cd3ab9fd41686bc7db4,GPU Domain Specialization via Composable On-Package Architecture,2021.0,5
2a8fa407e074bebeaf1e254be37fae7fc54610e3,SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,2021.0,86
614e53f957525fa652df64b9c45731a8b66cd73f,Why is AI hard and Physics simple?,2021.0,6
d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f,Understanding Robustness of Transformers for Image Classification,2021.0,223
65ba326ca90f6c6f204fad91db76af1076aeac30,Active multi-fidelity Bayesian online changepoint detection,2021.0,8
7198c33fcbd3561f9491c73529eb19a45ac298cc,Visual Grounding Strategies for Text-Only Natural Language Processing,2021.0,8
2642ab2e7f570891c06768c6a3bedb6033ba83b3,K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining,2021.0,1
b9ce9fea4634d6bfed5af2f4de410822295b3630,Can Vision Transformers Learn without Natural Images?,2021.0,18
238eb420c472bfdb1b4d34f9f53abec51f307a6b,FastMoE: A Fast Mixture-of-Expert Training System,2021.0,41
f111b5e43118891c517e9f47a36418870dc39533,How to decay your learning rate,2021.0,9
be0c0dbe570ea7f4f3470e7579ac765c63beaf16,The Low-Rank Simplicity Bias in Deep Networks,2021.0,57
2972bd6cb49883a5c75e26f8f7266dc91e1af25a,Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles,2021.0,25
a4bb144fc0e7b6d25ff770c556745d9149c04147,Training a First-Order Theorem Prover from Synthetic Data,2021.0,9
57d1e7ac339e783898f2c3b1af55737cbeee9fc5,Measuring Mathematical Problem Solving With the MATH Dataset,2021.0,215
79a79a4c1a43038973c49b0cbc6085e2c6caeeb1,Generating Images with Sparse Representations,2021.0,83
2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6,Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices,2021.0,12
63a9daf15ae2d4c1a7859d3105c9e6710903e072,Perceiver: General Perception with Iterative Attention,2021.0,489
268fedc5d786fa197b294dccab7eea02dc08038a,"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",2021.0,2
a194297f3703bae8040ddfd0c08a60cdcaea2aeb,Transformers with Competitive Ensembles of Independent Mechanisms,2021.0,16
453fc588d97958c6fefad96e79edd896873b3e09,Chess as a Testbed for Language Model State Tracking,2021.0,11
6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,Learning Transferable Visual Models From Natural Language Supervision,2021.0,7884
288cb169619bde78604450adc8cb5df536ef20f1,Learning Chess Blindfolded: Evaluating Language Models on State Tracking,2021.0,5
46130875c8c2d89ea23dfb29c3784a6e5e510e54,Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,2021.0,17
de18baa4964804cf471d85a5a090498242d2e79f,Improved Denoising Diffusion Probabilistic Models,2021.0,1164
19537be34dbadbcaa4fffcf028a8ada5095b1b5c,COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining,2021.0,144
065112180cd381ffc018780cf8fc0a14ae2580b1,Proof Artifact Co-training for Theorem Proving with Language Models,2021.0,44
c16835c8e535ebd9c10a550ca9455fe384a14449,High-Performance Large-Scale Image Recognition Without Normalization,2021.0,351
ec05bd6725ac6a5217021881cac8553581b3e313,Measuring Progress in Deep Reinforcement Learning Sample Efficiency,2021.0,7
be09ed6cd73654a23f78416433a1b23ea623ea79,Symbolic Behaviour in Artificial Intelligence,2021.0,26
ba233d75aa403092bda0bffc026be7913673ad69,Mind the Gap: Assessing Temporal Generalization in Neural Language Models,2021.0,98
3234d2810eb2d7cfd7ef2d3646de4581a78e42a8,Embodied intelligence via learning and evolution,2021.0,85
232d0c42a26a54f52266a09e0c7117291402d33c,Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research,2021.0,35
12b71736392209b4292471b7da0aed71ba2aa545,ZeRO-Offload: Democratizing Billion-Scale Model Training,2021.0,155
fdacf2a732f55befdc410ea927091cad3b791f13,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,2021.0,883
0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36,Deep Learning applications for COVID-19,2021.0,172
4b30dd65a26e573df9796beb582ba1e1a69f23f7,Reservoir Transformers,2020.0,7
9faecf3e18a833f2d49b030d591cc2ded0b54336,Towards Continual Reinforcement Learning: A Review and Perspectives,2020.0,159
f1d839078a29ebe2dfaee9f57d1d92f91eac99c5,Universal Policies for Software-Defined MDPs,2020.0,4
98cadf04e1f4a65552debc0376471f7370f2e8da,BERT Goes Shopping: Comparing Distributional Models for Product Representations,2020.0,11
5d76c2591334f56dc9155568f793b013df0f6613,Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training,2020.0,0
eb2fc03b8865b8e1b4cb933d917ea269ebe14584,Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training,2020.0,0
043e5e0cf3129284683260976c10d98c7f121f35,Transformer protein language models are unsupervised structure learners,2020.0,158
62d1a3137b01a69443bebf4d92c1990ec512a6a1,Extracting Training Data from Large Language Models,2020.0,743
4c5d4601a3a19c31da6588d2a34adfb161f68c0e,Imitating Interactive Intelligence,2020.0,50
64e1b6bae8de89d49b1b8f52d0ae09f8b48cbcc3,Parallel Training of Deep Networks with Local Updates,2020.0,15
33422275fbb9958f55419620697faf531482699b,How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering,2020.0,137
2f8f049cbaded475ed10808b60388ce7dc1374e1,Real-Time Social Media Analytics with Deep Transformer Language Models: A Big Data Approach,2020.0,3
4d9709e366afccd097129e80c92006189fc901ed,Bounds for Algorithmic Mutual Information and a Unifilar Order Estimator,2020.0,0
f0dad13022ae7c236517a88b6a151b304aa04963,Whale: Efficient Giant Model Training over Heterogeneous GPUs,2020.0,18
bcd81cd90ad587ecf0a8086f171f0de4cb888d28,Understanding Capacity-Driven Scale-Out Neural Recommendation Inference,2020.0,39
b5613da1f643159c97cbf8555d6f5c4f05b36a9a,High Performance Natural Language Processing,2020.0,4
9e104d440540d2ffc9caaa0952a9e5f7f9344ba9,Are wider nets better given the same number of parameters?,2020.0,32
9dbb94bdf18a90a04697b19975b6ada4d257cc0a,Stochastic Optimization with Laggard Data Pipelines,2020.0,10
bc87279d4b32a425377ff18ab63f7ecf95ff228c,Rethinking embedding coupling in pre-trained language models,2020.0,100
4d3f6673009589971973a81a097441e7c78a265e,Improving Multilingual Models with Language-Clustered Vocabularies,2020.0,43
9c4f0ccf63304361e257e7c82ff551966e49dd0c,"Deep Learning is Singular, and That's Good",2020.0,10
687b13c44f849d23c2496996b5da83e706094db9,Beyond English-Centric Multilingual Machine Translation,2020.0,458
1a6c5f6ce26914c2a7af0217e8cd3e844f2b2f37,"Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",2020.0,38
ccad27088b9098de4eaca8dc449b18766db4b3ab,Reformulating Unsupervised Style Transfer as Paraphrase Generation,2020.0,139
4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c,Contrastive Representation Learning: A Framework and Review,2020.0,334
7e55df7baa707cb9bd32c0f72e9882038f4fa5b8,Using Support Vector Regression in multi-target prediction of drug toxicity,2020.0,1
3d2660faebfcc1b7c7777f48b32a2eebec346bab,Guiding Attention for Self-Supervised Learning with Transformers,2020.0,13
b090cfc2179a789e7aeb684ee8c85e71bcbd4aa0,A Closer Look at Codistillation for Distributed Training,2020.0,6
dae4641eed6ddbe0a781ab5e78daf8204e60f397,Which *BERT? A Survey Organizing Contextualized Encoders,2020.0,38
ee5fff85d3ec62698eddba162f054b7e73670b2a,Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics,2020.0,235
f89e2f4fb44e30b1adb08d153bf22b063597f896,Current Limitations of Language Models: What You Need is Retrieval,2020.0,2
15aa86556be1579eadf8fbb0bc486f9427e681f0,Evaluating representations by the complexity of learning low-loss predictors,2020.0,21
486d580110a6780012b25c9783299e31f173ecf1,Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent,2020.0,35
07794f74ebe869e64ea6d9f63fe71abf5c25196b,Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain,2020.0,15
10bb7e2c54b947fa50e7bb65b0b5c700fe998044,Measuring Massive Multitask Language Understanding,2020.0,397
51ae2c451a1a05293334a509b71c9c9e0377d35c,"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",2020.0,65
e64154041938fdee4dd6855cc79bfb3393eee23e,Roadmap to a Roadmap: How Could We Tell When AGI is a 'Manhattan Project' Away?,2020.0,4
5a98bcb1082fc50d31ddb59671d3026f16a0dfc6,Forecasting AI Progress: A Research Agenda,2020.0,11
64da659c0687762359226b4cf455520c78acd165,"Neural Language Generation: Formulation, Methods, and Evaluation",2020.0,19
063f8b1ecf2394ca776ac61869734de9c1953808,AdapterHub: A Framework for Adapting Transformers,2020.0,360
91cfd15b587c5ed604e7e49326db6d045276c2a5,The Computational Limits of Deep Learning,2020.0,287
1f8c1b8943466e2698dca2f6335081c5b46506ec,Language Modeling with Reduced Densities,2020.0,6
7267c8343bd28b472e5b0684e98f067dfd70d5d4,Accuracy-Efficiency Trade-Offs and Accountability in Distributed ML Systems,2020.0,7
1882f194cb43828852cc052887671e55a80f945a,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,2020.0,487
8668fd1cb5cab820f8b2a136b2ef4adfad6c4dc1,Knowledge-Aware Language Model Pretraining,2020.0,51
c00ba15810496669d47d2ed5b627e6c7d2b1f6aa,Pre-training via Paraphrasing,2020.0,127
0ece9e13768dd818c677254d1185775c70eaa6cc,Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks,2020.0,104
5844cfc72c1b75c544ff487a3737de6305df8935,Statistical Mechanics of Generalization in Kernel Regression,2020.0,7
dc0fb0c81f87859a04314c94d74fc7d9c909393d,Online Handbook of Argumentation for AI: Volume 3,2020.0,1
c014f8bc3b521453a93a13bb2c90700fcf462738,Limits to Depth Efficiencies of Self-Attention,2020.0,32
3e7f5f4382ac6f9c4fef6197dd21abf74456acd1,Big Self-Supervised Models are Strong Semi-Supervised Learners,2020.0,1562
cbe9be3a9731c13dd18fc7bdfaf8dcedfe7a5544,What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,2020.0,133
1426c97cc8f8f23a6cb1cfa4d4e10f27b0e2a3a7,Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs,2020.0,80
f2fc9ef411846dd577c26225ce93f50bb1fa760b,Multi-scale Transformer Language Models,2020.0,8
d97e7561fa7710213ccd4f8128044ea6849be377,XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,2020.0,130
0771a9c573de33306b68d7ffe5564e285e4c74bb,Breaking (Global) Barriers in Parallel Stochastic Optimization With Wait-Avoiding Group Averaging,2020.0,9
9b539d413393047b28bb7be9b195f142aaf7a80e,Recipes for Building an Open-Domain Chatbot,2020.0,702
f6e0164466e827112fd415afdc28ddf8e0eb1ba3,Document Ranking with a Pretrained Sequence-to-Sequence Model,2020.0,268
cf59f5248aa6f64150a6a6f668579a3e04e6f64a,On a Class of Markov Order Estimators Based on PPM and Other Universal Codes,2020.0,2
2356781b8a98bf94e6fc73798c6cb65ac35e5f97,"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",2020.0,193
f375915dddc706d2df04ec103eb004fd71ec1a8f,Learning@home: Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,2020.0,5
19eaa4ac17550fab2917d3f6121ed25e6d857a58,Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,2020.0,25
0b838b84c8b8610b869f26eebec37228ebf347dd,The Transformative Potential of Artificial Intelligence,2019.0,23
00c957711b12468cb38424caccdf5291bb354033,ZeRO: Memory optimizations Toward Training Trillion Parameter Models,2019.0,483
72ff8fa78d2a669389041c1b5ebe015c43448e93,Meta-Learning Initializations for Image Segmentation,2019.0,17
f64a4256a8e79c8553e2e8eb701a7b0a9c306236,Text Implicates Prosodic Ambiguity: A Corpus for Intention Identification of the Korean Spoken Language,2018.0,6
da7627dd7769210fc6ec917d13b00365d0989c97,"Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",2023.0,1
789ccb1c15919fee66f1b2b67dc40365cb9996dd,HyperT5: Towards Compute-Efficient Korean Language Modeling,2023.0,0
1862ff140e0169c6b367dc3fd9e7c714dae38026,N EURAL N ETWORKS AND THE C HOMSKY H IERARCHY,2023.0,0
c5cf54b2b6abf658697d272c1377812fd9e24e11,Scaling Generative Pre-training for User Ad Activity Sequences,2023.0,0
75c08892179fc478f87d7020b5daff9fca4f3389,Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models,2023.0,7
33c61fcec57f5b412439e5a882b1a2a1e8a1de5a,MEMORY-EFFICIENT TRAINING,2023.0,0
df239785e6d26a45e9c8e06551cfecba92d1ecad,Exploring AI Ethics of ChatGPT: A Diagnostic Analysis,2023.0,103
809f3198289554fce309795219cdb42befede20e,Almanac: Knowledge-Grounded Language Models for Clinical Medicine,2023.0,2
e8f060d1947b2deedffd6cf7d7bf8e327b333236,A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training,2023.0,0
4deafcd9db4687904f068c14ecd0f1a7a8103eea,Experimental Case Study of Self-Supervised Learning for Voice Spoofing Detection,2023.0,1
4e8d03e525ea66bebdbc2769c5f60f40ad16fd05,What is the State of Memory Saving for Model Training?,2023.0,0
82dd7bd1b722803306b0ab93aa93afcfbcaaac0b,mSilent: Towards General Corpus Silent Speech Recognition Using COTS mmWave Radar,2023.0,0
70583309207f006020e514b7925103944029a1c6,BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware,2023.0,1
4e2ff3308fe59a682071bf1869b979eb62a6f4fc,Deep learning of genomic contexts predicts protein co-regulation and function,2023.0,0
499075670464a7426ea5d12171cc55e892fad1d1,"Why try to build try to build a co-creative poetry system that makes people feel that they have ""creative superpowers""? 67-80",2023.0,0
6d172caf3723ef9faee956d535a96e91560638da,GPT-2 Metadata Pretraining Towards Instruction Finetuning for Ukrainian,2023.0,2
2d1842056c4311c886ed9bf634e310490d725b33,On the Concept of Resource-Efficiency in NLP,2023.0,0
a8d6467e4126d0ada4e824b30a6098056b843242,Generative Models of Images and Neural Networks,2023.0,0
7889cc4e9565c9e7bd725509d7da4597e6a9b576,CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks,2023.0,2
c148d80a82308d04e45ed480aaed1ff8fed0d5bc,Semantic-Augment: Augmenting the Semantic Space of Transformers Improves Generalization,2023.0,0
793f7284ccaa6c2cd530e6d405f5fa75bfd283e8,Scaling Infrastructure to Support Multi-Trillion Parameter LLM Training,2023.0,0
9b6c38ef6eb937f4a802d75169bd18fba5394f71,Guideline for GPT-2: Investigating Model Variants For Different Computational Budgets,2023.0,0
07ae5b2c07302281b1f0d17f01efeb88091a0930,Minimum Generative Pre-trained Transformer with Human Feedback,2023.0,0
6ec6f56284232220e5c70e6138228fecf4576d10,GUAGE MODEL SELECTION,2023.0,0
4762b614958013039ed14c59367eca05165b0c21,INSTRUCTION-FINETUNED FOUNDATION MODELS FOR MULTIMODAL WEB NAVIGATION,2023.0,2
52344ebfe637d5d83e541185ae1e4753d572305f,Towards Disentangling the Roles of Vision & Language in Aesthetic Experience with Multimodal DNNs,2023.0,0
9801d47561825077621957cddf786cf1e26fbd3f,Road Barlow Twins: Redundancy Reduction for Motion Prediction,2023.0,1
2dbc8228b86ff20612403e5aa7dd2984ea01ca7a,"Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding",2023.0,0
63d8cd208d029d54a3c31f4c47d11ad9b2b16dee,Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times,2023.0,0
889a736ac167b5e465d0f0c83400244619efb566,RIGA at SemEval-2023 Task 2: NER Enhanced with GPT-3,2023.0,1
0db494283ace79ca1cabd169cc681ffccc0dcdc8,Corpus Complexity Matters in Pretraining Language Models,2023.0,0
899c7fd82d9afe38f76ffca105f2e60fe289ef09,U NMASKING THE L OTTERY T ICKET H YPOTHESIS : W HAT ’ S E NCODED IN A W INNING T ICKET ’ S M ASK ?,2023.0,0
04262297b65a85476343f6483c09fc4f7f21eb59,G ENERATE RATHER THAN R ETRIEVE : L ARGE L ANGU - AGE M ODELS ARE S TRONG C ONTEXT G ENERATORS,2023.0,0
d790fe6ed7280f0596133aeb9d5be6bd4e8f54e9,DCI-ES: AN EXTENDED DISENTANGLEMENT FRAME-,2023.0,0
ed69756ce1717c1f658f52f92f81e0a4efce143c,R EVISITING P RUNING AT I NITIALIZATION THROUGH THE L ENS OF R AMANUJAN G RAPH,2023.0,0
7c18d0be0c22648cfbae6360774db70110371a6d,Selection Learner Learner Train for Q epochsTrain for Q epochs Selection,2023.0,0
e4f0ab1408504c3873a849d341ec63fbca899534,Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent,2023.0,5
2bd6afb11d81fb97bf6e2114043c35bd12c96ce9,P A LI: A J OINTLY -S CALED M ULTILINGUAL L ANGUAGE -I MAGE M ODEL,2023.0,0
33d84b1531f88d2bd2e516e1574f22e139133065,Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier,2023.0,13
a2a2d664b5ac7b3c35e4448203c519c4a26bf092,MUX-PLMs: Pre-training Language Models with Data Multiplexing,2023.0,0
e50921bc8585bf78ef896e1f31718c5ca102aa67,Can Large Language Models Reason about Program Invariants?,2023.0,6
a4d30f1d3c518310229af88ce85af9c6f02589b4,Efficient Fine-Tuning Large Language Models for Knowledge-Aware Response Planning,2023.0,0
ecdbb4c306915685552e8cd752b8ab4c741ce59d,Not Enough Data to Pre-train Your Language Model? MT to the Rescue!,2023.0,1
9249abc1ca05aeb9ffb45db638e4e8e49fc851dc,A Statistical Perspective on Retrieval-Based Models,2023.0,0
8dc86c158696cf86b9bd51c85d9927e427bd63b4,On the role of resources in the age of large language models,2023.0,0
77b60fdaf00ba3287c07d5584df9be38bf8dcabc,MAP: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting,2023.0,0
bb3f6ff6d4ae0e74a5f46a3cb3697446915980cb,Recent Trends in China’s Large Language Model Landscape,2023.0,3
c05e852fa8fa8aa2493b9ace04320701102eb6cb,T RENDS AND CHALLENGES OF A RABIC C HATBOTS : L ITERATURE REVIEW,2023.0,0
75f43654ac8fe62095c84d8d201095e9959d6dcf,What Should Be Done About Google's Quasi-Monopoly in Search? Mandatory Data Sharing Versus AI-Driven Technological Competition,2023.0,0
a6d6694bff3b4004c787e3e792211a17674dc009,Classification-Based Framework for Remaining Useful Life Prediction With Limited Images and Unequal Time Intervals,2023.0,0
b28c36064b968e927c068edc8d96315862333e46,Uncovering the Risks and Drawbacks Associated With the Use of Synthetic Data for Grammatical Error Correction,2023.0,0
46299fee72ca833337b3882ae1d8316f44b32b3c,Reflexion: an autonomous agent with dynamic memory and self-reflection,2023.0,87
1773be628c1ac76eb0e2d024a96627a98f27b62c,The argument for near-term human disempowerment through AI,2023.0,0
75a259f3a61bf27c214d61cd2ef26f3291d77add,A trajectory is worth three sentences: multimodal transformer for offline reinforcement learning,2023.0,0
f8982fa1fa350b12b2fc8ba34b32af6896566652,Latency-Aware Short-Term Video Action Anticipation and its Application in Trajectory Prediction,2023.0,0
1b04ac51ffeed0d31fbdbab0e4ed65ffe8c35df6,AI Safety: Model Trojaning and Benchmarking,2023.0,0
af1036818d8359b974f449fb5a501bc02e29f77d,Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey,2023.0,0
0801de6ecb137b1603772328c9791d69c1ba654e,Semantic data augmentation for meaning maintenance on Task-Oriented Conversation with Large-size Language Model,2023.0,0
70f8e3c72e9178b408667e3619a87a153fd853e6,"Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned",2022.0,8
5ea63594ba2627dbd60ca062358e58178065e007,"A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model",2022.0,9
9490d42c4869e6d6f3308c9813b1cfe31ff80137,E FFECT OF MODEL AND PRETRAINING SCALE ON CATASTROPHIC FORGETTING IN NEURAL NETWORKS,2022.0,1
cb9082315a417b301282c33d547ba13d76d80993,Towards Automatic Generation of Messages Countering Online Hate Speech and Microaggressions,2022.0,9
66ba0765439c7f0189ace81cfd8626f88b260d3f,Optimisation & Generalisation in Networks of Neurons,2022.0,0
b42e3a759348f27cca2f918a6bd0b139a5312e44,A Survey of Pretrained Language Models Based Text Generation,2022.0,21
394d6dc274e28660333ff3c1c4ab3dd2883d87aa,Algorithms for Efficiently Learning Low-Rank Neural Networks,2022.0,2
55b9a2ade0a49e9cf10b71528d69dfee4e826025,C EDILLE : A LARGE AUTOREGRESSIVE LANGUAGE MODEL IN F RENCH,2022.0,0
326f4a9625a1a3fdf696ce87cd82a6595e8311d5,I-Tuning: Tuning Language Models with Image for Caption Generation,2022.0,5
6b86a91737809b869ae7bf3d34c231b697728825,Do Language Models Learn Commonsense Knowledge?,2022.0,6
57ceadbb37da24ce24b3ab8ff826ddcae717c0e2,Learning Transferrable Representations of Career Trajectories for Economic Prediction,2022.0,2
290ef971d936b9b727dc36dbf4f38be6c8d915c5,LiteTransformerSearch: Training-free On-device Search for Efficient Autoregressive Language Models,2022.0,11
ed0d9ef9891cf19c5c428e41effe5fedfdb5386e,Robust Question Answering: A study on Self-Attention Mechanism and Data Augmentation,2022.0,0
f15562af6b15c88f2b10a1d11aa26c15d2be3436,Summarizing Radiology Reports’ Findings into Impressions,2022.0,0
6ee286a5a0eceae111ad579f17aedb115e4e8c6c,Improved QANet on SQuAD 2.0,2022.0,0
cbad1c5fdc942bb92b090741ba975a0fec91f97e,QAN-et al.: Exploring Extensions on QANet,2022.0,0
83b80139de383c8aca991964d6248f066eef1426,Team Xuber: Question Answering via Modified R-NET Construction,2022.0,0
29fb34f80b0e42da74e0d451b8fdaf2e4f4e8375,Classifying and Automatically Neutralizing Hate Speech with Deep Learning Ensembles and Dataset Ensembles,2022.0,0
7ab536d567d60a9887c93adb562659be76138913,Prospectively validated disease-agnostic predictive medicine with augmented intelligence,2022.0,0
33f3f31f871070f19b0c3e967a24e322bfc178f2,Retrieval-Augmented Diffusion Models,2022.0,44
c04f7b80487a7081e0e603a9ae36c5297f5742a5,Foundational Models for Continual Learning: An Empirical Study of Latent Replay,2022.0,11
f40aeae3e522ada1f6a9f326841b01ef5c8657b6,Unifying Language Learning Paradigms,2022.0,98
7450a612ef291216b0cd48e09b8879be4675c6eb,Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions,2022.0,5
84f78cc33b124434aa8e4b670686de6831be02ba,Distributional Semantics Still Can’t Account for Affordances,2022.0,8
8b51019000acad8f96e1be8582a086d6f69f394d,Semi-Parametric Deep Neural Networks in Linear Time and Memory,2022.0,0
a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f,P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR,2022.0,0
104f7a96eba307056e1038e183ee8c24d009ba13,nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models,2022.0,36
8f248b5476666e700390f7f8ff6ca923f97d726c,Advancing protein language models with linguistics: a roadmap for improved interpretability,2022.0,6
d595e49ed44e28599b82c7a99ecf6794f50d41ce,Lightweight Transformers for Conversational AI,2022.0,1
26c7ed5aa1ef22dbd33358d12ef02b164e74c3ea,Multimodal large language models for inclusive collaboration learning tasks,2022.0,1
18ca53bc269b5d532256b5a7805f111427ef53cf,Evaluating Neural Network Pruning Techniques on Visual Transformers,2022.0,0
be995829139bad9e91a0554220fd2d5faf9d3d3b,Diet Selective-Backprop: Accelerating Training in Deep Learning by Pruning Examples,2022.0,1
c6368f4c480e4457dd40bee004bb46428283278d,A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer,2022.0,4
9a5e9e08734fbe5d3b7a6ba18351cd0ba52ab396,Pruning Adatperfusion with Lottery Ticket Hypothesis,2022.0,0
bb3c188768fd4481bf83353caa94eb1f65f0d30a,Reject Before You Run: Small Assessors Anticipate Big Language Models,2022.0,1
39a3bfa818c00853103e7928f76612d3fe7ffe9d,Superior generalization of smaller models in the presence of significant label noise,2022.0,2
9d7a75601e0e50dd68d40cfb8ef0e891dad797a6,Orca: A Distributed Serving System for Transformer-Based Generative Models,2022.0,21
4217467e747182b9ad8035e8a2d657d2ce80af07,On Reality and the Limits of Language Data,2022.0,5
dd0b3fef20fd9384e055648ed68ac4f8d793bd99,Generating Detailed Music Datasets with Neural Audio Synthesis,2022.0,1
48c646ba65d40c0de759ea6d05edd3d7ef43311e,Improving Offline Handwritten Text Recognition Using Conditional Writer-Specific Knowledge,2022.0,0
2e6ca609b301ea6d62d7e2ae40b59064727c6614,Nepali Encoder Transformers: An Analysis of Auto Encoding Transformer Language Models for Nepali Text Classification,2022.0,0
ef74ec406378390cfaa2a939e5141ee943fba01b,Few-Shot Regularization to Tackle Catastrophic Forgetting in Multilingual Machine Translation,2022.0,2
0e472830a6604c5ff54b9cc8837227b4bb68706d,ITAINNOVA@DA-VINCIS: A Tale of Transformers and Simple Optimization Techniques,2022.0,1
7f304f180c10715ab0e3624f06482a2b8e771a72,Bigger&Faster: Two-stage Neural Architecture Search for Quantized Transformer Models,2022.0,0
d461aef562bd9c13dc0f972f147df8be1fcca63c,Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training,2022.0,9
c5862fd1bb07216a2cd7fcca5b2536b74eba0d44,A Closer Look at Parameter Contributions When Training Neural Language and Translation Models,2022.0,1
3ec1723060c31489c5ab953b9402302d360eab83,Coordinating Heterogeneous Teams for Urban Search and Rescue,2022.0,1
2f21201ac9fcb88a72c56471402388ec2fc365a8,Inferring Implicit Relations in Complex Questions with Language Models,2022.0,7
0a9364ca06771d2b85da147a453bca0d02f8e248,Multimodal Machines from a perspective of humans Ph.D Thesis Proposal,2022.0,0
38338207e9ee2591e67c926b7da2294318a0dec2,Models with Conditional Computation Learn Suboptimal Solutions,2022.0,1
40accf927b75c49dad0f08199c390d1158ef202b,Contextualizing Language Models for Norms Diverging from Social Majority,2022.0,1
e352f5753cbb4cee32f1d5f8d922d1ce415183ac,Unmasking the Lottery Ticket Hypothesis: Efficient Adaptive Pruning for Finding Winning Tickets,2022.0,0
29d0e22287b05661cc7d9c90eabadd6733ef6946,Semantic Shift Stability: Efficient Way to Detect Performance Degradation of Word Embeddings and Pre-trained Language Models,2022.0,2
1fcd87c6aed5f4510b6ff7dd65e35974093282a5,SMART: S ELF - SUPERVISED M ULTI - TASK PRETR A IN - ING WITH CONT R OL T RANSFORMERS,2022.0,0
02d07c0d4e93e6c84357c0360bfc6229afce445f,Adversarial Attacks on Feature Visualization Methods,2022.0,0
4b43bc712cab78ef05d40730ff2db203fdd6c153,Incremental Processing of Principle B: Mismatches Between Neural Models and Humans,2022.0,0
f65e11bae859d318f286b36caaab8ff6ac11a771,PLM-based World Models for Text-based Games,2022.0,0
282000982c8f7752cbf9c4d226c0b22d2875ff9c,Multimodal Coreference Resolution,2022.0,0
9715be0a94c9b05bafe299cbfb4f846453bfd2ab,Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance,2022.0,2
178bbb0f9372e75d882419572360d693948e8472,Benchmarking data science: Twelve ways to lie with statistics and performance on parallel computers.,2022.0,0
70d89d380ca5d20564e1dd8ed2f4c59f5c7b3656,HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation,2022.0,2
e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da,Pitfalls of Static Language Modelling,2021.0,46
d66e80224cda0c1d5a4c1be3798df6a6bfe3713c,GPT-3 for Few-Shot Dialogue State Tracking,2021.0,1
e05efed686facda35aaac2fa9a69ba410e387ec0,ZeRO-Offload: Democratizing Billion-Scale Model Training,2021.0,0
0137de08b80ed7b5bfccd7d282c87ca4fc77f29e,Data and Parameter Scaling Laws for Neural Machine Translation,2021.0,42
80bbaf09b574ba42c8441e29ab476b50e13e996d,Emergent Unfairness: Normative Assumptions and Contradictions in Algorithmic Fairness-Accuracy Trade-Off Research,2021.0,3
1f3d593ac53336a807e7381611f7fe6c7de42a8d,GSHARD: SCALING GIANT MODELS,2021.0,0
40c3327a6ddb0603b6892344509c7f428ab43d81,Documenting the English Colossal Clean Crawled Corpus,2021.0,35
f6264b11ec0dd9133f1d88a5288a3267a93182f8,What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study,2021.0,90
52555e649aa43a60554863af43d99fa0e3932566,Easy and Efficient Transformer : Scalable Inference Solution For large NLP mode,2021.0,0
f1362000a1561924a3a07d7b9ab3d8cc3fd4e96d,Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images,2021.0,7
75aed4f0969fc801f39c4ed5e6305325947a142b,Very Deep Graph Neural Networks Via Noise Regularisation,2021.0,16
c6499788267a24b8616f7ea444fc91577160bf25,C L ] 4 F eb 2 02 1 Knowledge-Aware Language Model Pretraining,2021.0,0
98bee7c7b141084cd6668b47262befc81617d4c3,PFP: Universal Neural Network Potential for Material Discovery,2021.0,1
0c36366413f08c6f635af6897627a3f113f6115a,Catformer: Designing Stable Transformers via Sensitivity Analysis,2021.0,8
3357c0636ea7b1f3df990076ed6675c34000855d,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,2021.0,19
5861636a1c3c0e9ccf6f683fe70b3b5182ef1697,Whale: Scaling Deep Learning Model Training to the Trillions,2021.0,6
9657f9fe0ff4ed31692646d2263f76ab17c8c740,ING WITH LANGUAGE MODELS,2021.0,0
69c71029b898de7bc1ff7e9dab77d7fd8d3bb759,On the Role of Corpus Ordering in Language Modeling,2021.0,3
096cffc20d3bc89e8bc337607435a67e79d888d9,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,2021.0,10
9bbe1ca7ff0e6c3f95f20063cc733b481445ef09,Self-Supervised Learning to Prove Equivalence Between Programs via Semantics-Preserving Rewrite Rules,2021.0,3
a0033c2b38d289fd71194eb830b14d0db8f5a18b,Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning,2021.0,26
1b94ebedacda0c21a4b8a40a5a40afcea4cc719a,"When Combating Hype, Proceed with Caution",2021.0,7
921c1216edbf6b2931b15874f24847ff1007ad8c,EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks,2021.0,18
b2474a00d7de3373bab934c09acef1994fa82207,Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages,2021.0,92
d52302d302499cfd3430508b61b0f50fe7fce90c,A Systematic Investigation of Commonsense Understanding in Large Language Models,2021.0,6
0fd9c127463d62450337658fb59b96d0acbc5b41,F OR LARGE NLP MODEL,2021.0,0
eec7ad0270eda7298c139af6e2676599f1fd53f6,Data and Parameter Scaling Laws for Neural Machine Translation,2021.0,2
e58d8b717dfce4f37c5a4009c74c15915411442d,Dense-to-Sparse Gate for Mixture-of-Experts,2021.0,21
3ede1108e6cbad247b875aa46b4541967a83b980,Limits to Depth Efficiencies of Self-Attention Supplementary Material,2021.0,0
a0f482a50c5ead1bb054540aa48440fb92eaccf3,Empirics on the Expressiveness of Randomized Signature,2021.0,0
b2faac4ff48056f880bf836c1cc1f8a27ec0a73a,Towards the Automatic Mathematician,2021.0,9
d779908d8358febf9643e777fd72f4cb1df61620,Domain Randomization for Deep Reinforcement Learning in Self-Driving Environments,2021.0,0
8f3625dcb33f055be88c5542b8308b910b7dae31,Towards a Persona Aware Conversational Agent,2021.0,0
89d50d1f2b98fdee5ea7af86447c5bd39603a78d,Phase Transitions in Neural Networks,2021.0,0
eb870f3f203076f7769dedf6d3923f0a2cb5566e,O N E VALUATING AND I MPROVING THE E FFICIENCY OF D EEP N ETWORKS,2021.0,0
40a8996eef0a4a17579f1798764632ad0ad3996f,Deep Transfer Learning & Beyond : Transformer Language Models in Information Systems,2021.0,0
f3bb90070934727f5cad4fe8797bb9701fe1f010,Projecting Heterogeneous Annotations for Named Entity Recognition,2020.0,6
b5b2dad83e10c335e4c079e69b7a11fb79cc10b1,KungFu: Making Training in Distributed Machine Learning Adaptive,2020.0,43
be637822605968255e0157e856c5cd22949ac363,Reservoir Transformer,2020.0,8
011cdf321549ec4775618357f907fa36902d455d,PRETRAIN KNOWLEDGE-AWARE LANGUAGE MODELS,2020.0,2
13a76224c65d60435006b3c58527cd5a7c4dabbc,LENGTH-ADAPTIVE TRANSFORMER: TRAIN ONCE,2020.0,0
acd9db8c22ddd6402bc21b48c9b154a78fb23f5e,Learning Robust Representations using a Change Point Framework,,0
bbd322611857a830b4c616ee609ea2aa65946e62,Communication-efﬁcient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence,,0
37c3ce59c2cbc80849286c05296af159dd0eded2,"Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000",2022.0,0
