paperId,name,authors,year,month,num_citations
45cbff3a7e803dd949628a91bedef52b28bd63c7,Unlock Predictable Scaling from Emergent Abilities,Shengding Hu,2023,10,0
66380c4a519b7c3468d5a71fd1921d37a0890110,VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores,Roberto L. Castro,2023,10,0
c5c2870e05eae948fc58287447def73b6b192332,A Neural Scaling Law from Lottery Ticket Ensembling,Ziming Liu,2023,10,0
9bbb8fac9cf76ef919d290cb25cf6c4811a85cf1,Masked autoencoders are scalable learners of cellular morphology,Oren Kraus,2023,09,0
d0ddfc304c6a3490008835a944d8d0065ece77e2,Cluster-based pruning techniques for audio data,Boris Bergsma,2023,09,0
8e7254ab110cbf374d1c23f9bbce022ba9d01f1c,D3: Data Diversity Design for Systematic Generalization in Visual Question Answering,Amir Rahimi,2023,09,0
d3e6702c319cbd5314f0adea6e408562cb104e77,Uncovering Neural Scaling Laws in Molecular Representation Learning,Dingshuo Chen,2023,09,0
f9ab990ca3c0715e31854ec1087af572af8de8a6,Pretraining on the Test Set Is All You Need,Rylan Schaeffer,2023,09,2
77b603850094ff749c9040772f8169a75145d506,Explaining grokking through circuit efficiency,Vikrant Varma,2023,09,2
62d15b5a87961fd1a2b941e9fa83646db3324e2c,No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets,Lorenzo Brigato,2023,09,0
1516da95ff7eb6785b05ff11601342361667b0ba,International Governance of Civilian AI: A Jurisdictional Certification Approach,Robert F. Trager,2023,08,2
4c9eb09bfe9524574c0d4cd9614789f25f533623,CodeCoT and Beyond: Learning to Program and Test like a Developer,Dong Huang,2023,08,0
31d910504feca49bf9aa6adf7894f2cd9c0682db,Boosting Semi-Supervised Learning by bridging high and low-confidence predictions,Khanh-Binh Nguyen,2023,08,0
1711671b9c592b89602fa0974d529e8e214f10b5,Manifold DivideMix: A Semi-Supervised Contrastive Learning Framework for Severe Label Noise,F. Fooladgar,2023,08,0
4fe36896ba85b1e32318dcc9a7215c6c693ba524,PSRA-HGADMM: A Communication Efficient Distributed ADMM Algorithm,Yongwen Qiu,2023,,0
c236c1719945b115af2f39b9d825e3e53df2efa6,A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models,Bilel Guetarni,2023,08,0
819c9542d80da3c0ab2889090eefe75b181386d9,A Theory for Emergence of Complex Skills in Language Models,Sanjeev Arora,2023,07,3
d96454e89f1228bc7cdbaad211f3779574aa2b2c,The semantic landscape paradigm for neural networks,Shreyas Gokhale,2023,07,0
e9724e31b870b6e333f657a7b381325918e31ddc,Scaling Laws for Imitation Learning in NetHack,Jens Tuyls,2023,07,0
e3dc6559baad16f2edb8b03add561834e2e9439c,An Empirical Investigation of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration,Hiroki Naganuma,2023,07,0
27419d289f321bf194e020bc1cf504d9d3beb299,Noise2Recon: Enabling SNR‐robust MRI reconstruction with semi‐supervised and self‐supervised learning,Arjun D Desai,2023,,3
f581b2f1f2998ab4193e53bec7672f6f8948fc8a,"Choice Architecture, Privacy Valuations, and Selection Bias in Consumer Data",Tesary Lin,2023,08,0
bea7271c6d43efca381755551522a4f631ea6714,Automated T1 and T2 mapping segmentation on cardiovascular magnetic resonance imaging using deep learning,András Kalapos,2023,,0
494b043fce4da2ecc7f87bc96f7c29a5278cca61,Frontier AI Regulation: Managing Emerging Risks to Public Safety,Markus Anderljung,2023,07,12
43bf1f91cb31726725a01aea268a69ae4d84523f,The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit,Lorenzo Noci,2023,06,5
a20c6ce80872dbc6a8e6403b2a366973061a9f89,Scaling Laws for Discriminative Speech Recognition Rescoring Models,Yile Gu,2023,06,1
a46f081d7dcf87f0e3edccd2a738aaedcc60ea8d,Towards Sybil Resilience in Decentralized Learning,Thomas Werthenbach,2023,06,0
72833a1a9a154b9f5b670f9940586ae03d5598fb,Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data,Alycia Lee,2023,06,2
61e608a70faefb8faaf92124c4a8a7a8bf1fe099,Scaling MLPs: A Tale of Inductive Bias,Gregor Bachmann,2023,06,2
295ab302cb09c20c12ae5363cc9b9b5e0d75b8e6,On Hate Scaling Laws For Data-Swamps,A. Birhane,2023,06,2
2922768fd451ecdb45f48c1a83eb57f54a91221b,Textbooks Are All You Need,Suriya Gunasekar,2023,06,25
3670e1c5eecddcf6462b9f8571ba65084b00d550,A Deep‐Learning Ensemble Method to Detect Atmospheric Rivers and Its Application to Projected Changes in Precipitation Regime,Yuan Tian,2023,,0
6294f078e79828cac21e717813e8f3d02b18a97c,The importance of resource awareness in artificial intelligence for healthcare,Zhenge Jia,2023,,2
bfc5770dd7c4a3ba90d9a660909ed6936b001efd,Modern Data Pricing Models: Taxonomy and Comprehensive Survey,Xiaoye Miao,2023,06,0
52372c6e01d463420169a7b4ef9f68ae4b2d6e9d,"On the Joint Interaction of Models, Data, and Features",Yiding Jiang,2023,06,0
5db2dc69feb3d6d668151d88016f718b3f5afe5a,Leveraging human expert image annotations to improve pneumonia differentiation through human knowledge distillation,Daniel Schaudt,2023,,0
ac9c801bb5984f34ed21e564394012fab78f7590,Augmented Industrial Data-Driven Modeling Under the Curse of Dimensionality,Xiaoyu Jiang,2023,,3
31a68755ca6899e6c360ec8568704ae74f223a25,GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?,N. Ding,2023,06,0
403107745a725eb71623cec43df9a4e794340ae7,"Large-Batch, Iteration-Efficient Neural Bayesian Design Optimization",Navid Ansari,2023,06,0
0fc83abc4600e09c6c100242f9f8edf11b6ca9f0,MarginMatch: Improving Semi-Supervised Learning with Pseudo-Margins,Tiberiu Sosea,2023,08,0
01f7d12327ae81ca0d6414359155f25e764f0e8f,PubChemQC B3LYP/6-31G*//PM6 dataset: the Electronic Structures of 86 Million Molecules using B3LYP/6-31G* calculations,Maho Nakata,2023,05,0
f16e25394420bb15361539864fd4a9029ff0bf58,Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design,Ibrahim M. Alabdulmohsin,2023,05,2
e9b3e82b1c9eb4136df28e94f24cd823431be93b,Lifelong Language Pretraining with Distribution-Specialized Experts,Wuyang Chen,2023,05,2
3a8a09f9ca8ad711e02937795a9d8c9eb371bd76,A comparative study of model-centric and data-centric approaches in the development of cardiovascular disease risk prediction models in the UK Biobank,M. Mamouei,2023,,0
3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6,SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models,Shan Zhong,2023,05,3
6c3a34f23e8160f8e9adc494227aa276831fd1d1,Body size as a metric for the affordable world,Xinran Feng,2023,,0
07c86b23fd90ced230e8bca24c91de1742d9c232,Model-agnostic Measure of Generalization Difficulty,Akhilan Boopathy,2023,05,0
075e2259d81985a5d623f34f08209377311e2175,MncR: Late Integration Machine Learning Model for Classification of ncRNA Classes Using Sequence and Structural Encoding,Heiko Dunkel,2023,,1
d9da0c7a3addf12f18c354687cb3fb7a2a598705,Predictability of Machine Learning Algorithms and Related Feature Extraction Techniques,Yu-nan Dong,2023,05,0
ece77610adfb0fb162dd22ef694f2777393c319a,Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster,Nolan Dey,2023,04,26
d6c0c06891ce1ae6d5ede5eafa47e8da655d3c80,Multi-annotator Deep Learning: A Probabilistic Framework for Classification,M. Herde,2023,04,0
4a66d5b036e2a40cb39f1aa44072d508ebb15a0e,Predicting patient decompensation from continuous physiologic monitoring in the emergency department,Sameer Sundrani,2023,,0
1bba2a9c6db3b356b8ae6ef2efff5645e4d96c2b,Text-to-Image Diffusion Models are Zero-Shot Classifiers,Kevin Clark,2023,03,12
3ec5f0da304a606c5989de5b00e1246ee64b3e46,kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,Benfeng Xu,2023,03,13
a1058448710533ef3410e146ec9db400fa4a329d,The Quantization Model of Neural Scaling,Eric J. Michaud,2023,03,12
e00b40a7edac5d5a3b557e26727c62ce98855c26,ExplainFix: Explainable spatially fixed deep networks,Alex Gaudio,2023,03,1
2debe7236a3792db6cfea091db8021fbddbcd062,SemDeDup: Data-efficient learning at web-scale through semantic deduplication,Amro Abbas,2023,03,19
afdfae9bf54f220d41f375b4210eaf93ea305033,Semi-supervised Learning in Distributed Split Learning Architecture and IoT Applications,Mahdi Barhoush,2023,,1
42fc11d7a5d317bb467afa3b6f36dc5f3136b33f,Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks,Zahra Atashgahi,2023,03,4
ff4ac6c65100eb19024e187b3b6cb743a117fd60,Kernel Regression with Infinite-Width Neural Networks on Millions of Examples,Ben Adlam,2023,03,4
33a12f5c67a312adba869bad25ab1df2582c1899,Robust mmWave Beamforming by Self-Supervised Hybrid Deep Learning,Fenghao Zhu,2023,03,0
46d4fa1ce4ebd3a3280944b08cdadfbaf0282635,Spatio-Temporal Structure Consistency for Semi-supervised Medical Image Classification,Wen-Ling Lei,2023,03,0
8eb03afce0a06c72cfe5736edbb7b2c0ef410ba0,Implementation of Supervised Pre-Training Methods for Univariate Time Series Forecasting,Vidul Khanna,2023,,0
dab6c070ea92e33e4d70b38923a301b05052a968,A Meta-Learning Approach to Predicting Performance and Data Requirements,Achin Jain,2023,03,0
57e849d0de13ed5f91d086936296721d4ff75a75,LLaMA: Open and Efficient Foundation Language Models,Hugo Touvron,2023,02,1733
6c282567e9c452f81416214933b9b1e45ab3add4,The Dormant Neuron Phenomenon in Deep Reinforcement Learning,Ghada Sokar,2023,02,6
9a8645efed4a653023a287d0102f0a41afe5216d,Industrial Policy for Advanced AI: Compute Pricing and the Safety Tax,Mckay Jensen,2023,02,1
7af28fd91d91441ebbd029c002cb58d7de286210,Scaling Laws for Multilingual Neural Machine Translation,Patrick Fernandes,2023,02,7
b60e04fa6c497ea8fae3ff63e34a107054756442,Cliff-Learning,T. T. Wang,2023,02,0
712573cc74633ec2283724e868328fd2d319c091,"Ten Lessons We Have Learned in the New ""Sparseland"": A Short Handbook for Sparse Neural Network Researchers",Shiwei Liu,2023,02,13
9752ee1b191b8e5f69f667ae729c8dd32c6b9535,Power Laws for Hyperparameter Optimization,Arlind Kadra,2023,02,0
27be451186b3a1b0367591e5aa328eb9065e4e1f,A Closer Look at Few-shot Classification Again,Xu Luo,2023,01,1
468992bf970c37bd1fef58b78a6c2fcd8c018868,Scaling Laws for Generative Mixed-Modal Language Models,Armen Aghajanyan,2023,01,28
de80124e4e7977e1946e57f1c9dbbcd9ae425d96,Quality monitoring of projection welding using machine learning with small data sets,Johannes Koal,2023,,1
53535d38fe259a3aa7c911edd8048d764e09e8e1,The case for 4-bit precision: k-bit Inference Scaling Laws,Tim Dettmers,2022,12,39
a50f72f223004aedd2041e13e5d4468c4ee5be41,SkinFormer: Robust Vision Transformer for Automatic Skin Disease Identification,Mohamed Osman,2022,,0
924afcb1394807a4950fbcfe8da9263d3bed760c,Predicting the prevalence of complex genetic diseases from individual genotype profiles using capsule networks,Xiao Luo,2022,,1
e8aa5f51aaf29344174f90d7edca49cc153a6b00,Economic impacts of AI-augmented R&D,T. Besiroglu,2022,12,1
16de2006e2960ba410772c6b6d460b83c0a5cc4b,Reproducible Scaling Laws for Contrastive Language-Image Learning,Mehdi Cherti,2022,12,90
76181ce429bacac17394a9034aa51fe6d2bf840e,Whole slide image representation in bone marrow cytology,Youqing Mu,2022,,1
2e0caf5cc32390692eac3f9c7bdf5de3b19f7e04,Languages You Know Influence Those You Learn: Impact of Language Characteristics on Multi-Lingual Text-to-Text Transfer,Benjamin Muller,2022,12,2
dbf3686d10b9be177415f1a4550d8b18acaf2ee6,Matching DNN Compression and Cooperative Training with Resources and Data Availability,F. Malandrino,2022,12,0
06adfb6a76b7c69b678d0b81c5f3087939652102,Enhancements to Language Modeling Techniques for Adaptable Log Message Classification,Yusufu Shehu,2022,,0
e99422aa7044c8a42d9b69c833addd7c49800e45,How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective,Guang Yang,2022,11,3
fb49e38135302a1c16d644c0f746cef7d5f10ee4,Understanding BLOOM: An empirical study on diverse NLP tasks,Parag Dakle,2022,11,0
db0c318f9074ab5cba5675b601eb39a7aed44d66,A Survey of Learning Curves with Bad Behavior: or How More Data Need Not Lead to Better Performance,M. Loog,2022,11,1
96427f960b7fbb27c0ad693a4dc97f7ac3fae996,The role of artificial intelligence in the differential diagnosis of wheezing symptoms in children,L. Song,2022,,0
964bd39b546f0f6625ff3b9ef1083f797807ef2e,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,Teven Le Scao,2022,11,666
1cf9f7da2786af99cc503833c1e95366344a46f3,Mapping the Landscape of Care Providers’ Quality Assurance Approaches for AI in Diagnostic Imaging,C. Lundström,2022,,3
e24f4b28167b05fbf7d29000490fc0a4e4c109c7,eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers,Y. Balaji,2022,11,244
b1cd1c4119bddea1b2a2b06779d9aeee12882b9a,There Are Fewer Facts Than Words: Communication With A Growing Complexity,L. Debowski,2022,11,0
ae8bc0b22544db54c2bf711d1923f198b39a94ae,Weed detection to weed recognition: reviewing 50 years of research to identify constraints and opportunities for large-scale cropping systems,Guy R. Y. Coleman,2022,,4
f5b3cb14e0947c62b470d2072483481f14258738,A Solvable Model of Neural Scaling Laws,A. Maloney,2022,10,17
4b8ae99910c2a0226e01a6199da8e5fb56ee1e2a,Broken Neural Scaling Laws,Ethan Caballero,2022,10,25
e70724ae65ca650008f81ec846a63e1bcfb4e33e,Scaling Laws Beyond Backpropagation,M. Filipovich,2022,10,1
3035b84a64977b0498425e4aad63db4c6e01c5b1,'A net for everyone': fully personalized and unsupervised neural networks trained with longitudinal data from a single patient,Christian Strack,2022,10,0
31ee24ceedd6dec76836cff99762a2b82acb292f,Precision Machine Learning,Eric J. Michaud,2022,10,8
4172c0022966091d891e7953aa2a16dd52bcfe3c,Analysis of Data Parallelism Methods with Deep Neural Network,Ming Chen,2022,,0
b2f60d9aecb1e52c32e5ee2a0efa52c4443f2e03,Active Learning from the Web,R. Sato,2022,10,0
b941c4bea1a71066f6f32275641aea1efc99b21b,Meta-Principled Family of Hyperparameter Scaling Strategies,Sho Yaida,2022,10,4
c18ccfa8948a9138ab0211066657a382ba2bffb1,"A Generalizable Artificial Intelligence Model for COVID-19 Classification Task Using Chest X-ray Radiographs: Evaluated Over Four Clinical Datasets with 15, 097 Patients",Ran Zhang,2022,10,1
9c4a22ef4e6a8e365a7edf2d3d7a0e34a28585ab,Optimizing Data Collection for Machine Learning,Rafid Mahmood,2022,10,8
bf3c4c2d690c53936ee9ad5cac8857683f030e70,Superconducting optoelectronic single-photon synapses,Saeed Khan,2022,,13
b39d803f9e5f816744f10425fb9419828d03c301,Scaling Laws for a Multi-Agent Reinforcement Learning Model,Oren Neumann,2022,10,10
f94aa0d4c5c1dee834d6da0365bf0acf12f18850,Scaling Laws For Deep Learning Based Image Reconstruction,Tobit Klug,2022,09,8
1a133d1fa8a680b4a4944cc6d5c20bfdf963aed5,Local Grammar-Based Coding Revisited,L. Debowski,2022,09,0
7c4eeac5a82b387b1480f8fc7de6826732c6ad7d,Comparing 1-dimensional and 2-dimensional spectral feature representations in voice pathology detection using machine learning and deep learning classifiers,F. Javanmardi,2022,,2
53ebfcd5c98972de4a62f429512b2f17870f3a0a,Wi-Phrase: Deep Residual-Multihead Model for WiFi Sign Language Phrase Recognition,Nengbo Zhang,2022,,0
99934ef57a75f49afe68736cbc7dbf480687b552,Efficient Quantized Sparse Matrix Operations on Tensor Cores,Shigang Li,2022,09,4
fb9f9e98d35340875905730e1a80221fec818944,Revisiting Neural Scaling Laws in Language and Vision,Ibrahim M. Alabdulmohsin,2022,09,22
bf77f4305f9e990f56ff0a7939418350a0b8bfb6,Concept-Based Explanations for Tabular Data,Varsha Pendyala,2022,09,0
1d685aeeaffa17116baaace8058718f959a1bf8c,Self-Supervised Pretraining for 2D Medical Image Segmentation,András Kalapos,2022,09,8
25b79af99899876d38657983bb43b1aca2807ced,PercentMatch: Percentile-based Dynamic Thresholding for Multi-Label Semi-Supervised Classification,Jun Huang,2022,08,0
57390a1fab0b1137749de078d0df577c31254fe0,Deep learning image segmentation reveals patterns of UV reflectance evolution in passerine birds,Yichen He,2022,,0
93f9d29445a1236c0b1ab45026c2e308b9b74c15,Understanding Scaling Laws for Recommendation Models,Newsha Ardalani,2022,08,8
e3b67f86afe78dc222736550dcd03b0d8c527311,What Can Be Learnt With Wide Convolutional Neural Networks?,F. Cagnetta,2022,08,3
fb80f9efe1186a033f1f82fd742e458ac9b9f0fd,Semi-supervised Algorithms in Resource-constrained Edge Devices: An Overview and Experimental Comparison,Mahdi Barhoush,2022,,2
b169fdff50f00f348a53c83e705693caceb27d8c,Deep Learning Model for Prediction of Entanglement Molecular Weight of Polymers,Jihoon Park,2022,,0
4310dbe8f691c9b462bd920d97aa137e1e0a3aff,A Progressively Expanded Database for Automated Lung Sound Analysis: An Update,Fu-Shun Hsu,2022,,4
c5e4014c1660f759bcc58cdb4876ab7191defff4,Fast Heterogeneous Task Mapping for Reducing Edge DNN Latency,Murray L. Kornelsen,2022,,0
45122c8f76a4e2fd0163d1f0522db37e97ea4721,Beyond neural scaling laws: beating power law scaling via data pruning,Ben Sorscher,2022,06,127
016c3fc36fb926122f05817100db2a833c1b4dc3,Studying Generalization Through Data Averaging,C. Gomez-Uribe,2022,06,0
277d284363d9ae95ebea5ecaab30df41693cc6dd,Learning sparse features can lead to overfitting in neural networks,Leonardo Petrini,2022,06,9
f7e9e28fdbc458bb49f045e6143b427051457063,Supervised learning of random quantum circuits via scalable neural networks,S. Cantori,2022,06,1
036aef17a8c2c03202170aa85fd7be1dba974202,MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields,Ilyes Batatia,2022,06,83
8215f4e7dc7ea6f588bcbc9b0f4383672545594f,On Data Scaling in Masked Image Modeling,Zhenda Xie,2022,06,20
44791ef3cf02861b9ce085af632f84546d329c57,Trajectory-dependent Generalization Bounds for Deep Neural Networks via Fractional Brownian Motion,Chengli Tan,2022,06,1
d12ce2204eeefe5e1dc2a012d5ea949c8e135c44,Automatic recognition of flow cytometric phytoplankton functional groups using convolutional neural networks,Robin Fuchs,2022,,6
d3403aa9b57f69f85a61a84a83c0c5f2f284c97e,How Much More Data Do I Need? Estimating Requirements for Downstream Tasks,Rafid Mahmood,2022,07,8
8d7b6c23f159fc5f28283d1faf1e1e56165db033,Dist-PU: Positive-Unlabeled Learning from a Label Distribution Perspective,Yunrui Zhao,2022,12,9
2fbdae1ce5f993a42a66c43f03bcddb853af7963,Subverting Skynet: The Strategic Promise of Lethal Autonomous Weapons and the Perils of Exploitation,Lennart Maschmeyer,2022,,0
6e10343767ab09dde83cf99ea3442907402a9810,Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing,Linlu Qiu,2022,05,26
0008b1e49c3d4afe2cfffe82ea88be147b618504,Improving Short Text Classification With Augmented Data Using GPT-3,Salvador Balkus,2022,05,5
0c2e2769cd4d34bd459fe6129daaaf4755af9987,Investigating classification learning curves for automatically generated and labelled plant images,Michael A. Beck,2022,05,0
aa4d9972af3264d032dbee58501ed4ac49477103,Scaling Laws and Interpretability of Learning from Repeated Data,Danny Hernandez,2022,05,28
141ef16c41d7a0ab10e3f8c7ee06d7bdf131976f,Comparative Analysis of Transformers to Support Fine-Grained Emotion Detection in Short-Text Data,Robert H. Frye,2022,,0
f7820b52e3cdff6625e6bd0430a8d48ca66cca3f,Self-Programming Artificial Intelligence Using Code-Generating Language Models,Alex Sheng,2022,05,1
5032e61905f744bc753ab206ea9cbedf44bb4efa,Data augmentation and multimodal learning for predicting drug response in patient-derived xenografts from gene expressions and histology images,A. Partin,2022,04,3
6095f02d14a770f1691a134603e6404705ed7810,Demonstration of Superconducting Optoelectronic Single-Photon Synapses,Saeed Khan,2022,04,1
06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,Yizhong Wang,2022,04,208
aed45120bd674b9c801def63c25577de76348285,GemNet-OC: Developing Graph Neural Networks for Large and Diverse Molecular Simulation Datasets,J. Gasteiger,2022,04,21
cbd1588837e6c6b795d593f0269d92f295c09705,Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems,Zixuan Wang,2022,,6
3e6cc36d8db49aa472c9b57ec51383071a9e6336,Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels,Jiwon Kim,2022,03,7
0c97bcfa9a1fad61a47b99d2d48294ab9dceb9bc,Classification of Microscopic Images of Unstained Skin Samples Using Deep Learning Approach,KV Rajitha,2022,,2
318719849d813e99e43a38a49f929b2785ebf7a5,Trade-offs in Sampling and Search for Early-stage Interactive Text Classification,Zachary Levonian,2022,,2
a3e3a9d878999c7038c275e75f5cd8a232aa4999,SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities,Hsiang-Sheng Tsai,2022,03,54
7b7c21081abc432bbd9daba06846901658f9528e,Effect of Training Data Volume on Performance of Convolutional Neural Network Pneumothorax Classifiers,Y. Thian,2022,,4
7bd7562b015e12ef9a96bc31e7e5cd8963538822,"High-Grade Glioma Treatment Response Monitoring Biomarkers: A Position Statement on the Evidence Supporting the Use of Advanced MRI Techniques in the Clinic, and the Latest Bench-to-Bedside Developments. Part 2: Spectroscopy, Chemical Exchange Saturation, Multiparametric Imaging, and Radiomics",T. Booth,2022,,13
addbc8bd9e1d62aab5754e281f529b2db96a9433,Performance reserves in brain-imaging-based phenotype prediction,Marc-Andre Schulz,2022,,11
c5217f2687d78dbaef2efbbfb26cc856079a3492,Interpolation-based Contrastive Learning for Few-Label Semi-Supervised Learning,Xihong Yang,2022,02,21
ff91069896ebf1a948dc7214fa86d9d3ed0144ca,A Comprehensive Evaluation of Metabolomics Data Preprocessing Methods for Deep Learning,K. Abram,2022,,6
9f1753227e7dbbc3b79dec494982e96f1604b98e,Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness,Kate Donahue,2022,02,17
9cbc044e315cdefe9a255119037ac7c23e9abdd5,Predictability and Surprise in Large Generative Models,Deep Ganguli,2022,02,88
927a5203363fc9c8ba48599dc749cf0cc647444b,Compute Trends Across Three Eras of Machine Learning,J. Sevilla,2022,02,96
6c595950497cc5532698cf7e10b070838d79b2c5,Semantic Segmentation of Metoceanic Processes Using SAR Observations and Deep Learning,A. Colin,2022,,12
393e6c6f5e40faa5a307887fc7434e085341fd38,Distributed Deep Learning Training Using Silicon Photonic Switched Architectures,Ziyi Zhu,2022,,3
e4f04a221f9e13ddd8ddcf5fceafa10548353f24,Failure and success of the spectral bias prediction for Kernel Ridge Regression: the case of low-dimensional data,Umberto M. Tomasini,2022,02,10
b07ba6926a366a05f62cf32cebc635445297bf2c,Enhanced TabNet: Attentive Interpretable Tabular Learning for Hyperspectral Image Classification,Chiranjibi Shah,2022,,20
88e41c56e4dbe8b4b7af8aa8a24a1ac17e040286,"DASHA: Distributed Nonconvex Optimization with Communication Compression, Optimal Oracle Complexity, and No Client Synchronization",A. Tyurin,2022,02,12
4779d7f695413669d06cd8b84ece1a4aee9854c9,Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data,Sven Burdorf,2022,02,1
3cee0b563cd8872378eae52c55acbef896e56b76,Error scaling laws for kernel classification under source and capacity conditions,Hugo Cui,2022,01,6
773f1fcbaa6b5752c098766e3423e5ebfc7e4d50,Deep Learning Empowered Wearable-Based Behavior Recognition for Search and Rescue Dogs,P. Kasnesis,2022,,11
bb1d007aa8241b70f2ce7d1dd2cdb780cebe833e,Cerebro: Static Subsuming Mutant Selection,Aayush Garg,2021,12,14
a0afba8d6a30c653639387bece2218c366904f37,On the Impact of Dataset Size:A Twitter Classification Case Study,Thi-Huyen Nguyen,2021,,0
d1a082fb5e84420c68ecb4bcdfa029d7771f2d02,A Transferable Approach for Partitioning Machine Learning Models on Multi-Chip-Modules,Xinfeng Xie,2021,12,2
5264cbc294b1fbc0aed4653703ab84c600bf34a3,Fairness in model-sharing games,Kate Donahue,2021,12,11
fc321787220809e6a6accf691e793759772682d1,AugLiChem: data augmentation library of chemical structures for machine learning,Rishikesh Magar,2021,11,8
96e15da20453c86ee269d3fe52ca04bb4bc9c5a6,Deep Learning for automated phase segmentation in EBSD maps. A case study in Dual Phase steel microstructures,T. M. Ostormujof,2021,12,12
2ca09ee92154b4480230654989a3ecec6b29ef10,Turing-Universal Learners with Optimal Scaling Laws,Preetum Nakkiran,2021,11,1
865bc9d894a55f3e77ea29f11691f8bf550571d5,Emerging trends: A gentle introduction to fine-tuning,Kenneth Ward Church,2021,,15
90fa397436474f9b8937c185b85e83b8982b7038,Learning curves for Gaussian process regression with power-law priors and targets,Hui Jin,2021,10,11
c5fbf9a62a91e3182f65e3746d3263387effa4a7,The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail,Sam Bowman,2021,10,31
a50eac66607a8fd3eb69a182a45dac035a739204,Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations,Xinyu Zhang,2021,10,7
82e182b1a50135a353ee5d8c4fcf4cefcdfbd39d,Distribution-Preserving-Based Automatic Data Augmentation for Deep Image Steganalysis,Jiansong Zhang,2021,,3
9c392c7d79a28f6b6825c0193f1d1695ad1e73b5,Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers,Gabriele Prato,2021,10,6
e88f2a61fe04d2cb8a5929c28e677e9dbdca9b5c,RankingMatch: Delving into Semi-Supervised Learning with Consistency Regularization and Ranking Loss,Trung Q. Tran,2021,10,2
1982ea74e37b4e98819fb2f4202ebb2fdf86ee97,Unsupervised Selective Labeling for More Effective Semi-supervised Learning,Xudong Wang,2021,10,8
5e4e052278d30caeba13af26871df01122800c95,Max and Coincidence Neurons in Neural Networks,Albert Lee,2021,10,1
ef25af482d338ecd8e6d244b3756ef1f371e2c01,Noise2Recon: Enabling Joint MRI Reconstruction and Denoising with Semi-Supervised and Self-Supervised Learning,Arjun D Desai,2021,10,2
f581e1789824ad03d9cab89aaf5ec413f42b3afc,Robust Temporal Ensembling for Learning with Noisy Labels,Abel Brown,2021,09,0
05c2e1ee203be217f100d2da05bdcc52004f00b6,Unsolved Problems in ML Safety,Dan Hendrycks,2021,09,136
ccbd1c215944aecc8da2984e36619939c6bf8096,Is the Number of Trainable Parameters All That Actually Matters?,A. Chatelain,2021,09,2
05f5cf4ee6cdfde52a6632a642090dfb1ad17836,Deep Learning and Its Application to Function Approximation for MR in Medicine: An Overview,H. Takeshima,2021,,1
de1fdaf92488f2f33ddc0272628c8543778d0da9,Scaling Laws for Neural Machine Translation,B. Ghorbani,2021,09,54
8725d4f3222bcb03690e59129ce7cfe49de3fe59,Formalizing and Estimating Distribution Inference Risks,Anshuman Suri,2021,09,21
ff69d758764157e612f92f97a987838312c568a9,Compute and Energy Consumption Trends in Deep Learning Inference,Radosvet Desislavov,2021,09,34
5e7cd340ac25a103eed8ee8ab85601f1d89abf52,ShareLoc – an open platform for sharing localization microscopy data,Jiachuan Bai,2021,,0
2f98ab0aab23200b3c1803279c14924bcc7a5025,A Refutation of Finite-State Language Models through Zipf’s Law for Factual Knowledge,L. Debowski,2021,,3
27e63284b1adb0280bfb2963fda048108ad560e8,Why and How Governments Should Monitor AI Development,Jess Whittlestone,2021,08,16
6b827b800091fa7b1171b265846bf226cd92d3e2,Overview of Machine Learning Process Modelling,B. Brumen,2021,,4
b1e509bfee5f08dffc1e1b247c63a0260ed09fcc,A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?,Hiroaki Mikami,2021,08,8
7be5f6f8a8773d9cbdddd94cb8f2c0be4c4ef9c9,OpenCNN: A Winograd Minimal Filtering Algorithm Implementation in CUDA,Roberto L. Castro,2021,,2
f4349e6cdbe2095b6bde774e051efb88ebc62fe6,Applications of machine learning to undifferentiated chest pain in the emergency department: A systematic review,J. Stewart,2021,,9
ddbbb0fc48d897e433f10068314019fe419c8f44,Scalable Bayesian Transport Maps for High-Dimensional Non-Gaussian Spatial Fields,M. Katzfuss,2021,08,5
e3681b0ebc4bb4ae26071bba105b74859e5e99bb,A Deep Learning application for placing points of plumage regions on avian specimen images,Yichen He,2021,,0
fd411fe642c189c98df880eab0d18280b7dbdafc,On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models,Z. Emam,2021,08,5
39bf40fec9bb8a68198471d86bbd8b5a763d47be,Dataset Distillation with Infinitely Wide Convolutional Networks,Timothy Nguyen,2021,07,92
edd7fca50558906e6cc44053c1d6944c152c6f4d,The Value of Data and Its Impact on Competition,M. Iansiti,2021,,8
67e8084034a94c2f4649c2d9a03df33f88a96533,Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization,Divya Shanmugam,2021,07,6
1e3c328c60a5b7237be77595aa7e60f9cc1dabb9,A factorisation-aware matrix element emulator,D. Maître,2021,07,15
40bdb593420763ac7a779aa905183fed8ded572c,A dual-purpose deep learning model for auscultated lung and tracheal sound analysis based on mixed set training,Fu-Shun Hsu,2021,07,0
5efb3395cd4b8749c50dba2df46d0ccc62cd1a76,Structured Model Pruning of Convolutional Networks on Tensor Processing Units,Kongtao Chen,2021,07,45
b1668b29ec1ac378fe81fc3dbbfbd4a920440d5d,Meta-Learning Amidst Heterogeneity and Ambiguity,Kyeongryeol Go,2021,07,0
180f4882e5e246d2b587046a834d49a166da2b71,Design of Experiments and Optimization of Laser-Induced Graphene,Richard M. Murray,2021,,15
2922126aa07e6a29eb30fbfb3bdcaf86eb371b3b,Hi-BEHRT: Hierarchical Transformer-Based Model for Accurate Prediction of Clinical Events Using Multimodal Longitudinal Electronic Health Records,Yikuan Li,2021,06,23
110de4ef336804733f86e18dbb52f5d84b1769d4,Data Optimisation for a Deep Learning Recommender System,Gustav Hertz,2021,06,0
b8234574e5eb689d0ab11f2a37f9660663f75afb,Locality defeats the curse of dimensionality in convolutional teacher–student scenarios,Alessandro Favero,2021,06,25
1cf1a77994205a6a181b4c87011719f950d7d5bd,Top squark signal significance enhancement by different Machine Learning Algorithms,Fraga-Nodarse Jorge,2021,06,0
28a54945a3f7f4e607b2c4b9a791ab508fa19f0b,Redundant representations help generalization in wide neural networks,Diego Doimo,2021,06,2
28141ca1ce5a2c536358d3934a23045b837e2d0c,Self-Supervision is All You Need for Solving Rubik's Cube,Kyo Takano,2021,06,0
4862141c0283502fe30d0c3b2f01c87b30fd15dd,Search Spaces for Neural Model Training,Darko Stosic,2021,05,4
fe0081a3e766d4fa112a40e3cf4051f52126b5f5,Detection of oedema on optical coherence tomography images using deep learning model trained on noisy clinical data,I. Potapenko,2021,,6
76766e08874a75ec16ce6bf7fb116bf15a99205c,A Theoretical-Empirical Approach to Estimating Sample Complexity of DNNs,Devansh Bisla,2021,05,4
6f59cb30a6070df32c2c30888e56359ab0d94402,"Can ""Conscious Data Contribution"" Help Users to Exert ""Data Leverage"" Against Technology Companies?",Nicholas Vincent,2021,,6
2b567f4280b8528d7acb5028b87000e0fab8abbd,Landscape and training regimes in deep learning,M. Geiger,2021,,27
4454f82cee21dd0e681cc187bdff122b60f516a0,A Deep Neural Network Model for Speaker Identification,F. Ye,2021,,36
afbb0a31cc96c779fc363b65998a50f7b159e383,Scaling Scaling Laws with Board Games,Andrew Jones,2021,04,15
fee62ad23c88c3f395cdf39d26af84c41bf5aaeb,Application of Supervised Machine Learning to Recognize Competent Level and Mixed Antinuclear Antibody Patterns Based on ICAP International Consensus,Yi-Da Wu,2021,,5
2373ab9ba96403dedc9fb8da2b393a5927965c32,Vulnerability Due to Training Order in Split Learning,Harshit Madaan,2021,03,3
21ec9c0f869bdb33b06c7dbc8880169db0397d08,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,Nicholas Lourie,2021,03,89
0635138addc322a51c0a3df0ef3232f58490840d,The Shape of Learning Curves: A Review,T. Viering,2021,03,57
be0c0dbe570ea7f4f3470e7579ac765c63beaf16,The Low-Rank Simplicity Bias in Deep Networks,Minyoung Huh,2021,03,57
3836de2cb7300624e656c33aa1dd287fc138a69d,Is it enough to optimize CNN architectures on ImageNet?,Lukas Tuggener,2021,03,11
91bfa0458e09f094e110ca657a3fa10284761e8d,Data Augmentation and Dense-LSTM for Human Activity Recognition Using WiFi Signal,Jin Zhang,2021,,69
9efe8dbde586d6248ecfc69f08b918012e2ac478,Revisiting ResNets: Improved Training and Scaling Strategies,Irwan Bello,2021,03,191
39475b969947d67f0f3904b55a8399fe8191e2b4,Integration of Convolutional Neural Networks in Mobile Applications,Roger Creus Castanyer,2021,03,9
8cece578eea5a59f8d87ae55af9dc31cf58bc54c,Cancer Type Classification in Liquid Biopsies Based on Sparse Mutational Profiles Enabled through Data Augmentation and Integration,Alexandra Danyi,2021,,5
1d65242f2483bf51fe890878cc33cf9e7238b2cb,Towards Cross-Lingual Generalization of Translation Gender Bias,Won Ik Cho,2021,,11
6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4,Learning Transferable Visual Models From Natural Language Supervision,Alec Radford,2021,03,7903
7fd8c7f383211d373696fdd0ad85294bfbfebb2c,Efficient Client Contribution Evaluation for Horizontal Federated Learning,Jie Zhao,2021,02,15
9d0d614ef0170386fff42b5af56dc8088a68bcfb,Machine learning in pharmacometrics: Opportunities and challenges,Mason McComb,2021,,38
38f06d0f7d6dd02be5d71330b1b2ee9f2616095e,Performance of a fully‐automated system on a WHO malaria microscopy evaluation slide set,M. Horning,2021,,18
86c03f6a3ae8d04d6451197a230b34d2551218a7,Physics-informed machine learning: case studies for weather and climate modelling,K. Kashinath,2021,,213
dac4e6b4214d594974d713bcc8b361af80cfb31b,"Time Dependency, Data Flow, and Competitive Advantage",E. Valavi,2021,03,0
6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b,Explaining Neural Scaling Laws,Yasaman Bahri,2021,02,94
9ca4a8cdd1ef6677c5f33840a498bd7cfe761caf,A Position Statement on the Utility of Interval Imaging in Standard of Care Brain Tumour Management: Defining the Evidence Gap and Opportunities for Future Research,T. Booth,2021,,11
f19b5176a879f2fd8db452a715f30eb3a704c665,Machine Learning for Sleep Apnea Detection with Unattended Sleep Monitoring at Home,Stein Kristiansen,2021,,12
c082a42a29913ce04cee308338da9ca7b9c23c36,An Update of a Progressively Expanded Database for Automated Lung Sound Analysis,Fu-Shun Hsu,2021,02,6
07e420f5b350269aab6694cb19a49799874dadb3,Learning Curve Theory,Marcus Hutter,2021,02,29
4d2ef50e50252ab800478e4b06b522b832e03ff3,Network Support for High-Performance Distributed Machine Learning,F. Malandrino,2021,02,4
a629b92beb971b6d529f87df1dca84ba9eae2047,Benchmarking of eight recurrent neural network variants for breath phase and adventitious sound detection on a self-developed open-access lung sound database—HF_Lung_V1,Fu-Shun Hsu,2021,02,26
6a486afb97659125476ee6cfe7d1f1b4c5ff9d23,Identification of Crop Type in Crowdsourced Road View Photos with Deep Convolutional Neural Network,Fangming Wu,2021,,16
d08167fd8583b0f70ba8a26821c29ea8af420826,Benchmarking graph neural networks for materials chemistry,Victor Fung,2021,,109
0233cd95dd0bc327dd72a14d60216c98021250ab,Responsible AI Challenges in End-to-end Machine Learning,Steven Euijong Whang,2021,01,4
7456dea3a3646f2df6392773a196a5abd0d53b11,E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials,Simon L. Batzner,2021,01,438
e4c9bf95fba763af45a1c20d84861c146b31d98d,A Clinical Evaluation of a Low-Cost Strain Gauge Respiration Belt and Machine Learning to Detect Sleep Apnea,Stein Kristiansen,2021,01,3
3935d95fde314dbea3fde5c1f218fd9c66dafc78,Illusion of large on-chip memory by networked computing chips for neural network inference,R. Radway,2021,,0
a6a775824e65febf3750f246fea91937715230e7,"Perspective: A Phase Diagram for Deep Learning unifying Jamming, Feature Learning and Lazy Training",M. Geiger,2020,12,6
eae695ffd7ea8db1fdf9971544f6ddcd41a2d99e,"Analysis of the Scalability of a Deep-Learning Network for Steganography ""Into the Wild""",Hugo Ruiz,2020,12,7
f12051f407cd7da491ae5c965d6b03500450fa50,UMLS-based data augmentation for natural language processing of clinical research literature,Tian Kang,2020,,27
028035b89da7cfccb41bf9639d82cc38cc8098a8,Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies,Nicholas Vincent,2020,12,28
d3edc20ed4a07195f3663abc0ead4220266fd75b,*-CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task,D. Tsarkov,2020,12,15
673702d188c843b3e495e0037e30bf811fde5ebb,Generalization bounds for deep learning,Guillermo Valle Pérez,2020,12,28
20e0d17ed60bd293e53ba286851c25267d490eff,Strategies to improve deep learning-based salivary gland segmentation,W. van Rooij,2020,,4
eef4d10927c7830534ca7599bd6a0fe1d7d89da7,Combining Direct and Indirect User Data for Calculating Social Impact Indicators of Products in Developing Countries,Bryan J. Stringham,2020,,4
b1b54df9b9fc206341877c2bc6f6d291618e0405,Learning curves for drug response prediction in cancer cell lines,A. Partin,2020,11,11
81ec0ea6872a0f46e558feb7a5e807b8dc6d0d1c,Pre-Trained Deep Convolutional Neural Network for Clostridioides Difficile Bacteria Cytotoxicity Classification Based on Fluorescence Images,Andrzej Brodzicki,2020,,19
8af6fd9c299675a65ff0b34df2417f5ef158c72b,Feasibility and Assessment of a Machine Learning-Based Predictive Model of Outcome After Lumbar Decompression Surgery,A. André,2020,,7
4f55f6fbbd53679a71bd943da0c79d6b9e64fc28,Power-law scaling to assist with key challenges in artificial intelligence,Yuval Meir,2020,11,9
71446ab82087505d7849b1b6f1fce4dcda751c1d,Design Patterns for Resource-Constrained Automated Deep-Learning Methods,Lukas Tuggener,2020,,3
bcd81cd90ad587ecf0a8086f171f0de4cb888d28,Understanding Capacity-Driven Scale-Out Neural Recommendation Inference,Michael Lui,2020,11,39
daeb93c0419ee9af5d6c785ebe1e75c9fb3ea73f,Deep Learning and Financial Stability,G. Gensler,2020,,10
bf766d61bdbf53369e9b10c8c895f0616b7b0d4f,When BERT meets Bilbo: a learning curve analysis of pretrained language model on disease classification,Xuedong Li,2020,,2
9e104d440540d2ffc9caaa0952a9e5f7f9344ba9,Are wider nets better given the same number of parameters?,A. Golubeva,2020,10,32
f620d71fccdf3efad7be1748d40eaadea5c9d6dd,CopyPaste: An Augmentation Method for Speech Emotion Recognition,R. Pappagari,2020,10,22
10c13de627f438488fcdf17aa19969769754f20f,Decoding subjective emotional arousal from EEG during an immersive virtual reality experience,Simon M. Hofmann,2020,,23
9c4f0ccf63304361e257e7c82ff551966e49dd0c,"Deep Learning is Singular, and That's Good",Daniel Murfet,2020,10,10
e585f6e752fb2668b33f7d4b18c8af9bd5abc1a4,The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research,N. Ahmed,2020,10,63
8d90f9474b7432a1aabab113dbeca38ead42117a,Learning Curves for Analysis of Deep Networks,Derek Hoiem,2020,10,15
378bfce88ed3139f48fba4deeafc96846c31251d,Transferable Graph Optimizers for ML Compilers,Yanqi Zhou,2020,10,44
8c3436cd58cabb697350cab2a571d17e0c0e1381,Video Big Data Analytics in the Cloud: Research Issues and Challenges,A. Alam,2020,11,0
67f2796edad53affbae4bd3ea907e4dd613edfae,AI in the treatment of fertility: key considerations,J. Swain,2020,,17
8f980841cfab520ede9fafbbb92473940559e591,Pruning Convolutional Filters Using Batch Bridgeout,Najeeb Khan,2020,09,3
814d89c858050e3fa612948735aa4119777ce7a7,Automated detection of Hainan gibbon calls for passive acoustic monitoring,Emmanuel Dufourq,2020,,32
1434743c8b832e736478e1579a3ad8213487bdf7,A Single-Shot Generalized Device Placement for Large Dataflow Graphs,Yanqi Zhou,2020,,5
f407f3f88b2ec23ba47be374f8bdebed5817b375,Time and the Value of Data,E. Valavi,2020,03,12
caf56a5780d8aff16557af72964aa1fbcea1f8f9,"Prospective Deployment of Deep Learning in MRI: A Framework for Important Considerations, Challenges, and Recommendations for Best Practices",A. Chaudhari,2020,,46
2eb64c9dc6feea15e4c7b913e004e3cc520793b4,Action-Based Representation Learning for Autonomous Driving,Yi Xiao,2020,08,7
9faf9029981a0b92abdb983a60f2af96eca3aea2,"Video Big Data Analytics in the Cloud: A Reference Architecture, Survey, Opportunities, and Open Research Issues",A. Alam,2020,11,13
f79c50d92320ebd9c6dfc1725722271a5ab4e8fb,Model-to-Data Approach for Deep Learning in Optical Coherence Tomography Intraretinal Fluid Segmentation,Nihaal Mehta,2020,,25
8b1f482bf6734b2c2db95313e5d85129555f21f8,Geometric compression of invariant manifolds in neural networks,J. Paccolat,2020,07,29
c497375f792ebcc7298fdee80e00fe27e05270ce,Learning to Transfer Learn: Reinforcement Learning-Based Selection for Adaptive Transfer Learning,Linchao Zhu,2020,,8
5847432f923a2e851d3c2adb5b17a2ba731e2853,Add a SideNet to your MainNet,Adrien Morisot,2020,07,0
dd377d9132b2d89532032d69c342927f7b4a00a2,"Small Data, Big Decisions: Model Selection in the Small-Data Regime",J. Bornschein,2020,09,30
91cfd15b587c5ed604e7e49326db6d045276c2a5,The Computational Limits of Deep Learning,Neil C. Thompson,2020,07,289
0433da60929b31857a29f5269cab90b681c10d06,Artificial intelligence for clinical decision support in neurology,M. Pedersen,2020,,29
333f31f38bc41ba9547c121b846c1f81eca45e22,Data Preprocessing Technique for Neural Networks Based on Image Represented by a Fuzzy Function,P. Hurtík,2020,,24
198ee49d849726c6b15a9b362ef71e252201971e,Generalized Brain Computer Interface System for EEG Imaginary Speech Recognition,Ana-Luiza Rusnac,2020,,1
45eed86953cfe055342b68d13b913789d24e1ad9,"Is SGD a Bayesian sampler? Well, almost",Chris Mingard,2020,06,28
9c467b90fbbd127c287237a73194c358d5f91d11,Minimum Cost Active Labeling,Hang Qiu,2020,,1
1e1c6b3a4570adecb7b2738452839bc4668467c2,Pixels-off: Data-augmentation Complementary Solution for Deep-learning Steganalysis,Mehdi Yedroudj,2020,,11
6ea7201aad5d146ba481051d26b884d19a34af15,On the Predictability of Pruning Across Scales,Jonathan S. Rosenfeld,2020,06,28
3e7f5f4382ac6f9c4fef6197dd21abf74456acd1,Big Self-Supervised Models are Strong Semi-Supervised Learners,Ting Chen,2020,06,1564
dba9b89cf19952adfaa44aa2c5c7c47085c043d5,Assessing the Robustness of Frequency-Domain Ultrasound Beamforming Using Deep Neural Networks,Adam C. Luchies,2020,,10
81815d9a847e406f8d49fb5051e2ae1055e13208,To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks,Sinong Wang,2020,06,14
6b85b63579a916f705a8e10a49bd8d849d91b1fc,Language Models are Few-Shot Learners,Tom B. Brown,2020,05,9998
a5b12df354d3b526b85db644cfeb846c10f8055b,Photonic Switched Optically Connected Memory: An Approach to Address Memory Challenges in Deep Learning,Ziyi Zhu,2020,,7
ace560cc01f5a6b1b4efcc6cb7b4ed273a6eb177,Performance Analysis of Distributed and Scalable Deep Learning,Sean Mahon,2020,,2
fdc4a9e807dbdaa38507b7d2e5e36368abf60e87,Synthetic Crowd and Pedestrian Generator for Deep Learning Problems,Anish R. Khadka,2020,,3
0a205e88690b934ec937ed6530702050b6598454,Ground State Energy Functional with Hartree-Fock Efficiency and Chemical Accuracy.,Yixiao Chen,2020,05,39
0a78085721f70d82c1284c124c3137bb7c2b34e7,nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer,Réka Hollandi,2020,,125
5b4be79081e1d6eac83e8a9e7a38bd7c338a4c78,How fine can fine-tuning be? Learning efficient language models,Evani Radiya-Dixit,2020,04,37
e3afb7a33b5895c839a76fcd09957805578c4dd5,The Big Data razor,Ezequiel López-Rubio,2020,,0
44c27b3a62316e11815d895b067f4f001d78e7b7,Transfer learning with chest X-rays for ER patient classification,J. Stubblefield,2020,,9
6757e301bf68f10e24040efce2654fe635b08ab7,Embedded Large–Scale Handwritten Chinese Character Recognition,Y. Chherawala,2020,04,4
ae6872803e1c091e3c19c55b514723194fc2fef2,Harnessing data science to advance radiation oncology,I. Vogelius,2020,,13
1d1d59f76a1cda8b6982674d1e293820342c9e8b,Leveraging Gans to Improve Continuous Path Keyboard Input Models,Akash Mehra,2020,04,4
21207fbe0431c6ca971af74d5fc7fe7fdaddeef4,Slice Tuner: A Selective Data Acquisition Framework for Accurate and Fair Machine Learning Models,Ki Hyun Tae,2020,03,21
64499484b88e4766cf078aecbadea20f171c0921,SuperMix: Supervising the Mixing Data Augmentation,Ali Dabouei,2020,03,57
8094f024d793c3c304df13071fe57d5a0b705184,Slice Tuner: A Selective Data Collection Framework for Accurate and Fair Machine Learning Models,Ki Hyun Tae,2020,,7
0ad680afa07570b5bc00cded45341c3d7f496add,The Big Data razor,Ezequiel López-Rubio,2020,,1
77a65e190f0815a6ca4755c1f7bc0d71bf4ebfe2,Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks,Blake Bordelon,2020,02,133
98c9e9aea69ad04052c6d5bd56baa834a90d46ce,Evolving a Deep Neural Network Training Time Estimator,F. Pinel,2020,,2
e6c561d02500b2596a230b341a8eb8b921ca5bf2,Scaling Laws for Neural Language Models,J. Kaplan,2020,01,1440
13453ebc220e8f814a7d3f4c15308a3394c21b02,Inference Over Wireless IoT Links With Importance-Filtered Updates,I. Nikoloska,2020,01,0
299847adf3ee558a760475ffa364facac3ebbb16,FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence,Kihyuk Sohn,2020,01,1955
eafd2b85ce74ed8f7ac1d824363b0501b6f87401,Recognizing New Classes with Synthetic Data in the Loop: Application to Traffic Sign Recognition,Gabriel Villalonga,2020,,8
6a08fe36a687392b25d8dcb98d19bdb2a05e91dd,Designing for the Long Tail of Machine Learning,Martin Lindvall,2020,01,2
665c4c2e26c38f2ac1521e92660594190294a60f,Social and Governance Implications of Improved Data Efficiency,Aaron David Tucker,2020,01,9
4a32c1e360eb54dec75b60bbdf3a8c79fa083d91,Value-laden disciplinary shifts in machine learning,Ravit Dotan,2019,12,38
d3a5789a7dc8961281383ffcecdf8d3efc7fe1d9,Ordalia: Deep Learning Hyperparameter Search via Generalization Error Bounds Extrapolation,Benedetto Buratti,2019,,1
638c7b167d3265f4bc12b34538a28a16f08deffc,Rapid Knee MRI Acquisition and Analysis Techniques for Imaging Osteoarthritis,A. Chaudhari,2019,,38
2c1c847bfb4f8558f899e41e6b1c8bb521673289,Using error decay prediction to overcome practical issues of deep active learning for named entity recognition,Haw-Shiuan Chang,2019,,10
174822c00230a2d4758fbb0a7acc75a52eb37990,Overcoming Practical Issues of Deep Active Learning and its Applications on Named Entity Recognition,Haw-Shiuan Chang,2019,11,3
1aa8e07ed6b43f4c0599f0da99aa126b7cb639cf,Estimating the Required Training Dataset Size for Transmitter Classification Using Deep Learning,T. Oyedare,2019,,20
8d4ee285ea54841b8b9953aa7c325b96d636d6d7,Deep-learning-based image segmentation integrated with optical microscopy for automatically searching for two-dimensional materials,S. Masubuchi,2019,10,83
3cfb319689f06bf04c2e28399361f414ca32c4b3,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,Colin Raffel,2019,10,9706
478605a27adb1ab16d1728e51058064a888d59cb,Machine Learning Models for Abnormality Detection in Musculoskeletal Radiographs,Govind Chada,2019,,9
c5a6931d3c400b6f1974428de07cffbeb0b8ea49,Engineering Reliable Deep Learning Systems,P. Santhanam,2019,10,13
b68485f9cd888227a843273eb5a1b066066c30f0,An Empirical Study of the Relation Between Network Architecture and Complexity,Emir Konuk,2019,11,6
d28c18a3c2a0afdc0a8634d18345af8d36e1f948,A Constructive Prediction of the Generalization Error Across Scales,Jonathan S. Rosenfeld,2019,09,121
17b6829678802a20e51558ec28c5369414defe42,Data Valuation using Reinforcement Learning,Jinsung Yoon,2019,09,99
b4a4e6fe63426b9fb4393c939792da8e7c4a1a59,GDP: Generalized Device Placement for Dataflow Graphs,Yanqi Zhou,2019,10,32
f01ba18059246336bc90a8d1c2539be3d100a6fc,Improving Differentially Private Models with Active Learning,Zhengli Zhao,2019,10,4
7de6cd5763f697e94e08db8ad8a9121858afae72,Nonrivalry and the Economics of Data,C. I. Jones,2019,,230
bb85eeda0d19501ec875c41d71e368bde339fbe2,Learning to Transfer Learn,Linchao Zhu,2019,08,5
9fe69cf5c104b2205cdb7908df8cdb389256b4b5,TabNet: Attentive Interpretable Tabular Learning,Sercan Ö. Arik,2019,08,558
ba00ad7a16aea83432f58e02063db9f9168be80c,P2L: Predicting Transfer Learning for Images and Semantic Relations,Bishwaranjan Bhattacharjee,2019,08,9
d2a2be6ce932a0f1939f31cfff4d64ea3d76723d,Human Uncertainty Makes Classification More Robust,Joshua C. Peterson,2019,08,190
44670a05f6cb7285e80d82b799f4c2d4f3088885,Less (Data) Is More: Why Small Data Holds the Key to the Future of Artificial Intelligence,C. Greco,2019,07,3
c17985a669522e7e85ae3d34754c7df49c7187d1,Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges,N. Arivazhagan,2019,07,316
7a906a1df823bf0b53a23d1bf246972fb4162f80,Minimizers of the Empirical Risk and Risk Monotonicity,M. Loog,2019,07,25
0690a3eee705b85038c33e9469bec0af82014012,Deep Learning-Based Delineation of Head and Neck Organs at Risk: Geometric and Dosimetric Evaluation.,W. van Rooij,2019,,74
702e2a6502b6791e10be728e372755f21f046b73,The impact of the AC922 Architecture on Performance of Deep Neural Network Training,P. Rosciszewski,2019,,2
b06d260b504a20e90d07180aa3c4eaecb3b5cde5,Selection Via Proxy: Efficient Data Selection For Deep Learning,Cody A. Coleman,2019,06,146
7663669b9dacabe6d1fed68a6c1807bbaa29cfa4,Power law scaling of test error versus number of training images for deep convolutional neural networks,Vittorio Sala,2019,,5
572b6fe8d75032c159896c644319dacdf594e9c6,One Epoch Is All You Need,Aran Komatsuzaki,2019,06,29
b9fac0f5936a18df98841c66e14f6a2f13554e91,Artificial neural networks training acceleration through network science strategies,Luciana Cavallaro,2019,,6
6145c1b646b5a1b751b5ae439c758d358265006a,Landslide Geohazard Assessment with Convolutional Neural Networks Using Sentinel-2 Imagery Data,S. Ullo,2019,06,21
a30cce762c2ebf1cd13bd091c57d514db7094c47,Multimodal End-to-End Autonomous Driving,Yi Xiao,2019,06,129
ab73c20a7240208fba1cea046d0979ff002846ec,Asymptotic learning curves of kernel methods: empirical data v.s. Teacher-Student paradigm,S. Spigler,2019,05,60
9ef77a58fecab23d40dbf50753883d5c3ff2bf5d,A Performance Improvement Approach for Second-Order Optimization in Large Mini-batch Training,Hiroki Naganuma,2019,,3
bb0ba74be5248c3ec684e164b4c2cb92f04c1c0b,A survey on face data augmentation for the training of deep neural networks,Xiang Wang,2019,04,97
29c53684ac64453e3d8b16454eb19473eebeb83c,Deep Learning in steganography and steganalysis from 2015 to 2018,M. Chaumont,2019,04,36
6b39bea0ae1720dcbc1ed19ffa697114c4d356c4,Scalable Deep Learning on Distributed Infrastructures,R. Mayer,2019,03,120
bbfd6ced1dea12b467e4797bfc35568e95326f07,A brain-inspired algorithm for training highly sparse neural networks,Zahra Atashgahi,2019,03,6
a1af04fa0a581a9f134a734363b1786ab2c355b7,A deep learning framework for nucleus segmentation using image style transfer,Réka Hollandi,2019,,37
26384278cf5d575fc32cb92c303fb648fa0d5217,The State of Sparsity in Deep Neural Networks,Trevor Gale,2019,02,551
037907b18f5e2ff01747df85892e93961e31af85,Training improvements for ultrasound beamforming with deep neural networks,Adam C. Luchies,2019,,17
083afaf8c8eb4d3f61a884221b340c3436c4bc13,Beyond human-level accuracy: computational challenges in deep learning,J. Hestness,2019,09,49
9b3a174f2545ac3088b56142eb99ad22a8a6f47f,Advanced analytics through FPGA based query processing and deep reinforcement learning,Gorker Alp Malazgirt,2019,,0
a0911844005dd3521170e528ed3ebaf6c290a8d0,Combining learning rate decay and weight decay with complexity gradient descent - Part I,Pierre H. Richemond,2019,02,3
9ac954bc9e59feb7ce34127db1b86fa9608c0d60,Technical Considerations for Semantic Segmentation in MRI using Convolutional Neural Networks,Arjun D Desai,2019,02,27
0cec0c296efedb814342b4b841d4583efbfc6777,Active-Routing: Compute on the Way for Near-Data Processing,Jiayi Huang,2019,,20
e94d56ccdc35c6ed5788fd35d3eff3644f0a339e,Impact of Training Dataset Size on Neural Answer Selection Models,Trond Linjordet,2019,01,25
5940fbf3fbf3f71a23a2d473b6a9ab22eaf19fa5,Machine learning in resting-state fMRI analysis,Meenakshi Khosla,2018,12,115
eefa0df7c5678fa6004f8b48dbbc1c2696702fee,An Empirical Model of Large-Batch Training,Sam McCandlish,2018,12,175
fb73ea33979fbfe07314c8650732bfba4a359848,On the Potential for Open-Endedness in Neural Networks,N. Guttenberg,2018,12,7
e7d53a4231f685cfec9979a765017e380c6f9c96,Will Deep Learning Change How Teams Execute Big Data Projects?,Ivan Shamshurin,2018,,1
9a2c16cbfdafc965d4213612b5a560383c236aae,Combining high-throughput imaging flow cytometry and deep learning for efficient species and life-cycle stage identification of phytoplankton,S. Dunker,2018,,48
14d407ee17a3a4db30442b7b2e29cc0e660c906a,Rainfall–runoff modelling using Long Short-Term Memory (LSTM) networks,Frederik Kratzert,2018,,451
b55cdcc3f3fe5927eb9022974e31862da0e99df2,Extractive Summary as Discrete Latent Variables,Aran Komatsuzaki,2018,11,3
b2c8e834ac5f7be68b9ca3691d39925036dd74a3,Measuring the Effects of Data Parallelism on Neural Network Training,Christopher J. Shallue,2018,11,314
0a53d0c508c7d20901ca48205259fdf252ca3623,Cross Entropy of Neural Language Models at Infinity—A New Bound of the Entropy Rate,Shuntaro Takahashi,2018,,10
5e4a0a47e707f9042605c716231a7de8185b353b,Super‐resolution musculoskeletal MRI using deep learning,A. Chaudhari,2018,,228
57c13015444162846b8c9326d3a8be83d3712610,Language Modeling at Scale,Md. Mostofa Ali Patwary,2018,10,2
fb29af99e4ef690bcde788442b087fbac087f533,Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis,Kelly W. Zhang,2018,09,99
cc99cea345e6905172c1661748b96744181196e4,Large Scale Language Modeling: Converging on 40GB of Text in Four Hours,Raul Puri,2018,08,26
428e7519bfd92ae6ae0a5cbbcce29742b49918ca,Buffer overflow detection for C programs is hard to learn,Yang Zhao,2018,,3
cee34fa23b6437aabbdef752aedd39173ede4ba7,Decreasing the Size of the Restricted Boltzmann Machine,Yohei Saito,2018,07,3
0b83bfd13552b57fe551b6fcf088fdd3d6055854,The Reasonable Effectiveness of Synthetic Visual Data,Adrien Gaidon,2018,,17
fd674e10770eb72e66a20e1c752c62dc7c12c0a4,Why Is My Classifier Discriminatory?,I. Chen,2018,05,297
58ab276884da2a231230d83e063484f9eac28e83,"Deep Annotated Learning, Harmonic Descriptors and Automated Diabetic Retinopathy Detection",J. Desbiens,2018,,4
fb46d79829d19e9e87fc1bed7f2f846b73f9a43c,Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation,Zhong Zhou,2018,04,12
b5933350f2f895692bad7b6bcf66f238abc5c7ba,Challenging Images For Minds and Machines,Amir Rosenfeld,2018,02,1
f17fb84d509fc39a105552b5446097e84bb22a65,Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning,Fenghao Zhu,2023,,0
aabb47c4652ea905ba529078363fa02ec1797056,Geometry-Based Deep Learning in the Natural Sciences,R. Friedman,2023,,0
bfef17a7dffac471d945a4ba101b146ccea24dd4,SCALE THROUGH SEMANTIC DEDUPLICATION,Kushal Tirumala,2023,,0
ed69756ce1717c1f658f52f92f81e0a4efce143c,R EVISITING P RUNING AT I NITIALIZATION THROUGH THE L ENS OF R AMANUJAN G RAPH,Shiwei Liu,2023,,0
7c18d0be0c22648cfbae6360774db70110371a6d,Selection Learner Learner Train for Q epochsTrain for Q epochs Selection,Jingwei Zhuo,2023,,0
134e335ceccc6def0983c75906713ea7d7b71e54,DASHA: DISTRIBUTED NONCONVEX OPTIMIZATION,A. Tyurin,2023,,0
c0614c306faa7902bf9b1d93e241eb527a3e0399,Connectivity Estimation in Financial Time Series Using Deep Learning Models,,2023,,0
33d84b1531f88d2bd2e516e1574f22e139133065,Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier,P. D'Oro,2023,,13
5234eace9af0dd7851d1431d1e5c458169ab79cd,A Robust Test for the Stationarity Assumption in Sequential Decision Making,Jitao Wang,2023,,0
c39d861fc878cb6c2161006a652b2d2c9e4250fa,A Novel Approach to Generate Dataset for Object Detection in Assembly Lines,Ramesh Kaki,2023,,0
6907fde39248fd86cb9efd0a79a1d8cc7f85b3f1,Systematic Literature Review on Cost-Efficient Deep Learning,Antti Klemetti,2023,,0
a51b253312408563714c8f1a94d240a610eae0c9,Causal Drivers of Land-Atmosphere Carbon Fluxes from Machine Learning Models and Data,M. A. Farahani,2023,,0
734524ff0861c091e86aa1f68dbe4b094a17372d,Don’t fear the unlabelled: safe deep semi-supervised learning via simple debiaising,Hugo Schmutz,2022,,2
da2315d26c2412a634b2eb3998ce7ce4f2a9369f,ImageNet as a Representative Basis for Deriving Generally Effective CNN Architectures,Lukas Tuggener,2022,,2
08a7cd0d255f8bd449bdf7feae08091067133d65,Don't fear the unlabelled: safe deep semi-supervised learning via simple debiasing,Hugo Schmutz,2022,,1
ec64e324ce1210fe5245dfd0fb5a92058732e5b9,"Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks",Yizhong Wang,2022,,90
50d28a037f68b4788b8c0a748ac4c6a5c1ef0e2b,S CALING L AWS FOR N EURAL M ACHINE T RANSLA TION A,Orhan Firat,2022,,0
6531a25a5ddeaf20fcd552e5db1dfa4be138b185,Towards Better Characterization of Paraphrases,Timothy Liu,2022,,3
c8cc524bee989b7b03d351b6de6c08fff3f9d77c,How Wide Convolutional Neural Networks Learn Hierarchical Tasks,F. Cagnetta,2022,,6
62cf47127412fa8a3ad783d23aa3fc4ab36a7d44,A Scaling Law for Syn2real Transfer: How Much Is Your Pre-training Effective?,Hiroaki Mikami,2022,,0
5905e8146099d8b69ab4f51c2f1e2a54eb65d3e9,On the Influence of Tokenizers in NLP Pipelines,,2022,,0
78c7fc1ee596e31aaf1b9d3eaf17468f1f729c4a,Estimating Running Speed From Wrist- or Waist-Worn Wearable Accelerometer Data: A Machine Learning Approach,John J. Davis,2022,,0
42bd342b227d20a395aafccca982b24fbd12c567,Learning to Limit Data Collection via Scaling Laws: Data Minimization Compliance in Practice,Divya Shanmugam,2021,,1
680f09c0640e6157d0811f387802c67dea4e35ad,Model Performance Scaling with Multiple Data Sources,Tatsunori Hashimoto,2021,,13
905c6d4708972fb535627a646b3695c8bef4fc24,"Statistical Numerical PDE : Fast Rate, Neural Scaling Law and When it’s Optimal",Yiping Lu,2021,,0
b4e111981eac28a7628958f77f0a2ba02fb6cc69,Unsupervised Data Selection for Data-Centric Semi-Supervised Learning,Xudong Wang,2021,,4
663f44c79cdad05a0afa806b26f7d4227c996921,Improving Hierarchical Product Classification using Domain-specific Language Modelling,Alexander Brinkmann,2021,,8
622d5d10b6cf0826021e50a91590a703fb2b34b7,Improved Breath Phase and Continuous Adventitious Sound Detection in Lung and Tracheal Sound Using Mixed Set Training and Domain Adaptation,Fu-Shun Hsu,2021,,1
550ae577850161c68fc3566ad21a31bba045d414,Segmenting biological specimens from photos to understand the evolution of UV plumage in passerine birds,Yichen He,2021,,1
5a5c05f7d4f4bdd374268f9365362cd66089d21c,Noise2Recon: A Semi-Supervised Framework for Joint MRI Reconstruction and Denoising,Arjun D Desai,2021,,9
6eed98e7d04e4968da9f28a117ec39f2624465fe,Representation mitosis in wide neural networks,Diego Doimo,2021,,0
1b94ebedacda0c21a4b8a40a5a40afcea4cc719a,"When Combating Hype, Proceed with Caution",Samuel R. Bowman,2021,,7
4e7e272c6728f4dea2e5cd38fde986d8d1c49329,Classification of Human Protein Atlas Image using Deep Learning CNN,S. Khan,2021,,0
996417ff1490a55ba1870928b7189d0afd6b858d,Power-law asymptotics of the generalization error for GP regression under power-law priors and targets,Hui Jin,2021,,0
84ff8703d72dac38b4065932759fc9e80ea600da,A Deep Neural Network Model for Speaker Identiﬁcation,Feng Ye,2021,,2
987a49435eb8a7e8f0623625d66169feb7dead4b,Time and the Value of Data_21-016_ec0bb93e-f6b4-4a52-8b04-ad609af7a1d8.docx,E. Valavi,2020,,1
54db220d9804f489118cd300afe8e80feb637527,Rental Demand Forecasting for Bike Sharing System using Satellite Image,Sehyeong Kim,2020,,0
393b13094c3ae127952cf3b086e18a47bdd529fc,Input Feature Mappings-Based Deep Residual Networks for Fault Diagnosis of Rolling Element Bearing With Complicated Dataset,Liangsheng Hou,2020,,17
d6fda549c3c8e0ac6f4dcfb585320afe59d8af91,Diagnosis of STEMI and Non-STEMI Heart Attack using Nature-inspired Swarm Intelligence and Deep Learning Techniques,M. Mamun,2020,,3
28f0a2defdf3fb8f7be31929691e120613c03dee,ROBUST TEMPORAL ENSEMBLING,,2020,,0
71f8b19c00498adf0a3d6c991a9b69f78bae3a55,P REDICTING THE IMPACT OF DATASET COMPOSITION ON MODEL PERFORMANCE,,2020,,1
3108603fe012276bc977bda4a85a4e333c33818a,Concept-Based Explanations on Tabular Learning,Jihye Choi,2020,,0
7a9df98de2078acf07c18ffdc356938540881260,"TrainData Model "" Madry "" Train Data Model "" George Clooney "" Poisoned Data George Clooney xN A",C. Shu,2019,,0
9405cc0d6169988371b2755e573cc28650d14dfe,Language Models are Unsupervised Multitask Learners,Alec Radford,2019,,9998
1815740dc540097cac07ed87e90c785c56f960ce,Unsupervised Pretraining for Text Classification using Siamese Transfer Learning,Maximilian Bryan,2019,,0
e1f3a5ed27dc47376279072287bfd84542403fcb,Neural code summarization : Experiments in Python and Bash,Benjamin Peloquin,2019,,0
e8fa8f02e937154570809597eed685d3d40cec43,TURING MANUSCRITAS PARA LA EVALUACIÓN ASISTIDA ( Automatic detection of hand-written Turing machines for assisted evaluation ),DE Máquinas,2019,,0
98b3cd3f28f37950ec91a96411fd817afc4e76f3,ICLR 2020 Concatenated Nodes Features GraphSAGE Sparse Representation Adjacency Matrix Node Feature : Ops Type Output Shape Input Ops Aggregator 1 Aggregator 2 Segment 1 Segment 2 Transformer,Dataflow Graphs,2019,,0
edc649f3add959a2f854888147c01d76b70ad049,Smarter Prototyping for Neural Learning,Prabhu Pradhan,2019,,1
ec350ed912fd379c42b67994d4c43755d355b151,CHAPTER 1 Deep Learning in steganography and steganalysis from 2015 to 2018,M. Chaumont,2019,,25
7cdb46dd8ba4440a8e3859a001fd38da93fbba4a,Language Modeling Teaches You More than Translation Does : Lessons Learned Through Auxiliary Task Analysis,,2018,,28
a9605503ce5f090267ef5afa7d3f2752ec2a8f36,Predicting accuracy on large datasets from smaller pilot data,Mark Johnson,2018,,28
87cf955adf778b8328695b53a48ea4d53faf4d2a,Have a Larger Cake and Eat it Faster Too : A Guideline to Train Larger Models Faster,Newsha Ardalani,2018,,1
950a6cb4c0bbb989f94e019d30eae92fb2d87db1,MULTIPHYSICS MODELING OF Ge2Sb2Te5 BASED SYNAPTIC DEVICES FOR BRAIN INSPIRED COMPUTING,Yiğit Demirağ,2018,,2
34bce689ec705f8d6bf08c8143865616ea9cbd2a,Decoding subjective emotional arousal 1 from EEG during an immersive Virtual 2 Reality experience 3,Simon M. Hofmann,,,0
1f1672655d4ba9567b549072027f38ac16108987,Electromagnetic imaging and deep learning for transition to renewable energies: a technology review,Octavio Castillo-Reyes,2023,,0
08d5d3f67cb783ebe6fcb4274116335607a4b3ba,Performance Comparison of Transformer-Based Models on Twitter Health Mention Classification,P. Khan,2023,,7
f275242ea1c3e5d4ef1bd5cb508b8fd963ce6417,Distillation Sparsity Training Algorithm for Accelerating Convolutional Neural Networks in Embedded Systems,Penghao Xiao,2023,,1
ddf298a5580effb4ff4fe507f8531b531accaa22,Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection,Sahil Suneja,2023,,0
b4b94c2088bdc76b232e467036e8f5bc83d622a8,Recurrent U-Net based dynamic paddy rice mapping in South Korea with enhanced data compatibility to support agricultural decision making,H. Jo,2023,,0
778dc188745d65b31e6d798393d3d63f51066d11,Rapid surrogate modeling of magnetotelluric in the frequency domain using physics-driven deep neural networks,Zhong Peng,2023,,0
4ef86160b00fbcd0e378f07358fae1296fee081a,System and Design Technology Co-optimization of SOT-MRAM for High-Performance AI Accelerator Memory System,Kaniz Mishty,2023,03,0
05fc44ed0630a6c7b97075a03be2601a8ba6ed19,Hera: A Heterogeneity-Aware Multi-Tenant Inference Server for Personalized Recommendations,Yujeong Choi,2023,02,0
4eb8e8860b3ff654534e36a8728a2a9f7bedad12,A strain gauge-based Bridge Weigh-In-Motion system using deep learning,Bence Szinyéri,2023,,3
f7401995af66e5d2cfadf1239f5311926173ad48,From centralized to Federated Learning: Exploring performance and end-to-end resource consumption,G. Drainakis,2023,,1
4c3723a1e7d2cf49d19050eadb6dd008a800abd8,Navigating causal deep learning,Jeroen Berrevoets,2022,12,1
1f6db328c4b4d43d8bce19cb36371e3f8a9208f0,Deep learning in airborne particulate matter sensing: a review,J. Grant-Jacob,2022,,1
33ddf7b37328c1319a62666013b2e882839f7e2a,DeepFlow: A Cross-Stack Pathfinding Framework for Distributed AI Systems,Newsha Ardalani,2022,11,2
a7bad8fb2c9f0de129fa6ffdfafcb9249542ebf0,The requirements for performing artificial-intelligence-related research and model development,Anuj Pareek,2022,,1
f4f74f5fe88e49da79a16b819a60d4d5919051fd,Cross-Subject Deep Transfer Models for Evoked Potentials in Brain-Computer Interface,Chad A. Mello,2022,01,0
ba77991d19cf8c50ae2d2efcc9b5fb141acaa7b4,Requirements Engineering for Machine Learning: A Review and Reflection,Zhong Pei,2022,10,10
0286b2736a114198b25fb5553c671c33aed5d477,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,Yuntao Bai,2022,04,348
1007116426f1007e3b76fd7fc3836e7a25be2757,SpectraGAN: spectrum based generation of city scale spatiotemporal mobile network traffic data,Kai Xu,2021,,7
5cec12ee5084cb553b1cd1a28ea5eb10ff775d49,Towards Reliable AI for Source Code Understanding,Sahil Suneja,2021,,6
bf59b890cd5b009ff05ea614f67897eb725e0005,Communication Algorithm-Architecture Co-Design for Distributed Deep Learning,Jiayi Huang,2021,,9
95aff3843524a65bdf70db9034d244f4e60fa62e,GradeML: Towards Holistic Performance Analysis for Machine Learning Workflows,T. Hegeman,2021,,2
90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb,Accelerating Sparse Deep Neural Networks,Asit K. Mishra,2021,04,98
8ae8c5b8f8c1d77e00029606670572a16ac450c1,Residual Neural Network precisely quantifies dysarthria severity-level based on short-duration speech segments,Siddhant Gupta,2021,,21
eab938ef4dc9dc764afacd4241db2ca934c4773a,Deadline-Aware Offloading for High-Throughput Accelerators,T. Yeh,2021,,8
ce10bcf613c30c2184fd53d9ed2149ecb9cddfee,"Centaur: A Chiplet-based, Hybrid Sparse-Dense Accelerator for Personalized Recommendations",Ranggi Hwang,2020,05,74
2c5a8950cf0a13e229ad19093ba064495fda8de7,A Neural Scaling Law from the Dimension of the Data Manifold,Utkarsh Sharma,2020,04,33
89f5236f12a9a5f16ab912be4b53682968cff537,AutoTM: Automatic Tensor Movement in Heterogeneous Memory Systems using Integer Linear Programming,Mark Hildebrand,2020,,46
34fac2bfff3052a1b961da4ed592658bd9a35f00,Distributed Mean Estimation with Optimal Error Bounds,Dan Alistarh,2020,02,2
022aabd29585523ee64063eeaeff8a20767c7c38,Distributed Variance Reduction with Optimal Communication,Peter Davies,2020,,11
0fdc1a05a67631d61dd4e6cf1b513c02403bfcb5,NeuMMU: Architectural Support for Efficient Address Translations in Neural Processing Units,Bongjoon Hyun,2019,11,18
3001695fdcbbf36924fad3d7cff17a5667cc3749,TensorDIMM,Youngeun Kwon,2019,,4
976945fb2879961e5faee43472d3df0d8510ada9,PREMA: A Predictive Multi-Task Scheduling Algorithm For Preemptible Neural Processing Units,Yujeong Choi,2019,09,84
eb34a6505ddb091aea916d0d64d3d1f4a5bc8558,A Disaggregated Memory System for Deep Learning,Youngeun Kwon,2019,,21
989fa897fdb002a9a7cc4175f3b011ef5f9480cb,TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning,Youngeun Kwon,2019,08,124
e1e5069a00b8a393b718589c704b4ed7e9134dc6,Taxonomy of Saliency Metrics for Channel Pruning,Kaveena Persand,2019,06,4
ddf87176e2fdb94a718bf71e277c29ece4e9ec3c,Reaching Beyond Human Accuracy With AI Datacenters,G. Diamos,2018,,0
b17c2f0c2dc3f98e85322889dc144e79d183fcff,Scaling Laws from the Data Manifold Dimension,Utkarsh Sharma,2022,,12
19db6b65e37509ca61874100888b28b3991c710b,EECS 442 Project Report: Monocular Image Depth Prediction via Encoder-Decoder-based Deep Learning,Huijie Zhang,2021,,0
eec7ad0270eda7298c139af6e2676599f1fd53f6,Data and Parameter Scaling Laws for Neural Machine Translation,Mitchell A. Gordon,2021,,2
2374117bf95a23d0aa052151a6d9642aca6b2183,PIMCaffe: Functional Evaluation of a Machine Learning Framework for In-Memory Neural Processing Unit,Won Jeon,2021,,3
0137de08b80ed7b5bfccd7d282c87ca4fc77f29e,Data and Parameter Scaling Laws for Neural Machine Translation,Prafulla Dhariwal,2021,,42
6cd6118da9bbaa50c673118ded401ad506bb0f7c,LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework,Zihan Zhao,2023,08,0
df2666bde176803c05c2085689b0b3ea3aa058fd,Reduce the Handicap: Performance Estimation for AI Systems Safety Certification,Julius Pfrommer,2023,,0
225a242405d1629b18b7c4367a3101509c9274bb,Scaling Laws Do Not Scale,Fernando Diaz,2023,07,0
1205d6406577389f583247b2eae4abae6215656d,Delegated Classification,Eden Saig,2023,06,1
29c7f009df21d0112c48dec254ff80cc45fac3af,Are Emergent Abilities of Large Language Models a Mirage?,Rylan Schaeffer,2023,04,57
a7a40b35b6f37c554f1c5c2038892ed70c693a64,Learning to Grow Pretrained Models for Efficient Transformer Training,Peihao Wang,2023,03,9
7d0221f3d8282a84375790768aafafb63a69cbf5,Data pruning and neural scaling laws: fundamental limitations of score-based algorithms,Fadhel Ayed,2023,02,2
5445e246bb2ea0d71026f299604afe4cffa39a0d,Evaluating Self-Supervised Learning via Risk Decomposition,Yann Dubois,2023,02,1
4129fb99f5c8785452d60ddaf29d178525e29d0a,Measures of Information Reflect Memorization Patterns,Rachit Bansal,2022,10,2
6bd90b6343367806abab262256f1b2eeccdeebf3,Data Budgeting for Machine Learning,Xin-Bo Zhao,2022,10,1
fbad88bb09b45ba78d3b3df644c639f21760b20a,Impact of dataset size and long-term ECoG-based BCI usage on deep learning decoders performance,Maciej Śliwowski,2022,09,0
5f709f6279b9eb0edfee203413d5b30b5f65c818,Limitations of the NTK for Understanding Generalization in Deep Learning,Nikhil Vyas,2022,06,20
497d5e7861e5e89ab599f8936ccdae10162776d8,"Evaluating the Diversity, Equity, and Inclusion of NLP Technology: A Case Study for Indian Languages",Simran Khanuja,2022,05,0
8b293973061026d9d0eed90e71e30928e029171e,Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models,Kushal Tirumala,2022,05,63
0b0d7d87c58d41b92d907347b778032be5966f60,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,Greg Yang,2022,03,76
04ff95e0edc3759fc5d18a1b929b3ccf79b032b2,Deconstructing Distributions: A Pointwise Framework of Learning,Gal Kaplun,2022,02,11
e404bdfaa858b3c25540aa5d2c5dfe20c16ead37,Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments,Maor Ivgi,2022,02,6
37ddb9305c8c9120c21a2fae5a851ce8e4384a9c,Data Scaling Laws in NMT: The Effect of Noise and Architecture,Yamini Bansal,2022,02,21
c2536182c010c41941e8a031071a1880c34cec60,Unified Scaling Laws for Routed Language Models,Aidan Clark,2022,02,64
ff3e25bfe48d795b4c563b5f64359a1343a29661,Auto-Compressing Subset Pruning for Semantic Image Segmentation,Konstantin Ditschuneit,2022,01,1
3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e,A General Language Assistant as a Laboratory for Alignment,Amanda Askell,2021,12,177
2a9541011d490977204eb11217a952ae15be7193,Benchmarking down-scaled (not so large) pre-trained language models,M. Aßenmacher,2021,05,1
8512718bafa447f9b433da9e809215dfc28b6b28,Towards More Fine-grained and Reliable NLP Performance Prediction,Zihuiwen Ye,2021,02,24
4383a975c09b72ba2f1a77cd779bb6965dbfb2fb,Scaling Laws for Transfer,Danny Hernandez,2021,02,110
4454a763c891afb3fb8fa6567a367d05b1938e97,Meta-learning with negative learning rates,A. Bernacchia,2021,02,14
3efbcfeeb0ea1051a71101d3318da4411081f0b8,Scaling Laws for Autoregressive Generative Modeling,T. Henighan,2020,10,187
95cb2118e74e68727d2b9429606ff95afe09a7bd,Domain Divergences: A Survey and Empirical Analysis,Abhinav Ramesh Kashyap,2020,10,25
21cc6e612da081b4d28fab75d09b2e0e6e2cf5f2,Automatic Feasibility Study via Data Quality Analysis for ML: A Case-Study on Label Noise,Cédric Renggli,2020,10,1
647a0e74203cc134c500588616ba60f9717de782,The Deep Bootstrap: Good Online Learners are Good Offline Generalizers,Preetum Nakkiran,2020,10,11
e0b8b6bf45f845702a684e078eefcc440cad6c83,On Power Laws in Deep Ensembles,E. Lobacheva,2020,07,32
3836ccb33191799e748e8e96f85a813eaf650ff8,Data Movement Is All You Need: A Case Study on Optimizing Transformers,A. Ivanov,2020,07,71
deedb9b61a01d686b28e6034770fccc142e77fab,Predicting Performance for Natural Language Processing Tasks,M. Xia,2020,05,41
5290d7921f0266c8b50b79fc8a0b7d22868f4f60,The Cost of Training NLP Models: A Concise Overview,Or Sharir,2020,04,141
850464c9006261bd632c4203f3e630db09a32faf,Comparing Rewinding and Fine-tuning in Neural Network Pruning,Alex Renda,2020,03,283
0495d9df8eb84dcdab4e5536179823cd26279949,Big Transfer (BiT): General Visual Representation Learning,Alexander Kolesnikov,2019,12,890
a88d957759d2b1fd4f161e45356f18bfb53d6897,Deep Power Laws for Hyperparameter Optimization,Arlind Kadra,2023,,0
36a329ecbd38c84b9706ccce933f60533338a530,Data Efficient Neural Scaling Law via Model Reusing,Peihao Wang,2023,,1
3881801d73a223cac1fd317a5c47df26be87af57,Empirical Limitations of the NTK for Understanding Scaling Laws in Deep Learning,Nikhil Vyas,2023,,1
2d2b68c201986f47d39911a926d1bfd98f8439f5,Ease.ML/Snoopy: Towards Automatic Feasibility Studies for ML viaQuantitative Understanding of “DataQuality for ML”,Cédric Renggli,2022,,1
8db7f1f679edfc4826eb137421b7ff5d3d0852f2,"Evaluating Inclusivity, Equity, and Accessibility of NLP Technology: A Case Study for Indian Languages",Simran Khanuja,2022,,7
76e2b3b6e1da49764d342ea922290410162125ca,Measures of Information Reﬂect Memorization Patterns,Rachit Bansal,2022,,0
8e839692c9b4edcbc64780cb94c2cda2584085b1,A Scaling Law for Synthetic-to-Real Transfer: A Measure of Pre-Training,Hiroaki Mikami,2021,,1
990d042905b8a41f601d3fdd975bc4051a852bcf,Data Movement Is All You Need: A Case Study of Transformer Networks,A. Ivanov,2020,,4
32a6870be73105faa2faab41b550f087639ea281,Survival Loss: A Neuron Death Regularizer,Emilio J. Almazán,2020,,0
c96297261467b5daa2d01227496a70d444602434,Baichuan 2: Open Large-scale Language Models,Ai Ming Yang,2023,09,8
f8afa4bd5b05f52ffa304f56aed7a5792a42ef1f,FLM-101B: An Open LLM and How to Train It with $100K Budget,Xiang Li,2023,09,2
d62daf809266e02a3e3be4bec160579ff3839cc9,An Investigation on Hardware-Aware Vision Transformer Scaling,Chaojian Li,2023,,0
3ac3c10e1317fe8419f794cf30ce3227e95e1f54,Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes,Yosuke Miyanishi,2023,08,0
8ab27e590fccc2a2a1fcd6b820ff3e2d3d36e36e,Effective Enforceability of EU Competition Law Under AI Development Scenarios: a Framework for Anticipatory Governance,Shin-Shin Hua,2023,,0
91206346edbe28abb606d7b3425cd455d4019d4f,Scaling Relationship on Learning Mathematical Reasoning with Large Language Models,Zheng Yuan,2023,08,15
6ba360aee802b18d8ecc303e8855b832b1c29dc7,Applicability of scaling laws to vision encoding models,Takuya Matsuyama,2023,08,0
fee492fe200fd8dbb0f9d762dfbbe188fd200599,Mining of Single-Class by Active Learning for Semantic Segmentation,Hugues Lambert,2023,07,0
951284f2c6749024fe5ca06aade16888f7c8ecce,FedYolo: Augmenting Federated Learning with Pretrained Transformers,Xuechen Zhang,2023,07,1
c5ef32afd5dcdf396fa61ce6a92e5233c1aebfd1,Absorbing Phase Transitions in Artificial Deep Neural Networks,Keiichi Tamai,2023,07,0
5f4821dbf93e81464282bc7981a85499c73114ee,DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data,Jin Zhu,2023,06,0
ecc2dc870345f22bd3d4d8b77b5e24b238cb975e,Empowering Business Transformation: The Positive Impact and Ethical Considerations of Generative AI in Software Product Management - A Systematic Literature Review,Nishant A. Parikh,2023,06,0
c8c6408437ac4bce0e00192ff0c339206350f598,Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior,Shashank Subramanian,2023,06,2
7d1e59ce254bea5228da634dbe7c5c4160df6f98,Transfer learning enables predictions in network biology,Christina V. Theodoris,2023,,26
4d7c4c371d9742d3f2d9c338bfb898fc856e7349,Faith and Fate: Limits of Transformers on Compositionality,Nouha Dziri,2023,05,21
0fbf7ea1a3bd1754ed9aa12ed25906b731ece589,Training Data Extraction From Pre-trained Language Models: A Survey,Shotaro Ishihara,2023,05,2
024d966e6f75e3672d4200e51bc99af6b141b70e,Scaling Data-Constrained Language Models,Niklas Muennighoff,2023,05,18
672491163a327f80e08ce3ef4751e94c78631822,"Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",Haw-Shiuan Chang,2023,05,3
36cb6bc0adbbceacaf336ea44bdbbe53c3e5f124,Few-shot 3D Shape Generation,Jin Zhu,2023,05,1
785bb49af762efd64d841e52aa82c708341a7c43,Code Execution with Pre-trained Language Models,Chenxiao Liu,2023,05,4
ebb93e2b3d4fdbd5689cca8ddc01da016738b57e,Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism,Xin Chen,2023,04,1
deb8f26509ae320fc975b32922416cb156c61bbd,Emergent and Predictable Memorization in Large Language Models,Stella Rose Biderman,2023,04,14
69b86c9379a5990f6e9faefc6c537aad62c30993,Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales,Yiqun Yao,2023,04,2
f144738de47f109d7016cc817831387ed4a256df,Ambiguous Medical Image Segmentation Using Diffusion Models,Aimon Rahman,2023,04,12
8f4e198467de15fdbb305d0982ff6f15565ab601,To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency,Daniel Fernando Campos,2023,04,1
be55e8ec4213868db08f2c3168ae666001bea4b8,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,Stella Rose Biderman,2023,04,137
40eec924e30d235df94dab154541b888095f63b1,Machine learning and Bayesian inference in nuclear fusion research: an overview,A. Pavone,2023,,4
8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48,GPT-4 Technical Report,OpenAI,2023,03,1284
cd854d3d7a1231b1dfbbfa09a697ac064026be51,Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models,Alberto Testolin,2023,03,2
7a3f4935d55b8c2cc7fb44d502f128886ccb75c2,Architext: Language-Driven Generative Architecture Design,Theodoros Galanos,2023,03,3
105113d20fcb237bfb4b2c1f66816c31a7786416,Synthetic ECG Signal Generation Using Probabilistic Diffusion Models,Edmond Adib,2023,03,5
2ae807688d5bae0a7331992793be066b93d7655f,Does Deep Learning Learn to Abstract? A Systematic Probing Framework,Shengnan An,2023,02,1
9d877509f3fb8fbbf3e3d54eeef3c84bc0e1e3b2,A Simplistic Model of Neural Scaling Laws: Multiperiodic Santa Fe Processes,L. Debowski,2023,02,3
f7140c45b6dd9574e9ef16633b4c4dfa7ba71940,Scaling laws for single-agent reinforcement learning,Jacob Hilton,2023,01,6
5278b81db686b4d36143941bff1c683bea963a63,SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient,Max Ryabinin,2023,01,8
16764e8ed60b424859893e2e8f6cc251ea75a6e7,Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning,Hyunsoo Cho,2023,01,2
cd44fd81820fc36c6ebac4e6a9a9d848dd854d2b,Safety of self-assembled neuromorphic hardware,Can Rager,2023,01,0
874deb5f06f35e52ae13a921b23611eec4abd1da,ClimaX: A foundation model for weather and climate,Tung Nguyen,2023,01,44
bad6fa523ecf782c837a2eecaaffa4e1f7477c24,Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction,Jonathan Pilault,2023,01,7
06edda0310b4ec7c5012d012349252a3a77521b6,Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners,Hyunsoo Cho,2022,12,8
cef330bacf014d60daabbd489647b2006af130ca,Discovering Language Model Behaviors with Model-Written Evaluations,Ethan Perez,2022,12,76
736973165f98105fec3729b7db414ae4d80fcbeb,Scalable Diffusion Models with Transformers,William S. Peebles,2022,12,100
89d1997bb400534ddae24dfc8456378f108e18a7,Data management in training AI chatbot with personality based on granular computing,Piotr Podolski,2022,,1
40dbba2fee6a9d801d085db17afcf15c4195d466,Dendrocentric learning for synthetic intelligence,K. Boahen,2022,,14
965e409a3e7b5670d609837fac9823b160d6639c,Logical Tasks for Measuring Extrapolation and Rule Comprehension,Ippei Fujisawa,2022,11,4
1f036b092a74f0c2f7eef37daa16eeb0f5954d9b,Development of a Neural Network-Based Mathematical Operation Protocol for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS),Victor Robila,2022,11,0
2c525c0a0e058b0f0d0a351c1fd43fd92929433a,Few-shot Image Generation with Diffusion Models,Jin Zhu,2022,11,9
734f927349acbd23feac82d32772010ec8a7621a,A Declarative Systematic Approach to Machine Learning,G. Babu,2022,,0
bb15f3727f827a3cb88b5d3ca48415c09b40a88f,What Language Model to Train if You Have One Million GPU Hours?,Teven Le Scao,2022,10,44
ed38c6b157c11476939c426ec6871c926f2f3524,Leveraging Large Language Models for Multiple Choice Question Answering,Joshua Robinson,2022,10,31
fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b,Scaling Laws for Reward Model Overoptimization,Leo Gao,2022,10,58
cbd6478c6aaec64481c6e3463725d8269b268748,The Chamber Ensemble Generator: Limitless High-Quality MIR Data via Generative Modeling,Yusong Wu,2022,09,4
fe828d8dc632780c10b49ed67cc74a526076b966,Ising models of deep neural networks,D. Stosic,2022,09,0
4be7d1524edb0137599a5cc95f72844b85a52fe1,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,Tim Dettmers,2022,08,161
6edccbd83a9aae204785d4821f97855677c33866,Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,Yi Tay,2022,07,41
5275611d030f4280edecec60834a2a13abdb03da,Improving the Accuracy of Progress Indication for Constructing Deep Learning Models,Q. Dong,2022,,1
b294dc2703d25caa8988ecfd96b1d02bfff82f7e,Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning Rates,Jacob P. Portes,2022,06,2
2fd1f995210eabaee50f21d684b1c341d40448e3,Active Learning Helps Pretrained Models Learn the Intended Task,Alex Tamkin,2022,04,19
e37018d3cfab9cfc29a7b78404e6c86ea18a907e,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,Sid Black,2022,04,304
5288b9f3a9f575543f44c39e1d3b78b3ca4c99da,InCoder: A Generative Model for Code Infilling and Synthesis,Daniel Fried,2022,04,213
1b261d347181af289e950354db6e0095e84e28b2,Vision Transformer Compression with Structured Pruning and Low Rank Approximation,Ankur Kumar,2022,03,2
8fbc2d349d3d0945efa5e92fd3713734ce63d19e,Autoregressive Image Generation using Residual Quantization,Doyup Lee,2022,03,70
f70f9f9c0187fc830081124f984560057891dad7,Nonlinear Initialization Methods for Low-Rank Neural Networks,Kiran Vodrahalli,2022,02,1
0d564d688e4e25bf640adf46387f0baf31beefbb,Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification,Takashi Ishida,2022,02,2
1415479215dcfec5e2ee0d33f1e1565ae2c65bb9,Examining Scaling and Transfer of Language Model Architectures for Machine Translation,Biao Zhang,2022,02,8
f354ceedbd0e8ca94e9ff32845805cc92d5475d8,Safe Deep RL in 3D Environments using Human Feedback,Matthew Rahtz,2022,01,4
fd3cd7a6089d6715605bf219875108864fc12943,Bringing Atomistic Deep Learning to Prime Time,Nathan C Frey,2021,12,0
1aeb16ad1586e0a0fe94fd60e8ae530d21075364,Scalable Geometric Deep Learning on Molecular Graphs,Nathan C Frey,2021,12,4
0290d7d28892c2f04404985e37cda69322e6a74b,Investigation of Training Label Error Impact on RNN-T,I-Fan Chen,2021,12,4
3ea60cbce6c9065661d207fccf021c5d58a83f01,Scaling Up Vision-Language Pretraining for Image Captioning,Xiaowei Hu,2021,11,132
02bba282dfbcee7fb549d87ce6d23ce9ac784f25,DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning,Alex Tamkin,2021,11,0
c109bf9c97536b6ba3caac73cf94d195f16336c0,DP-REC: Private & Communication-Efficient Federated Learning,Aleksei Triastcyn,2021,11,8
b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df,LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs,Christoph Schuhmann,2021,11,501
92bc69500c16fce47bcfd06ada14ffc4a7e8ddca,PAGnol: An Extra-Large French Generative Model,Julien Launay,2021,10,3
55d1133ec3bd8851dc0172cf7454063872a11898,Unsupervised Neural Machine Translation with Generative Language Models Only,Jesse Michael Han,2021,10,19
11fe37ab6faf6bf85ad2f5746c154dec5412bd04,8-bit Optimizers via Block-wise Quantization,Tim Dettmers,2021,10,67
aead4418733b998792deb9cbf198a834449e00d2,Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics,S. Welleck,2021,09,13
2d4f66046bb436864cd6bf589e3a931c405f9f44,Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers,Yi Tay,2021,09,70
0f2199296f01694ee46b6059879260fb80a84fa6,Teaching Autoregressive Language Models Complex Tasks By Demonstration,Gabriel Recchia,2021,09,17
9289826beb6206eeaf500105f7329d6d5a495d8a,Robust fine-tuning of zero-shot models,Mitchell Wortsman,2021,09,262
88afeaf0a4208477e845170daa8a189cc0a13a73,On the Multilingual Capabilities of Very Large-Scale English Language Models,Jordi Armengol-Estap'e,2021,08,7
35b985097514cca866c5dc85b644724f67da4139,Declarative machine learning systems,Piero Molino,2021,07,14
15dc19495e95703f96989bd66135eb3bc4057976,Compact and Optimal Deep Learning with Recurrent Parameter Generators,Jiayun Wang,2021,07,4
2132eac5628bc200de226b51f1dfb82423ff1d24,MarIA: Spanish Language Models,Asier Gutiérrez-Fandiño,2021,07,56
d0dae92c4d37520ae20c072ec64fdb718874bfd0,A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis,Joseph Palermo,2021,07,2
0cfce36622a017a86c2248ebe5dfdd0f8d643df7,Language models enable zero-shot prediction of the effects of mutations on protein function,J. Meier,2021,,194
8afdd8b9dbdd1073b2a177688768ae8ac5060498,MassGenie: A Transformer-Based Deep Learning Method for Identifying Small Molecules from Their Mass Spectra,A. Shrivastava,2021,,28
43e35f6a5930caf964b891c7f57f93f111fbe251,Transflower,Guillermo Valle Pérez,2021,06,35
de8f92c8a7ebde8bffb968a536f79e5fb7cd225e,Scaling Laws for Acoustic Models,J. Droppo,2021,06,13
2a805d0e1b067444a554c5169d189fa1f649f411,Scaling Vision Transformers,Xiaohua Zhai,2021,06,565
a2a97c944a5a987435cbe8249693b63a3a2c2745,"Effect of pre-training scale on intra- and inter-domain, full and few-shot transfer learning for natural and X-Ray chest images",Mehdi Cherti,2021,06,3
74e24fa1628780d269ba7bff7237f7ac1fccea7e,A generative nonparametric Bayesian model for whole genomes,Alan N. Amin,2021,,6
e3b8ed7c6d98ddb5ef7a41ce80dbbfbf60e8e5a6,High-Resolution Complex Scene Synthesis with Transformers,Manuel Jahn,2021,05,18
6a8cb4fb5a20c7e5733a9bd50cd5feaad6c11360,Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level,Ruiqi Zhong,2021,05,26
d13a0c8d49cb268d8d245925baee0316c1fe1875,Which transformer architecture fits my data? A vocabulary bottleneck in self-attention,Noam Wies,2021,05,15
39692df948172a319ca33434f27c213d99d941a2,AI Risk Skepticism,Roman V Yampolskiy,2021,05,4
1adadbfa95e43a70fcd17e6ce947a0652b86bfc3,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,Jesse Dodge,2021,04,159
5e6a6383dc6de74c71134e6460a13434d9968900,TENT: Efficient Quantization of Neural Networks on the tiny Edge with Tapered FixEd PoiNT,H. F. Langroudi,2021,04,5
57d1e7ac339e783898f2c3b1af55737cbeee9fc5,Measuring Mathematical Problem Solving With the MATH Dataset,Dan Hendrycks,2021,03,216
79a79a4c1a43038973c49b0cbc6085e2c6caeeb1,Generating Images with Sparse Representations,C. Nash,2021,03,83
268fedc5d786fa197b294dccab7eea02dc08038a,"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",Qing Li,2021,03,2
2cc3ab9fa41ba2804e301f7eae9598636e62422a,Investigating the Limitations of the Transformers with Simple Arithmetic Tasks,Rodrigo Nogueira,2021,02,53
de18baa4964804cf471d85a5a090498242d2e79f,Improved Denoising Diffusion Probabilistic Models,Alex Nichol,2021,02,1167
065112180cd381ffc018780cf8fc0a14ae2580b1,Proof Artifact Co-training for Theorem Proving with Language Models,Jesse Michael Han,2021,02,44
3234d2810eb2d7cfd7ef2d3646de4581a78e42a8,Embodied intelligence via learning and evolution,Agrim Gupta,2021,02,85
9faecf3e18a833f2d49b030d591cc2ded0b54336,Towards Continual Reinforcement Learning: A Review and Perspectives,Khimya Khetarpal,2020,12,159
f1d839078a29ebe2dfaee9f57d1d92f91eac99c5,Universal Policies for Software-Defined MDPs,Daniel Selsam,2020,12,4
09dfae5d1d56d0405416945cba572943f611050e,"Measuring the Occupational Impact of AI: Tasks, Cognitive Abilities and AI Benchmarks",Songül Tolan,2020,,32
a8d6467e4126d0ada4e824b30a6098056b843242,Generative Models of Images and Neural Networks,Bill Peebles,2023,,0
c5cf54b2b6abf658697d272c1377812fd9e24e11,Scaling Generative Pre-training for User Ad Activity Sequences,Sharad Chitlangia,2023,,0
1b04ac51ffeed0d31fbdbab0e4ed65ffe8c35df6,AI Safety: Model Trojaning and Benchmarking,Akul Arora,2023,,0
394d6dc274e28660333ff3c1c4ab3dd2883d87aa,Algorithms for Efficiently Learning Low-Rank Neural Networks,Kiran Vodrahalli,2022,,2
5ea63594ba2627dbd60ca062358e58178065e007,"A Holistic Assessment of the Carbon Footprint of Noor, a Very Large Arabic Language Model",Imad Lakim,2022,,9
dd0b3fef20fd9384e055648ed68ac4f8d793bd99,Generating Detailed Music Datasets with Neural Audio Synthesis,Yusong Wu,2022,,1
61fe748e1f9f901f947a99f13e4fa5707eadcd3f,SciDeBERTa: Learning DeBERTa for Science and Technology Documents and Fine-tuning Information Extraction Tasks,Yuna Jeong,2022,,2
bf93fe733932fd25780ef84911b8a507bec1c372,Neural Scaling of Deep Chemical Models,Nathan C Frey,2022,,21
f1362000a1561924a3a07d7b9ab3d8cc3fd4e96d,Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images,Mehdi Cherti,2021,,7
3866a179667886e383a73d14808f9960ba05e022,Detecting Backdoored Neural Networks with Structured Adversarial Attacks,Charles Yang,2021,,1
00344e54337e5db155ad42b18191294e773cc01a,Spanish Language Models,Asier Gutiérrez-Fandiño,2021,,27
8424082e3bf4792462eb112d7ebcecf5b0dc3613,"Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning",Chadi Helwe,2021,,29
096cffc20d3bc89e8bc337607435a67e79d888d9,Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images,Mehdi Cherti,2021,,10
9657f9fe0ff4ed31692646d2263f76ab17c8c740,ING WITH LANGUAGE MODELS,Jesse Michael Han,2021,,0
b2faac4ff48056f880bf836c1cc1f8a27ec0a73a,Towards the Automatic Mathematician,Markus N. Rabe,2021,,9
7d52161336907a873c71c015550d43240565250f,BOX LCD: A S IMPLE T ESTBED FOR L EARNED S IMULATOR R ESEARCH,M. Wilson,2021,,0
9a5711594a565dc489f2b882a8bb5c3152b2d087,Snowcat: Efficient Kernel Concurrency Testing using a Learned Coverage Predictor,Sishuai Gong,2023,,0
2c8dfb7d2edf748d90dfa28cbb0613367455eb22,Large Language Models for Software Engineering: Survey and Open Problems,Angela Fan,2023,10,0
f73cd1c9eba950c04fbd81e1f024392978059d59,Scaling Laws for Associative Memories,Vivien Cabannes,2023,10,0
8923aec569a13f94148e3e90a94c68730f6ad03d,Searching for High-Value Molecules Using Reinforcement Learning and Transformers,Raj Ghugare,2023,10,0
74e0f3400b6ef05a5eb7bdac1b6d0758cbf5b1f1,NOLA: Networks as Linear Combination of Low Rank Random Basis,Soroush Abbasi Koohpayegani,2023,10,0
b6e773e8a69ae6c13c5fab08a6d97b554318ea19,Multiple Physics Pretraining for Physical Surrogate Models,Michael McCabe,2023,10,0
635cfc277ef634c37db5c6b15793c5b303b5a9ff,Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication,Zhe Zhao,2023,10,0
95f2bdbc0fa4f93fa2917e9a313b29183b6e65de,Video Transformers under Occlusion: How Physics and Background Attributes Impact Large Models for Robotic Manipulation,Shutong Jin,2023,10,0
a2bf0f9d41c550bc7cf69706b322f9c9838d3b00,Large Language Models for Test-Free Fault Localization,Aidan Z.H. Yang,2023,10,0
c35c41dc9c93eff38ec8913cb4d639b06a915d33,Linear attention is (maybe) all you need (to understand transformer optimization),Kwangjun Ahn,2023,10,0
4e3254d90560d19b42419e9b4c9367cf3674dcad,Coordinated pausing: An evaluation-based coordination scheme for frontier AI developers,Jide Alaga,2023,10,0
9486179c5ab6e0cff0ef919c34080a882bb9fe9f,AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ,Jonas Belouadi,2023,10,0
bfeda6c7aa7899a80adb01894555b09d24756a59,Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,Qiushi Sun,2023,10,0
64bbc6f10a701490d2acff4bc4e6782c563fbb12,On the Stability of Iterative Retraining of Generative Models on their own Data,Quentin Bertrand,2023,10,0
98478ac589e5b40a20630ff54bb4eec4ab4c5f6b,GAIA-1: A Generative World Model for Autonomous Driving,Anthony Hu,2023,09,0
f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55,Data Filtering Networks,Alex Fang,2023,09,0
12db3efff4cc9e16822dd64bb1cad66f3f034f3b,L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models,Ansong Ni,2023,09,0
02bc90e9fb4a681b048c6652720afe439d16e6cd,Scaling Experiments in Self-Supervised Cross-Table Representation Learning,Maximilian Schambach,2023,09,0
9f9cdced51568c623ec447bf0ea9709b383b5a0f,Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks,Hao Chen,2023,09,0
dbfd2b3178f1b7ce45db22531fc8ae5dae450894,LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud,Mengke Zhang,2023,09,0
039b24eb29bfd65145bf66549735bbd57bb4d4f0,Neural scaling laws for phenotypic drug discovery,Drew A. Linsley,2023,09,0
9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b,Transformer-VQ: Linear-Time Transformers via Vector Quantization,Lucas D. Lingle,2023,09,0
8d727ce66eeef021462c14ab7afbbc1110495b01,A Benchmark for Learning to Translate a New Language from One Grammar Book,Garrett Tanzer,2023,09,0
ad91394aaa1dad451e1ea52acb73b525c9574642,Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit,Blake Bordelon,2023,09,0
ac35e13321b3fd108b1a427964872514ea3c3eb7,The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges,Emanuele La Malfa,2023,09,0
9099ee08e59cc33ed1c88d4708cf5c931bf46dc4,LawBench: Benchmarking Legal Knowledge of Large Language Models,Zhiwei Fei,2023,09,0
048c43c9e373c19c27395c9e8a51370e52635543,Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories,Benjamin Hoover,2023,09,0
4d88bcb406e1eb604bdba1d659f2d8493d6a9a85,ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens,Yangyang Guo,2023,09,0
d7afdc5b3d11471cb8b6d6a6e4ddd708126cf4c3,FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets,Fatemeh Tavakoli,2023,09,0
e157ab212c441f61686d955b75ed697eca69f367,Effective Long-Context Scaling of Foundation Models,Wenhan Xiong,2023,09,1
dd06b331be3e72f5e7b74275aa4065b76b4df5dc,Supersonic: Learning to Generate Source Code Optimizations in C/C++,Zimin Chen,2023,09,1
73bd1ee5193d1f9fa5913e15fcf111db9f75bc75,Large Language Model Alignment: A Survey,Tianhao Shen,2023,09,0
fc709c4e746f5fef146c19a8ac7873db26a57d74,General purpose large language models match human performance on gastroenterology board exam self-assessments.,Shuhaib Ali,2023,,0
56b5b15ab8f6b8aefa5ff497f07ec083a83638d5,Small-scale proxies for large-scale Transformer training instabilities,Mitchell Wortsman,2023,09,0
03ec8d1d8f91dd727371512f9079cad31e202f66,(Predictable) Performance Bias in Unsupervised Anomaly Detection,Felix Meissen,2023,09,0
32479758dc9ff9820828a12aa7f3d066f187dc1c,LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models,Ahmad Faiz,2023,09,0
a808907ca9ab19f1ed10fe81f399d666fbfad50d,Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces,Hariharan Subramonyam,2023,09,0
3f1b5af721e7085ccedc4a27e1272d412e396ee8,Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity,Joel Ye,2023,,0
aaf866c645cd9bcb35ae8567ce0eef77d466abf8,"De novo peptide sequencing with InstaNovo: Accurate, database-free peptide identification for large scale proteomics experiments",Kevin Eloff,2023,,0
91158ac126e1394ac056d98b87bd6994d503f0ef,Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models,Valentin Leonhard Buchner,2023,09,0
7b689adb8c156d6158660f90d1c86888ee281f63,DreamLLM: Synergistic Multimodal Comprehension and Creation,Runpei Dong,2023,09,1
77b046c5d568b329a927cfc895ea2e6c8f43ff43,The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute,Aleksandar Stanic,2023,09,0
c9896820b341e339d07e830f42898ab79b48c997,Explosive growth from AI automation: A review of the arguments,Ege Erdil,2023,09,0
c17c3c31e104e04a2f33f87d1a38909082df81c9,Prompt engineering of GPT-4 for chemical research: what can/cannot be done?,Kan Hatakeyama-Sato,2023,,0
bc9f29881c1d93d225f0a74fa700531202c7043a,OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch,Juntao Li,2023,09,0
437a386f3fe4b8c7449a37e2364412a26e0a478c,PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models,Chenhao Tang,2023,09,0
21091f8133ab034baacb92fdb958e14989eb427f,Language Modeling Is Compression,Gr'egoire Del'etang,2023,09,1
ac70fb2c74aa87420878c441c3d24969947e0294,Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training,Eghbal A. Hosseini,2023,,0
292097a93d1cd97c317bbe0bd663d3c9669042d5,Initial policy considerations for generative artificial intelligence,Philippe Lorenz,2023,,0
e716e6e0b3dd5124268780dc9bed521a07f371b8,Contrastive Decoding Improves Reasoning in Large Language Models,Sean O'Brien,2023,09,1
62b4e06f5249d22e4a153ec4a2dc934c6a014372,OWL: A Large Language Model for IT Operations,Hongcheng Guo,2023,09,0
5038d82c10fc6784d96fca8d3dc5f97c4e479efd,BROW: Better featuRes fOr Whole slide image based on self-distillation,Yuan Wu,2023,09,0
9b98ca94b7733feee1cff5c57596611ad35fa7aa,Scaling Laws for Sparsely-Connected Foundation Models,Elias Frantar,2023,09,0
e892a225c417fbac7545c3e31b45d1c42dc9c933,Chain-of-Thought Reasoning is a Policy Improvement Operator,Hugh Zhang,2023,09,0
68ed29e5d398e6030cddcc575f1977973c8b0791,"A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time",Yeqi Gao,2023,09,0
dc4e366166a505320a5423393c2869591ec70200,Complexity Scaling for Speech Denoising,Hangting Chen,2023,09,0
7d7734879954700a0654bf9ddd3ba5dccad7b0ac,ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3),Edisa Lozi'c,2023,09,0
c413a339d7784574ed43debea494ef405ee09d81,"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs",Angelica Chen,2023,09,1
6483a6f2038cd8583ad5b6678602bc904459a7f7,EarthPT: a foundation model for Earth Observation,Michael J. Smith,2023,09,0
87ed9604338cca1c075b8988b43864781b32a0d0,The Grand Illusion: The Myth of Software Portability and Implications for ML Progress,Fraser Mince,2023,09,0
9bb3deca32af8d632e0d916c587cca6c185a6576,Uncovering mesa-optimization algorithms in Transformers,J. Oswald,2023,09,0
5aae7d84f8eaa55f3386cee41d94769e7ab01e9d,Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning,Ted Zadouri,2023,09,2
0726d711e4098f8ab73c6dc6af141cc2b66186a6,"Future-proof: Monitoring the development, deployment, and impacts of Artificial Intelligence",Anson Ho,2023,,0
b3c406596c1223e1d91f763cd2f107b0eea77b72,Generalization error bounds for iterative learning algorithms with bounded updates,Jingwen Fu,2023,09,0
110804428354df709b3693f9efc81946a9036ebf,"Neurons in Large Language Models: Dead, N-gram, Positional",Elena Voita,2023,09,1
3146b57943bf0f052b2fe3ebd58a8babb3aef343,Meta predictive learning model of natural languages,Chan Li,2023,09,0
00e889fcfaf4396a20f37f681cf8b14f3e878879,LLMCad: Fast and Scalable On-device Large Language Model Inference,Daliang Xu,2023,09,0
6fa243245110dde83317770a05398c01056f8251,Public opinion evaluation on social media platforms: a case study of High Speed 2 (HS2) rail infrastructure project,R. Yao,2023,,0
2aa0488985e500fdfa55bc11ee64ca45aa232955,"Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck",Benjamin L. Edelman,2023,09,0
a570d3d8f1b66a8ab3fff7876dc9bba3bcdc789b,Aligning Large Language Models for Clinical Tasks,Supun Manathunga,2023,09,0
9f5409401b78c587fad84e3065d1232e2d1ca817,Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness,Ze Peng,2023,09,0
d315ca681e95b73f2a6a6115d1e218dec9720d6f,QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm,K. Behdin,2023,09,1
4bb8aae62c6f6023705d27504e48cdc9f01e6044,RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,Harrison Lee,2023,09,11
135ae2ea7a2c966815e85a232469a0a14b4d8d67,Taken out of context: On measuring situational awareness in LLMs,Lukas Berglund,2023,09,3
a9caf21a845cb0b1b1d453c052188de118006093,SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills,Amey Agrawal,2023,08,0
3fd67db1afb95d46a67bd302f36aefe1624afc9f,The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants,Lucas Bandarkar,2023,08,3
48144ca4e20b183cfe9b410a3e9819da31df350a,Using Deep Learning for Flexible and Scalable Earthquake Forecasting,K. Dascher‐Cousineau,2023,,0
5c577988ccebfea96de86678d04fd94fad367d2e,Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,Neha Sengupta,2023,08,2
0ad412319d4e186c26f423ccc94d560bcaffd997,Serving MoE Models on Resource-constrained Edge Devices via Dynamic Expert Swapping,Rui Kong,2023,08,0
432d85f5d2165db6e7ffbe6bc7ad43c5fdeabf66,Generative Model for Models: Rapid DNN Customization for Diverse Tasks and Resource Constraints,Wenxing Xu,2023,08,0
48eb70ed72f985dc9d300042aeeac77084e3465f,Large Language Models to generate meaningful feature model instances,J. Galindo,2023,,0
19b43ff57e5d8f8a99da4110fbc30b4ecc39a527,Spoken Language Intelligence of Large Language Models for Language Learning,Linkai Peng,2023,08,0
a5cec58c2525b6f5b011d2170bf37a67b42d1fbb,Large Graph Models: A Perspective,Ziwei Zhang,2023,08,0
50a124ba06f63bb6d462a02a3f442af8b15bd0f5,The Poison of Alignment,Aibek Bekbayev,2023,08,1
841015849a9bde6d95b60143dad2b9f429a08ae5,AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning,C. Lessig,2023,08,2
374ebdc8240a35820cb7ab8bfca37e180e21b605,Sparks of Large Audio Models: A Survey and Outlook,S. Latif,2023,08,1
6ab61706e21c5360f3206f20031c994ae794a5c1,Considerations for health care institutions training large language models on electronic health records,Weipeng Zhou,2023,09,0
9cbbb250a565228ba328038ee7944b89cff53e84,Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning,Jiasheng Ye,2023,08,0
11cf88dce827bd67cbfa60400306318022e736d5,D4: Improving LLM Pretraining via Document De-Duplication and Diversification,Kushal Tirumala,2023,08,1
f8b90d640158f61c4553518a8554a73b540e07e7,From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models,Jing Yao,2023,08,1
3ed178316be914658a80e561bf00576577f34389,Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference,Ranggi Hwang,2023,08,1
7fedf859b24ac14b8016542750dd8c4c695d5151,Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature,Walter Hernandez,2023,08,0
99ec8a1e221e1ab86e9df308656639144729f628,A mathematical theory of relational generalization in transitive inference,Samuel Lippl,2023,,0
681f9009e22c947007b53455e9f8f22e29209010,Towards an Understanding of Large Language Models in Software Engineering Tasks,Zibin Zheng,2023,08,2
aa05929a10a7891a5081b5bfb67fb9ef35041640,GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training,Xi Deng,2023,08,0
8cf37154fc183d16e4c17c86309855248662b709,SeqGPT: An Out-of-the-box Large Language Model for Open Domain Sequence Understanding,Tianyu Yu,2023,08,0
b169cbff7d5a11afac18f929d5c69ea0933a4da1,GradientCoin: A Peer-to-Peer Decentralized Large Language Models,Yeqi Gao,2023,08,2
3826d8ef406de77f05c8774fb37d9431c5ceda95,Unreflected Acceptance - Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education,L. Krupp,2023,09,1
7bd6a6d55c55f5b66ab04f2f5fa6659bad928c68,Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants,Martin H. Radfar,2023,,0
a402c9b48238fb9755d8117f7c57eed039906939,CoMFLP: Correlation Measure based Fast Search on ASR Layer Pruning,W. Liu,2023,09,0
79395f4ef32b925381f7ec9a824b05bdb982fd33,Large Language Models as Zero-Shot Conversational Recommenders,Zhankui He,2023,08,3
7054a39213d8ab0b96fe632867df5df915af4f7f,Latent State Models of Training Dynamics,Michael Y. Hu,2023,08,1
f92e80ce8c78d733a7a2895ba3fd63707911b61d,Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training,Xiaoyang Wu,2023,08,0
d3741ebfe500da26a8a08b8f4c9325ab26015202,Two Phases of Scaling Laws for Nearest Neighbor Classifiers,Pengkun Yang,2023,08,0
451a657dabf80ebc43f6a3be518250b2cd5dfe1a,Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,Ziyu Zhuang,2023,08,1
54d1c51b5b4c70eba1caeb77397a1f330c089b9b,Mind your Language (Model): Fact-Checking LLMs and their Role in NLP Research and Practice,A. Luccioni,2023,08,0
64e802ea8e9dbe247c31fb06184c04dbf9e55e4e,EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce,Y. Li,2023,08,0
96f6ad72733599db609332987ec6b65e30f11d07,"Platypus: Quick, Cheap, and Powerful Refinement of LLMs",Ariel N. Lee,2023,08,4
0b220041eb83c23b7b10d32a5d08c0309d528071,Large Language Models for Information Retrieval: A Survey,Yutao Zhu,2023,08,1
9144b4010332136da0584f202db624ce81d1bcba,"Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",Jiajia Li,2023,08,0
c51192d7440807dc98cc4374fb5d919390d70b0b,OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization,Gustaf Ahdritz,2023,,47
88cba3a919aa10c18f42ccbf2bab753c17b3d947,Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling,Marija vSakota,2023,08,1
d55d9cc276f283a080ed003b097eb93a8202419c,Composable Function-preserving Expansions for Transformer Architectures,Andrea Gesmundo,2023,08,0
4cc2d056365b8b5a58ce57f4b0bf7b20cfa2b6b7,Universal Fuzzing via Large Language Models,Chun Xia,2023,08,3
f5ec0b22930faf15c3a7912bda367e8a9f4c8bd4,On the Unexpected Abilities of Large Language Models,S. Nolfi,2023,08,0
2f181fae61d469e651a8cc14eaa3ca639a62359a,Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK,Florian J. Kiwit,2023,08,0
26d4cb3aae776fd140e1197465e3e484a87dbae1,Topological Interpretations of GPT-3,Tianyi Sun,2023,08,1
0d77314166c54a17a98e1317f9ba1e48dcfe9e83,Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023),Jordan Kodner,2023,08,1
d78282bd8539a50dfaaff5690aaedffbc833e924,TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties,Karima Kadaoui,2023,08,0
049ad4be9cdfd92f02c36f8fb056c5c7a98ff750,PromptSum: Parameter-Efficient Controllable Abstractive Summarization,Mathieu Ravaut,2023,08,0
a37aee483e392678e6c376f6f9ab70ebda2952c5,Efficient and accurate sequence generation with small-scale protein language models,Yaiza Serrano,2023,,0
0a04d0f9ffc0d30157dea059abbf344c681908ed,DNA language models are powerful predictors of genome-wide variant effects,Gonzalo Benegas,2023,,4
f56f19fcae3dfbaa157fd4660595052a1d809dea,Advances in machine-learning-based sampling motivated by lattice quantum chromodynamics,K. Cranmer,2023,09,2
f6a503bd80a640ad7cb7e038e9e1b5618f8c24ec,The Capability of Large Language Models to Measure Psychiatric Functioning,I. Galatzer-Levy,2023,08,1
1e2eba005ccd8ab7a668a525c5b43245853bdaf1,Reasoning in Large Language Models Through Symbolic Math Word Problems,Vedant Gaur,2023,08,2
1e26b42669b060a3850e4766dea0db6e3c85cdec,Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey,Shihan Dou,2023,08,1
eacc01a7886e01c7e79bdebec242d3c1a2608dd0,The Paradigm Shifts in Artificial Intelligence,V. Dhar,2023,08,0
2edccb8fa562ed52cd49ea6fc67ed32db6218247,From Sparse to Soft Mixtures of Experts,J. Puigcerver,2023,08,1
c6026b642f7967484a353fc34815e184c0eb727d,Jack and Masters of all Trades: One-Pass Learning Sets of Model Sets From Large Pre-Trained Models,Han Xiang Choong,2023,,0
447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e,Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models,Jiaao Chen,2023,08,1
f4f090cd549c5961894b7f94a499ca4d3c4b0842,Linguistic Explanations of Black Box Deep Learning Detectors on Simulated Aerial Drone Imagery,Brendan Alvey,2023,,0
02718f60311b790a67788d6517574cf1d1a3f73f,Consciousness beyond the human case,J. LeDoux,2023,,0
7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4,When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities,Jin Chen,2023,07,7
f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d,Scaling Sentence Embeddings with Large Language Models,Ting Jiang,2023,07,0
d851a5096c74a4fd7a41be38acbaad518615af20,The physics of optical computing,P. McMahon,2023,08,0
5404f4e28645861ebed175e580e2b95a6fff9ca9,UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models,Sen Fang,2023,07,0
88695b5bb6462872ce1dd946cff00dd6ebabf2d9,Scaling TransNormer to 175 Billion Parameters,Zhen Qin,2023,07,1
4d21debb0f5fec315181e0912b5105c6ce4fc67f,Backdoor Attacks for In-Context Learning with Language Models,Nikhil Kandpal,2023,07,2
d6234d686b22db52ebb6c41f74706bb99df0faaf,On Physical Origins of Learning,Alex Ushveridze,2023,10,0
f1e77517b8921ad2570af3f35f36ec11958a26ef,Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI),Barnaby Crook,2023,07,1
304f8b4edea01fdb5a2f7f8b998c83188deeccff,Towards Generalist Biomedical AI,Tao Tu,2023,07,8
26fbf886ea26591536056b3d4a1724187356789f,Educational data augmentation in physics education research using ChatGPT,Fabian Kieser,2023,07,2
5e6f5d1dbb4b2b26755cd40466a4441e5b6f262e,ICON: A Linguistically-Motivated Large-Scale Benchmark Indonesian Constituency Treebank,Ee Suan Lim,2023,,0
c29dbfbc17fa190b787a2662d49f08a38c8bd166,ARB: Advanced Reasoning Benchmark for Large Language Models,Tomohiro Sawada,2023,07,8
3d04f5bb0599ce02d8fa47420f72f500758c4660,How to Scale Your EMA,Dan Busbridge,2023,07,0
e7634ee6523860fa893f6a82faaeb29fcb8cabf6,ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer,Youwei Pang,2023,07,0
685df9dfc68655096e02db7b9c372cd169e07b99,Optimized Network Architectures for Large Language Model Training with Billions of Parameters,Weiyang Wang,2023,07,0
6795e4ea791d05863c8496cced66ba13520f53cf,Deep learning subgrid-scale parametrisations for short-term forecasting of sea-ice dynamics with a Maxwell elasto-brittle rheology,T. Finn,2023,,3
e06595ebb2fe4d73fe42e566b57d7a109df75615,Instruction-following Evaluation through Verbalizer Manipulation,Shiyang Li,2023,07,3
435925f84dbe507fc7a16ac82b7824c8419d089c,Exploring the Landscape of Natural Language Processing Research,Tim Schopf,2023,07,2
87e5bb672d578c0f2bc654ba53d476186fd4b813,Exploring Usability Issues in Instruction-Based and Schema-Based Authoring of Task-Oriented Dialogue Agents,Amogh Mannekote,2023,,1
7aad760762c4a10dfbc2d3391eb8bdb28c80b236,Federated Large Language Model: A Position Paper,Chaochao Chen,2023,07,3
80b4a44ab0303ace08afc1110381866461048b23,Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey,Amogh Mannekote,2023,07,0
104b0bb1da562d53cbda87aec79ef6a2827d191a,Llama 2: Open Foundation and Fine-Tuned Chat Models,Hugo Touvron,2023,07,488
8282c6b141c335fc144818f20d86e68800691a01,"Reproducibility, Replicability, and Insights into Dense Multi-Representation Retrieval Models: from ColBERT to Col*",Xiao Wang,2023,,0
f5e670c22d1125de557aaa79f721fcfb557fcb36,Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning,Zachary B. Charles,2023,07,0
24a46662853bf85d316d655f2c8790aa67815d02,Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning,F. Liao,2023,07,0
94450a1c1fcfe2c57f2d345867e0394a48cf481b,A newcomer's guide to deep learning for inverse design in nano-photonics,Abdourahman Khaireh-Walieh,2023,07,2
a68dc9208aae7578e8ee384caa8ccbcf34e539e8,"Mini-Giants: ""Small"" Language Models and Open Source Win-Win",Zhengping Zhou,2023,07,0
94ce1d5924e05e8d75e43ce70044293ddcef850a,Large language models in medicine,A. Thirunavukarasu,2023,,30
291955cc13853de19c58337d6a68816d00c2fa74,CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models,Weilin Zhao,2023,07,0
97d8400f4e02e3831e00acbdedc612adfc9a3a90,How large language models including generative pre-trained transformer (GPT) 3 and 4 will impact medicine and surgery,S. Atallah,2023,,2
c064c79e3026f81e5043cd5b0f4264b4d43336e6,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,Bo Chen,2023,,5
3c8f5d5e04f5c46898632a9d3e631cf8bedddeb0,Implicit regularization in AI meets generalized hardness of approximation in optimization - Sharp results for diagonal linear networks,J. S. Wind,2023,07,1
42029832864c30c42a77538939f176f572b324a6,SecureFalcon: The Next Cyber Reasoning System for Cyber Security,M. Ferrag,2023,07,2
99b0c3a18050889c591e1db6d51ca01298638437,Transformers in Reinforcement Learning: A Survey,Pranav Agarwal,2023,07,0
0e41ae9360a962430650d5bb174de223aa8deea5,Navigating the Complexity of Generative AI Adoption in Software Engineering,Daniel Russo,2023,07,1
0b0d22adc201913c7ff186504db129cc51d9971c,No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models,Jean Kaddour,2023,07,5
3e664adb009dce373129a3563e4b2cb08731bc76,PolyLM: An Open Source Polyglot Large Language Model,Xiangpeng Wei,2023,07,5
2e4ad5efadaaa317d7f8f148e8c7d10fce97ba59,Scale Alone Does not Improve Mechanistic Interpretability in Vision Models,Roland S. Zimmermann,2023,07,2
e94e5f89f92018f2ff0a6b77e4373fb77a9d1c17,Stack More Layers Differently: High-Rank Training Through Low-Rank Updates,Vladislav Lialin,2023,07,3
4c4effecc6b59d442f1ae7dd76bdf1118fe8421c,Large Language Models,Michael R Douglas,2023,07,41
1b90e9e9734bed6b379ae87d688cb3b887baf597,Objaverse-XL: A Universe of 10M+ 3D Objects,Matt Deitke,2023,07,6
2111fb505e95de968d7a031aadbc5d5582851f70,Writer adaptation for offline text recognition: An exploration of neural network-based methods,Tobias van der Werff,2023,07,0
68fd6cc9b41291d625b41761149016be6485c0b3,ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey,S. Mohamadi,2023,07,0
ef4b604fca0c62dcd0d5caf7ca24ad74e285632d,MultiQG-TI: Towards Question Generation from Multi-modal Sources,Zichao Wang,2023,07,0
2d23de7d30419336633dd94115382818bcd68e8b,When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment,Tianwei Ni,2023,07,0
67470539bcd9c51016294c3e325b608554dc90a4,T-MARS: Improving Visual Representations by Circumventing Text Feature Learning,Pratyush Maini,2023,07,4
8eba1bb64ecfafb097e565f167b61a6a450ae733,Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources,Feiyang Kang,2023,07,0
4ad771a10145e5e3b7e74bf6e98b165d7258889f,What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?,Yan Zeng,2023,07,7
ed391d9143bb81715c051d866e2b51a4d4c9c772,Won’t Get Fooled Again: Answering Questions with False Premises,Shengding Hu,2023,07,1
3e5741ee9cfd23c79d2af2e209ebb1b57da96e2a,Improving Automatic Parallel Training via Balanced Memory Workload Optimization,Yujie Wang,2023,07,1
82b67d45ad067c55ae7616e20f1a4a1533b9037c,Postmodern Fermi Liquids,Umang Mehta,2023,07,0
1e3ef48abeef882e12f9553a1baf8944f3782c88,Several categories of Large Language Models (LLMs): A Short Survey,Saurabh Pahune,2023,07,2
72543b71571576e1a86ef55aadf3f17458670d0f,"LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens",Jiayu Ding,2023,07,13
425675cc3a4cfadf3b750e1563090a0ed378ddc2,"Abstractions, Scenarios, and Prompt Definitions for Process Mining with LLMs: A Case Study",A. Berti,2023,07,2
85d75d62e8c6955ce9d4bf2cd98cf2496bc7e800,Improving Language Plasticity via Pretraining with Active Forgetting,Yihong Chen,2023,07,0
36ccf3967e4cb30caeb9419c113eecde0074aa7e,Minimum Levels of Interpretability for Artificial Moral Agents,Avish Vijayaraghavan,2023,07,0
275fb93244b5a465d7e30fc6111e3403b47557be,scGPT: Towards Building a Foundation Model for Single-Cell Multi-omics Using Generative AI,Haotian Cui,2023,,10
1b173d9b8b0a2529259b6fa16376aff11c1ac08f,BatGPT: A Bidirectional Autoregessive Talker from Generative Pre-trained Transformer,Z. Li,2023,07,5
621902ab679a6e746760198bae556590813b1b4e,What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?,C. Conwell,2023,,9
8dd74ae5a0e580f69b08706d79eb1abcbbef132a,"An extended clinical EEG dataset with 15,300 automatically labelled recordings for pathology decoding",Ann-Kathrin Kießner,2023,,2
bab5e35001757719d0f8338f94dde2860dae784a,How Large Language Models Will Disrupt Data Management,R. Fernandez,2023,,0
942130a875ccfe55a4c60c27c636f693e25cb13d,Personality Traits in Large Language Models,Mustafa Safdari,2023,07,9
72160b3c0f73c968fcb903db71817d1bed695f4d,"Look, Remember and Reason: Visual Reasoning with Grounded Rationales",Apratim Bhattacharyya,2023,06,1
edccb296307f1ea187c403072159fc00b96cb888,λ-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces,C. Voelcker,2023,06,0
2adc13eb55c92e026c4cefc89a47a0ee0ac95111,SMILE: Evaluation and Domain Adaptation for Social Media Language Understanding,Vasilisa Bashlovkina,2023,07,0
a75166c6afaa6923dfc00ffaf388dc8f5194a9ce,Stitched ViTs are Flexible Vision Backbones,Zizheng Pan,2023,07,0
e11111dfda2a1f7aa9ecb8720032739233fb72f4,CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models,Yufei Huang,2023,06,3
f5dfba34121f0f2cfc3074d70194d7ac6e3bb4fe,Synthetic Alone: Exploring the Dark Side of Synthetic Data for Grammatical Error Correction,Chanjun Park,2023,06,0
6f4e5c46cea178819901e55852339b7d85d7e8e5,Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition,Meena Jagadeesan,2023,06,0
8ea59ec82cf28be5ccc428edc18ff3b7bedf4e1d,Recurrence and repetition times in the case of a stretched exponential growth,L. Debowski,2023,06,0
18b556c3d8b2b714a1eaca033a715c4f6c7493c8,The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets,N. Levi,2023,06,1
8aaeaec441c9f8bb6407b6633caa998bb35bc537,Exploring Data Redundancy in Real-world Image Classification through Data Selection,Zhenyu Tang,2023,06,0
2f0c2b38da784c81e983c7cc1e7bbabb47237c55,Lower Bounds for Multiclass Classification with Overparameterized Linear Models,David X. Wu,2023,,0
8c88c693d5dc2802d3b76b85740e1f04fdaaf801,Large sequence models for sequential decision-making: a survey,Muning Wen,2023,06,0
84960c4d1c4fed017413ef869f7a94cc043c1fc2,Action Q-Transformer: Visual Explanation in Deep Reinforcement Learning with Encoder-Decoder Model using Action Query,Hidenori Itaya,2023,06,0
c520d8a888355f7abb7728b2e2510fe7bc63f814,Large Scale Foundation Model on Single-cell Transcriptomics,Minsheng Hao,2023,,6
128f921e01d3c71d644346353bf72f830654ca49,OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents,Hugo Laurenccon,2023,06,9
f60137ff115c4741abb40c909ab94dfee92becd9,Eight challenges in developing theory of intelligence,Haiping Huang,2023,06,1
b18c62d515072ccc35709772388b91bac1045514,Deep Fusion: Efficient Network Training via Pre-trained Initializations,Hanna Mazzawi,2023,06,1
02e699561624adae5ea3550e8ebee988a2557286,Self-learning Monte Carlo with equivariant Transformer,Y. Nagai,2023,06,1
dfd1927e00519848b124449373168d00eb140bde,DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization,Amey Agrawal,2023,06,0
6946275ebf6fe106aa8c16d7ae75fab965dc470e,RedMotion: Motion Prediction via Redundancy Reduction,Royden Wagner,2023,06,0
1b0378d52d8988ffd5ecfb507e23420985171306,A Comparative Analysis of Automatic Speech Recognition Errors in Small Group Classroom Discourse,Jie Cao,2023,,1
c1104befbe6b4ded6274ccca28750b20982bbcb5,Graph Ladling: Shockingly Simple Parallel GNN Training without Intermediate Communication,A. Jaiswal,2023,06,1
1c7d3a359f4029e5384d784ce118ab53e71877b6,DropCompute: simple and more robust distributed synchronous training via compute variance reduction,Niv Giladi,2023,06,0
ed3101df8bd7b0addfb9ea8713ddb590a15461a2,Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness,E. Zelikman,2023,06,3
cc1723cf8a6cecb2295360ba01a01da90452403e,Structured Thoughts Automaton: First Formalized Execution Model for Auto-Regressive Language Models,T. Vanderbruggen,2023,06,0
b109a1b6254ca0f9467c98285a1d9a1f710f6b2a,Deep Generative Models for Decision-Making and Control,Michael Janner,2023,06,1
80aa5cc89de1c044e9218890e2726b791a413d56,Modularity Trumps Invariance for Compositional Robustness,I. Mason,2023,06,0
31d65e179b1d00484154b3525d93846dd82f23d8,Inverse Scaling: When Bigger Isn't Better,I. R. Mckenzie,2023,06,12
c117158a0cd31a3fbf44b6adc6fffca715d63c6b,Solving Large-scale Spatial Problems with Convolutional Neural Networks,Damian Owerko,2023,06,1
051549d8ef56937b2f4d113afdcf8c7586d3770b,Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models,Lingxi Xie,2023,06,2
f5359f596e0306599b4aa4157e6fe03567b35c01,Knowledge Distillation of Large Language Models,Yuxian Gu,2023,06,4
7eddac2807fa24901625b680bf7f2eec650e555a,Machine learning in computational histopathology: Challenges and opportunities,Michael Cooper,2023,,1
9afa0c3227fd0ec3a76928784e59c4205cbace24,"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",Alexander Tornede,2023,06,3
7c8c2067c5d6f39daa58e5271e25be00836c2cf5,Generic Attacks against Cryptographic Hardware through Long-Range Deep Learning,Elie Bursztein,2023,06,0
fdeebf187c390becf9890ccd37e0f189fa3929fd,Adversarial Attacks on the Interpretation of Neuron Activation Maximization,Géraldin Nanfack,2023,06,1
c23707cdd0966ea4bb31755e2d59fefa0970a707,End-to-End Neural Network Compression via $\frac{\ell_1}{\ell_2}$ Regularized Latency Surrogates,Anshul Nasery,2023,06,0
82da02137bae421a3f7a89c3bf2ab662037f4dfa,Embodied Executable Policy Learning with Language-based Scene Summarization,Jielin Qiu,2023,06,1
0f10fc57c7a4aae8181feeca69ad232a05ead7cb,Value function estimation using conditional diffusion models for control,Bogdan Mazoure,2023,06,0
beca17564ef9a03d42ce9db4e303689fba3ffcc1,Towards Autonomous Testing Agents via Conversational Large Language Models,R. Feldt,2023,06,5
c6d3262df3bf9b25a6890e1d59cd1287796b97b9,RRWKV: Capturing Long-range Dependencies in RWKV,Leilei Wang,2023,06,0
32f541216112de78037d8e0f95ddc152eb6f05fa,K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization,Cheng Deng,2023,06,0
378a545c3a1cf6c4aada8f9ee8820c0d8008220a,Benchmarking Foundation Models with Language-Model-as-an-Examiner,Yushi Bai,2023,06,13
bda25425e952fef6ecba708c3337425c6ccaa105,Privately generating tabular data using language models,Alexandre Sablayrolles,2023,06,0
1645e93f34ce34c0ff248a7349bf757a416c5312,Health system-scale language models are all-purpose prediction engines,L. Jiang,2023,,20
d11ae7f22045a2217fb2ef169037fba216153c63,Stabilizing Contrastive RL: Techniques for Offline Goal Reaching,Chongyi Zheng,2023,06,2
122f2d18a7b621576254ec12079941ecf0632360,GEO-Bench: Toward Foundation Models for Earth Monitoring,Alexandre Lacoste,2023,06,0
c44bdf4ff085f577a7cdd9a1cebf4af0b4eb0c82,Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex,Drew Linsley,2023,06,2
bf4810017b54e50354cccffd8966121c7166cb17,Iterative Translation Refinement with Large Language Models,Pinzhen Chen,2023,06,3
0ce17ce506657527d0e3ef3679c8be369c785d2b,Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models,Manisha Mukherjee,2023,06,1
066a9da13badca3791832f50f47103f31d681189,On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research,Made Nindyatama Nityasya,2023,06,0
3ccc28903fadce0ddbd5c54f1422d78acc8ec932,TPNoC: An Efficient Topology Reconfigurable NoC Generator,Jiangnan Yu,2023,,0
cf59882f25cacb1b688388ca46e6d306dd9fbd41,Efficient GPT Model Pre-training using Tensor Train Matrix Representation,V. Chekalina,2023,06,0
51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df,SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression,Tim Dettmers,2023,06,9
70ba08769e83d5869f89ee9f5f19fab443ab7e4e,Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models,Sunny Sanyal,2023,06,1
9cc5df8dffcd99a65b31076aed511d3d63d21c5e,Adversarial alignment: Breaking the trade-off between the strength of an attack and its relevance to human perception,Drew A. Linsley,2023,06,0
588180debec5be162364ad86d280390cb1693852,Large-scale Language Model Rescoring on Long-form Data,Tongzhou Chen,2023,06,2
2286562f2b185ce476a23f218f2de83b0561fbe9,bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark,Momchil Hardalov,2023,06,0
3fae5fb60ba9eeead5f1b2681ac6b09fe9cc4926,A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models,H. Ko,2023,06,1
0d9484f20e966c3faccc219b71cb1aa21e877569,OMNI: Open-endedness via Models of human Notions of Interestingness,Jenny Zhang,2023,06,3
751563cf0c32fe4dfa43d3416c916f8eb053e5f3,GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration,Aleksandra Piktus,2023,06,1
b045e0048d6362f5649cc4d0fb6aadb776e5a5c3,Multi-Objective Population Based Training,A. Dushatskiy,2023,06,0
8ea24b1dbb3e690ebc64543c03f0552a6c1fb49d,Knowledge of cultural moral norms in large language models,Aida Ramezani,2023,06,4
5a90a8f4ec612cef6b1bb9cf4eae897385d33c2d,An Overview on Generative AI at Scale with Edge-Cloud Computing,Yun Cheng Wang,2023,06,3
b15848be370a0ed38be2e75c3be807cc77b4e51e,Wearable based monitoring and self-supervised contrastive learning detect clinical complications during treatment of Hematologic malignancies,M. Jacobsen,2023,,0
d525996d538248685dccb038f85197619df14d93,Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning,Baohao Liao,2023,06,3
844b22bb025f485d85d00f1f61555a8ff0131658,STEVE-1: A Generative Model for Text-to-Behavior in Minecraft,Shalev Lifshitz,2023,06,2
6cd94eee6bb0d10e095af1a297919fc73c636297,Training-free Neural Architecture Search for RNNs and Transformers,Aaron Serianni,2023,06,0
e4a95f595b5d60a0858725996b9355f7275492cf,Hierarchical Attention Encoder Decoder,Asier Mujika,2023,06,0
7a1e71cb1310c4a873e7a4e54d1a6dab0553adce,"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",Guilherme Penedo,2023,06,90
c398de8d4a18ec49b8f2eaaf3b0473186b99e1e1,Reimagining Retrieval Augmented Language Models for Answering Queries,W. Tan,2023,06,1
7e55408ab2b3ffd7dca5109375058bac96f6e89c,Latency Matters: Real-Time Action Forecasting Transformer,Harshayu Girase,2023,,3
43d1320630e4e5ddc08e703b2ab5c32f3a121d1e,Data-Free Model Pruning at Initialization via Expanders,James Stewart,2023,,0
3167db922c8532213d909890a43c77e8cc9f9098,Explanations as Features: LLM-Based Features for Text-Attributed Graphs,Xiaoxin He,2023,05,9
d9ffb44ee3c8ec0b6692df8a90451384c1edd89b,Likelihood-Based Diffusion Language Models,Ishaan Gulrajani,2023,05,2
0a44b9cc5496e0cb26082f880cf1ed52ebc42d4a,Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey,Chen Ling,2023,05,7
17dfa45f14fcc1b861bc06b7f0b4678d870628d9,Intriguing Properties of Quantization at Scale,Arash Ahmadian,2023,05,3
97dd366154f62d07372b5e09e0ef006a4544b724,Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods,Tobit Klug,2023,05,1
32ff7e5ea4ef146cc63fdee23af1cc47e89af095,NetHack is Hard to Hack,Ulyana Piterbarg,2023,05,3
28902dc5b6dc0289962c8b88a04ffe03503388ea,Dynamic Sparsity Is Channel-Level Sparsity Learner,Lu Yin,2023,05,2
7dddeee6552bfc313e3bb9f0109f3ac5402c561e,Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity,Lindia Tjuatja,2023,05,0
04270591b6006a83b0a8970ef80bcbfc26a835d9,Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models,Y. Hu,2023,05,4
54c94a33c4140c532d4cd7e01713a71a1499fa40,A Rainbow in Deep Network Black Boxes,Florentin Guth,2023,05,2
4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5,Brainformers: Trading Simplicity for Efficiency,Yan-Quan Zhou,2023,06,2
e9f966dfac520ddf0ab053f93ec94f6b9fbe6412,Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks,Minki Kang,2023,05,0
5875e7cc6d8941f078e32b51b8d36fc08f9c1774,Feature-Learning Networks Are Consistent Across Widths At Realistic Scales,Nikhil Vyas,2023,05,1
1a527ac2736239019a9aedd3494443d5a22b57ad,Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models,Yuhui Zhang,2023,05,3
49e294319811fa4d122dcbd127e15188825d8c8e,Federated Conformal Predictors for Distributed Uncertainty Quantification,Charles Lu,2023,05,1
24811cadf16519910f643b6084107164e6ca4219,Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In,Zichun Yu,2023,05,1
9aaa71d9311b44e2228eac213c24c37bb9ca64d1,"Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale",Vijeta Deshpande,2023,05,2
6309a53b7321a6e71ddb248b45c37d3e25643ffb,Manifold Regularization for Memory-Efficient Training of Deep Neural Networks,Shadi Sartipi,2023,05,0
31a7d8c4a5ab6bab522494b57270249105c8748e,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks",Kaiyuan Zhang,2023,05,10
efabf7727dc3212676f03f1384809ce019db109d,Future-conditioned Unsupervised Pretraining for Decision Transformer,Zhihui Xie,2023,05,0
51b169701290cd129e0781fc9f3a9918604c89b5,Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model,D. Soong,2023,05,3
89ea9c3ebba8e518328e0f9598d3d3f79aa14c46,"Exploiting large neuroimaging datasets to create connectome-constrained approaches for more robust, efficient, and adaptable artificial intelligence",Erik C. Johnson,2023,05,0
1dc98c796078210f2734161b7af6d42ff8569338,"LanYUAN, a GPT large model using Curriculum Learning and Sparse Attention",Gonghai Zhou,2023,,0
e8d6dc483b439c1e5ab839e86794ba301dabed88,Automated Tensor Model Parallelism with Overlapped Communication for Efficient Foundation Model Training,Shengwei Li,2023,05,0
c193eb176985a81ae64f63c5e50b2f11cfb7c4e6,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,Sotiris Anagnostidis,2023,05,2
e904967df6fb2530d79d2772e8f56b2543a01648,Genomic language model predicts protein co-regulation and function,Yunha Hwang,2023,,0
b57509b85eb466d2c011796a5227500dcce3b05a,A Mathematical Interpretation of Autoregressive Generative Pre-Trained Transformer and Self-Supervised Learning,Minhyeok Lee,2023,,4
4631398b0d61061b9ca9489d76ded4dd05bcf1ec,"The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",Antonio Valerio Miceli-Barone,2023,05,7
762cccb8e0897579dd3882488b13a1100b6ab6e2,How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench,Qinyuan Ye,2023,05,0
1deb8eb83cc3bb30db98bc21917b815e758b15d4,Delving Deeper into Data Scaling in Masked Image Modeling,Cheng Lu,2023,05,0
6d31db7c53853de62cacec26facdb4300d6b5092,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,Zirui Liu,2023,05,2
ca05dacd9dc0a327cf91c646dca319f963e54946,Annotation Imputation to Individualize Predictions: Initial Studies on Distribution Dynamics and Model Predictions,London Lowmanstone,2023,05,0
bdb0cc86a8dd8f3a1558b7f3a1d001eea521c6c1,OverPrompt: Enhancing ChatGPT Capabilities through an Efficient In-Context Learning Approach,Jiazheng Li,2023,05,1
5d2dbbcccf3a47a73f746296408b500f399dc8d0,Emergent inabilities? Inverse scaling over the course of pretraining,J. Michaelov,2023,05,0
a460d28507b63b7265461cd62badd3dc095f600f,Boosting Cross-lingual Transferability in Multilingual Models via In-Context Learning,SunKyoung Kim,2023,05,0
5df78a86dc942d3c53aec5ed2e8ac141cb41aa61,Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions,Jiahuan Li,2023,05,6
19c63eade265d8a47d160098d97194b3b83d3770,In-Context Impersonation Reveals Large Language Models' Strengths and Biases,Leonard Salewski,2023,05,12
6dd44624ac912fb50c21c691806ee52d27e73abb,Large Language Models are Few-Shot Health Learners,Xin Liu,2023,05,8
1041bdde891d58247c0adbbaa5778e0b64dbf059,Think Before You Act: Decision Transformers with Internal Working Memory,Jikun Kang,2023,05,3
9141480721653789597b6e537ee0eeab401f3e60,PromptNER: Prompting For Named Entity Recognition,D. Ashok,2023,05,6
e4ceaac6047fa4931d11d06ace81b72bf5ffbde5,Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge Retrieval from Foundation Language Models,Tim Schott,2023,05,0
39af21a31aea2824d23bcb8b2812537c6597d1b5,When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale,Christos Baziotis,2023,05,0
7fb5b154b7cb6b6761e25b82f67138ccb5f116f6,The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning,Seungone Kim,2023,05,3
bb8f7fbec020675d269ccfa0e6e603f02b664c0d,PaD: Program-aided Distillation Specializes Large Models in Reasoning,Xuekai Zhu,2023,05,5
b555fbe8d4e30fc1312c615ab7dfe6ac030dbcff,Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction,Samuel Cahyawijaya,2023,05,0
e251587936562309fb0373b62c753a29b0952809,Multilingual Large Language Models Are Not (Yet) Code-Switchers,Ruochen Zhang,2023,05,5
6bf981314d81ca838d2cc55fc6f6265717792b67,Generating Data for Symbolic Language with Large Language Models,Jiacheng Ye,2023,05,4
ecc7b6708ea4c61b8fe16cf9b4cef4d392c670a8,Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model,Leo Liu,2023,05,0
e9a0b58b498e4d8b16706f04452e854caca1444a,Revisiting Acceptability Judgements,Hai Hu,2023,05,0
33b68ea3ce551e33d634660f20ce43fbff0b5fca,Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training,Hong Liu,2023,05,12
552572fa975344350b8af09009c647b4d4598f8c,Deep Learning based Forecasting: a case study from the online fashion industry,M. Kunz,2023,05,1
a9d61f1688247bb8d92c393395129c52b7c0d0f8,"Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors",Ridong Han,2023,05,10
569b6242853dd6299c3f5cdbc040dec53ead8033,Training Transitive and Commutative Multimodal Transformers with LoReTTa,Manuel Tran,2023,05,0
66d2021641c2003d8614c898bbddb653ec349b22,Rethinking Semi-supervised Learning with Language Models,Zhengxiang Shi,2023,05,5
1567bcac0ab09269c9d0ff33c9a406132417fab9,"A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",S. Longpre,2023,05,13
a6df4b0c0cee5865a29bb7b9d4d424821de0681f,Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance,Yue Zhang,2023,05,4
1e4c49c9c93678dec95326ce25715fd2a1e64192,Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model,Xiao Wang,2023,05,1
026b3396a63ed5772329708b7580d633bb86bec9,RWKV: Reinventing RNNs for the Transformer Era,Bo Peng,2023,05,34
07ca3f17e63e9415be9fe830cc14df507d271330,To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis,Fuzhao Xue,2023,05,6
fdb10253a1c32f4c51a6499f0e3bc3fdb11a51da,Learning Interpretable Style Embeddings via Prompting LLMs,Ajay Patel,2023,05,1
daf9e24adbba3d1aead91cbac26502d3043db069,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,Mutian He,2023,05,6
851b7a7110eba40cba9c85635ec7fe53f82e1a58,Communication-minimizing Asynchronous Tensor Parallelism,Siddharth Singh,2023,05,0
1d98e3de197c56ff89754fb4423418d8df5e931f,Automated Few-shot Classification with Instruction-Finetuned Language Models,Rami Aly,2023,05,0
8376e50e81329b3db5049a90851cc0418d071e3d,Scaling laws for language encoding models in fMRI,Richard Antonello,2023,05,4
5692501c10d0c1762842f92c66fcf0bffe2c0342,Multimodal Web Navigation with Instruction-Finetuned Foundation Models,Hiroki Furuta,2023,05,6
506d709b4a11ebf1b2e2f142f4b6f0d58abac450,AutoTrial: Prompting Language Models for Clinical Trial Design,Zifeng Wang,2023,05,2
4f0c7f4df04f07609bdb67944af2a529d5a4517b,A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation,Xiaowei Huang,2023,05,7
c07eebebee2b2d8519dc730dbcd979f8dfd7b562,Reducing Sequence Length by Predicting Edit Operations with Large Language Models,Masahiro Kaneko,2023,05,2
018e943ba0452b05edd903c3eaf746068ebca138,LeTI: Learning to Generate from Textual Interactions,Xingyao Wang,2023,05,5
ef0a8963022aa11ac9e1042acc086052b8ec1678,Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model,Jiageng Wu,2023,05,4
a4b1c935b9bf55fbb003e2c3f49c7bb28906c789,PaLM 2 Technical Report,Rohan Anil,2023,05,236
3fb0731538c59f8520a309996a0567b58965f0fe,Pre-Training to Learn in Context,Yuxian Gu,2023,05,3
5c7aaee5651221893ea0e67c363cab4c4be53b83,Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning,Haowen Chen,2023,05,5
d50ddfa3f02b8fa5d0a7bfd396ddaf9afc895cb1,Revisiting the Minimalist Approach to Offline Reinforcement Learning,Denis Tarasov,2023,05,1
2743ce22ace6362be2a01900106bf4b62280e22c,LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development,Ilias Chalkidis,2023,05,3
412e266cddfd87c79087a88ba1e4d11b89a45a13,MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,L. Yu,2023,05,11
28085f480ce456a376ebace9b899e3bc93dbc048,TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,Ronen Eldan,2023,05,22
742a4098ddc25fd3c53d4ebdfc845d1e5d2bbff7,How Good are Commercial Large Language Models on African Languages?,Jessica Ojo,2023,05,0
b5982d5ef4eed477bb3dacfe43c5c5b5fe173694,WebCPM: Interactive Web Search for Chinese Long-form Question Answering,Yujia Qin,2023,05,7
ac1d4a24e58871d0173515709b9ef0c5d64c3294,Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,Haoyang Huang,2023,05,15
661e8ac4908a9d2a85835245ea99b6a314cc4a60,Large Language Models Can Be Used To Effectively Scale Spear Phishing Campaigns,Julian Hazell,2023,05,16
1fbaf2d8b69ef6e42a38c233f5d01bea70bad5b7,Fast Distributed Inference Serving for Large Language Models,Bingyang Wu,2023,05,2
6cec825e32b1790a69893a5b2506818241506217,A Glimpse in ChatGPT Capabilities and its impact for AI research,F. Joublin,2023,05,4
516631db8d75b3f223ae66260a3e048d6d8eae72,LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM,Wenhui Hua,2023,05,0
ea8197cb357af6f89e8b6e5548897236a24719b1,What is the best recipe for character-level encoder-only modelling?,Kris Cao,2023,05,0
b7ec5ff2ebe38c2d0191ad0a728e1726d200f645,StarCoder: may the source be with you!,Raymond Li,2023,05,101
9e41cd86a5ebdcbecfef6fd7395e736e2453e495,When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution,Zhengxiang Shi,2023,05,5
9d872153d5905e4555cbfd59a1955d3c89879432,The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation,Dung Nguyen Manh,2023,05,2
e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2,Augmented Large Language Models with Parametric Knowledge Guiding,Ziyang Luo,2023,05,6
a026ff41d370292caa13b2c2ce540f1e19bb79fc,Leveraging Synthetic Targets for Machine Translation,Sarthak Mittal,2023,05,0
82b17686abba18dfc821a262dab5fbb081aa2388,Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization,Anastasia Razdaibiedina,2023,05,2
04000ad50c18192a322f8fc031ce4225aa3cede4,Conformal Nucleus Sampling,Shauli Ravfogel,2023,05,5
e71a0140d9d690a6236c07451c7f03272b534562,BranchNorm: Robustly Scaling Extremely Deep Transformers,Yanjun Liu,2023,05,0
29312bc12a22c423dd0968a18cd9e422881e29c6,Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity,Haoran Xu,2023,05,1
d4d7966a7b4dbf4cd764e443392d22826618a293,Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs,D. Narayanan,2023,05,0
450b5490cc653478c272be50aa986798df828a20,Uncovering ChatGPT’s Capabilities in Recommender Systems,Sunhao Dai,2023,05,22
c982c02c56a3d06e7c46ab98211bab0181b1da7e,Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy,Aly M. Kassem,2023,05,1
3a5ee836d71e1360015990e5a67f545d00e6ca10,Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations,Moucheng Xu,2023,05,1
5e13bf41c138fb0518a1bdebe4d2cbe699812d8f,Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner,Zhengxiang Shi,2023,05,7
5aec1050fd62971f2b40b597ef7526fbc820beab,Automated Program Repair in the Era of Large Pre-trained Language Models,Chun Xia,2023,,26
13e0f0bf9d6868d6825e13d8f9f25ee04285cd29,Poisoning Language Models During Instruction Tuning,Alexander Wan,2023,05,14
8cf819f6ee33909484ece40d79944c9c37f01a89,"A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development",Tianyu Wu,2023,,31
d9f7a191c1febd6a4d2d3e2530d8811cb0686898,Concepts and methods for transcriptome-wide prediction of chemical messenger RNA modifications with machine learning,P. Mateos,2023,,0
96c9524916bc38c14e1569c8a21e6497dac709cf,Language Models for Multimessenger Astronomy,Vladimir Sotnikov,2023,,0
1b31c83ecb6da7964e8294cb5bd8cb6d3b380e34,Exploiting Input Tensor Dynamics in Activation Checkpointing for Efficient Training on GPU,Jian Liao,2023,,0
b533c638bcae456fbecc9716516122e923b51a32,The Emotions of the Crowd: Learning Image Sentiment from Tweets via Cross-modal Distillation,Alessio Serra,2023,04,0
47975c425e03911ad2ac9e618bd9ae1b23e4c852,Hyperparameter Optimization through Neural Network Partitioning,Bruno Mlodozeniec,2023,04,3
186e96fe036927182ec963b63f9dd7f8ff650158,"ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",Chunkit Chan,2023,04,14
4128eb532a044cc67f21df82d4b78b62f9c7f6ee,Towards Automated Circuit Discovery for Mechanistic Interpretability,Arthur Conmy,2023,04,22
b4eacdaa5a0d765b1804ccc6289dce84a86840d6,Revisiting Network Value: Sublinear Knowledge Law,Xinbing Wang,2023,04,0
389ec3e8902a5dcfcde1adec735854e93f845937,LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions,Minghao Wu,2023,04,18
43111716c14c9e509dceffdd60196ad3f5912a36,DataComp: In search of the next generation of multimodal datasets,S. Gadre,2023,04,33
04ee9597be4d6d2457214334e495e591000b5542,PMC-LLaMA: Towards Building Open-source Language Models for Medicine,Chaoyi Wu,2023,04,21
4f0c057ebc71dc8804d503495f1a35c9cfb9b86f,Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models,D. Honegger,2023,04,0
131c6f328c11706de2c43cd16e0b7c5d5e610b6a,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,Jingfeng Yang,2023,04,70
3caaea71b338a1a6d33263be06e5fa8d35438093,Enhancing Inverse Problem Solutions with Accurate Surrogate Simulators and Promising Candidates,Akihiro Fujii,2023,04,0
ca3037fed8ed14dea92985b9f288b05185f867d0,Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks,Anders Giovanni Møller,2023,04,7
8bc617c9139648d7a92991d70c671230bac7b2e2,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",Rongjie Huang,2023,04,42
097adbdd0afb2c4f4bdb694a2a8228a248ed1e36,Application of Segment Anything Model for Civil Infrastructure Defect Assessment,Mohsen Ahmadi,2023,04,5
a1d990d92bf199525aea68b84a56c62eafc7f27b,Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search,Andrei Kucharavy,2023,04,0
261549439aebdda72b648ecc462448fd24857ac1,Progressive-Hint Prompting Improves Reasoning in Large Language Models,Chuanyang Zheng,2023,04,36
682346d2a78fa57e8e9649052f3fea01f5924f1e,Robust flight navigation out of distribution with liquid neural networks,Makram Chahine,2023,,2
0ffd57884d7957f6b5634b9fa24843dc3759668f,Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study,Perttu Hämäläinen,2023,,30
05e003a34148d4663734d3f39deefa0979d2a0e6,GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information,Qiao Jin,2023,04,14
d0a93ad1af752853f73d89288e8fafb0c9906801,"ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models",Yikang Liu,2023,04,8
d4d461288e76c7b6a36ffd6b66c26815bc7bd8e6,STen: Productive and Efficient Sparsity in PyTorch,Andrei Ivanov,2023,04,0
ebb1acf1eb3705510de5a19065092404427428a5,STU-Net: Scalable and Transferable Medical Image Segmentation Models Empowered by Large-Scale Supervised Pre-training,Ziyan Huang,2023,04,8
a43a3fadc9190e61b34f59a913f1716e443519e4,On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence,Gengchen Mai,2023,04,27
4812ea10550820dee56ce6e38795ce04e0a351b5,Exploring the state of the art in legal QA systems,Abdelrahman Abdallah,2023,04,2
93752cae0d4ecd2d09d6660feb3c1860af973f18,Can Large Language Models Transform Computational Social Science?,Caleb Ziems,2023,05,39
b66fe3aa4a795060c2f0f001c502eb126d4d2876,RRHF: Rank Responses to Align Language Models with Human Feedback without tears,Zheng Yuan,2023,04,42
281a7a99c16ce8f53bfbfb7aeb460dbd28648d28,Toxicity in ChatGPT: Analyzing Persona-assigned Language Models,A. Deshpande,2023,04,42
31296e09c844eb684136ee1b4930da3b1396bba6,Balancing Privacy and Performance for Private Federated Learning Algorithms,Xiangjiang Hou,2023,04,0
052e9e31544869957996c1a44084a9c41d29bcfe,Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis,Wenhao Zhu,2023,04,37
38179848e2d6a3ad373b1793848816111428ac36,OpenAGI: When LLM Meets Domain Experts,Yingqiang Ge,2023,04,16
f274ce903eca789be99fd6e721dc6e213f8debeb,Learning a Universal Human Prior for Dexterous Manipulation from Human Preference,Zihan Ding,2023,04,2
dbbc5003af690799fa4fe6330fb795311cde106f,FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement,Xiaonan Nie,2023,04,7
9af359ce4b885759e5b775b36e9332289efefb33,Comparative single cell epigenomic analysis of gene regulatory programs in the rodent and primate neocortex,Nathan R Zemke,2023,,1
13581a46d32822e44cbeb1acdba4a59cef2b2ec1,On Efficient Training of Large-Scale Deep Learning Models: A Literature Review,Li Shen,2023,04,10
0665b9c0637c416699eab8b205bbce6564f99328,On the Pareto Front of Multilingual Neural Machine Translation,Liang Chen,2023,04,0
79a574788b99f3a065cdc972c94497a921af5239,Inductive biases in deep learning models for weather prediction,Jannik Thümmel,2023,04,2
7470a1702c8c86e6f28d32cfa315381150102f5b,Segment Anything,A. Kirillov,2023,04,578
b2fafd5a5a6376c1afea627ffea571d5c6acf134,Predictive Coding as a Neuromorphic Alternative to Backpropagation: A Critical Evaluation,Umais Zahid,2023,04,1
5861df95084cf739a6ca3185d6523dd702bd1f10,Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT,Yi Qi,2023,04,3
23a183676b28269e7a427c41da7329b6326a9f17,Eight Things to Know about Large Language Models,Sam Bowman,2023,04,32
e06b097b0b3865d627b3bcc0d8454242bfb3ffab,Artificial Intelligence for Scientific Discovery at High-Performance Computing Scales,King Lee,2023,,0
5a23a489bb9e742edacc8b8e778b06e1594365d3,Generative AI at Work,E. Brynjolfsson,2023,04,29
35e7f3c5f70389e445f4ef615bff1ef4a8b16c84,AI SoC Design Challenges in the Foundation Model Era,Zhengyu Chen,2023,,0
2b8d28149a43b9659a6da2c56014ec4206a912b4,Ticket automation: An insight into current research with applications to multi-level classification scenarios,A. Zangari,2023,,1
1d29334cfbe9a1a943082058876f0c22d44c62fd,A Survey of Large Language Models,Wayne Xin Zhao,2023,03,396
06c6eca6bf50d10f7e8a9d9a29f9457526a0d7a5,Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare,Dongyeop Jang,2023,03,5
83edcfbb206ddad38a971d605da09390604248ea,BloombergGPT: A Large Language Model for Finance,Shijie Wu,2023,03,111
11265dd3ba850f41c52d90c7a88c8e8f8688364d,Accuracy and Architecture Studies of Residual Neural Network Method for Ordinary Differential Equations,Changxin Qiu,2023,,0
138fa5f64fe54376022998fe553b6156a93ff19e,Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need,Vivien A. Cabannes,2023,03,2
7b52b03775c72503684d48411f562ae29870dd3a,Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture,Peiyu Liu,2023,03,0
cfb5ef050cad787196d60a55a9583107d02e58e3,BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning,Changdae Oh,2023,03,4
fbf6a107dea17d2c5e0b714ad5541f5bef8317d3,Guided Transfer Learning,Danilo Nikoli'c,2023,03,0
424132ec245c3173685751ac1101c3be6cc55a67,xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data,Jing Gong,2023,,1
15145a009d3532b8cfb663805115f5f229396079,A Simple Explanation for the Phase Transition in Large Language Models with List Decoding,Cheng-Shang Chang,2023,03,0
38b722315873e4519c47cd27b578a14989a0a453,Exploring the Benefits of Visual Prompting in Differential Privacy,Yizhe Li,2023,03,3
285dae5c2f2ef55c70971094a1ddd45afe720eee,Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense,Andrei Kucharavy,2023,03,5
23c3f8139124c65533566bf14cc66f8fbf29bc37,Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture,Jascha Achterberg,2023,03,0
3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1,EVA-02: A Visual Representation for Neon Genesis,Yuxin Fang,2023,03,17
b9efcc0930946333bbf4dcf59a5c7a06a5c68389,What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network Training via Compute Monitoring,Yonadav Shavit,2023,03,8
12d16f426edc6ab248fb476007bd1646282d4d68,Language Model Behavior: A Comprehensive Survey,Tyler A. Chang,2023,03,10
348a1efa54376fa39053e5e25d52bd0eb6a0ba68,Capabilities of GPT-4 on Medical Challenge Problems,Harsha Nori,2023,03,166
362cbfd0d05e139cd6cf049754098a6e1520b910,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,Xiaozhe Ren,2023,03,18
78627329741d84f7590b5c1fa8297a074dcc0f4a,"Bridging the Global Divide in AI Regulation: A Proposal for a Contextual, Coherent, and Commensurable Framework",San-hee Park,2023,03,0
5501d00310b06e00351295529498cc684187148d,GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models,Tyna Eloundou,2023,03,108
0c996bb80acce751973940175862962cf0b3a2fe,CoLT5: Faster Long-Range Transformers with Conditional Computation,J. Ainslie,2023,03,16
6b2395ebcdc3f284d30fefe5d9877aa3caa7b53f,Reclaiming the Digital Commons: A Public Data Trust for Training Data,Alan Chan,2023,03,2
f59559bbc4640a63f5e3b9ede8df763acb82353f,Towards the Scalable Evaluation of Cooperativeness in Language Models,Alan Chan,2023,03,1
c926bd6ebdbcf0bfabb55c1f0171055bc2dbe516,Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs,Kelvin Guu,2023,03,8
443c1bef6a7dc3db941375ae76451c884ceffb8a,A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training,Siddharth Singh,2023,03,0
4a8da6b76b6d4396e407973d236c7b7d282d7259,Complement Sparsification: Low-Overhead Model Pruning for Federated Learning,Xiaopeng Jiang,2023,03,0
2c5460afa19ad6fc2568b7e210115acacc14a40c,An Overview on Language Models: Recent Developments and Outlook,Chengwei Wei,2023,03,9
e770bdf8f00beb180a6b6509dd9a09b1bdbaed68,"ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the LexGLUE benchmark",Ilias Chalkidis,2023,04,8
ee2452dd435c1587678f1f831c8db25501f5ea9f,Word meaning is both categorical and continuous.,Sean Trott,2023,,0
f661bdc053ba13df80eda479791306e1178db235,Magnushammer: A Transformer-based Approach to Premise Selection,Maciej Mikuła,2023,03,4
11f558df6f4e3c9da5a4d3a2cedeb54e288cb637,On the Risks of Stealing the Decoding Algorithms of Language Models,A. Naseh,2023,03,2
09cc98aaf9cffc174846ffc69fc076c2871df19a,Exploring Efficient-Tuned Learning Audio Representation Method from BriVL,Sen Fang,2023,03,0
24cf9a7b11fa4cf945773f3d2b7e0e0f8d2f615d,"Extending the Pre-Training of BLOOM for Improved Support of Traditional Chinese: Models, Methods and Results",Philipp Ennen,2023,03,1
36cd3d3a9e8a64f322476153513ece1fd617acfc,nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models,Matthias Cosler,2023,03,4
41cb6257f262822ff517e2ff5226c283b674caa4,Performance of ChatGPT on the MCAT: The Road to Personalized and Equitable Premedical Learning,V. Bommineni,2023,,4
16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6,The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,Hugo Laurenccon,2023,03,64
d7e00702bbb5a0cccc97033f0405b634ae9e2d3c,Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent,Xiaonan Nie,2023,03,3
542905f5fc96bce7572f6ded7f56aedfa62270c1,Understanding plasticity in neural networks,Clare Lyle,2023,03,8
1462a0e5b7db47301bb0995db56426e1f4a0ac7d,Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers,Tianlong Chen,2023,03,6
1f040c3a8d49f8e54169a0e07013692c7d58de4b,How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks,Xuanting Chen,2023,03,18
02eb7a537825c56865eee239bada016f9e5b0901,Learning curves for deep structured Gaussian feature models,Jacob A. Zavatone-Veth,2023,03,0
aeeadd43a95ac7af93f6fbf7b165655157a30055,Large Language Models: The Next Frontier for Variable Discovery within Metamorphic Testing?,Christos Tsigkanos,2023,,1
430aa6966c15c4a20a4fb2d8383e136b9cb6cde7,Almanac: Retrieval-Augmented Language Models for Clinical Medicine,W. Hiesinger,2023,03,7
6e036e28e7af03bfcdd98ffa254df6644f7657c5,GNOT: A General Neural Operator Transformer for Operator Learning,Zhongkai Hao,2023,02,10
932b9fd1e2aaf3c56841304b7a49e30c804f6234,Inseq: An Interpretability Toolkit for Sequence Generation Models,Gabriele Sarti,2023,02,13
77e5d0a68afcffb27191572590deced60feb9d5d,Retrieved Sequence Augmentation for Protein Representation Learning,Chang Ma,2023,02,1
c23c2c8df918e4f327dbc27c9471a1485360933e,MUX-PLMs: Pre-training Language Models with Data Multiplexing,Vishvak Murahari,2023,02,2
afeef45429c9c349133c88e9069cf979c89c4279,What makes a language easy to deep-learn?,Lukas Galke,2023,02,0
5be35511f774e09877e4c6ce7a4fd81891db188b,"Phase diagram of training dynamics in deep neural networks: effect of learning rate, depth, and width",Dayal Singh Kalra,2023,02,1
4c4b1bd9f36ccc545cf051efa9d03db2ed045770,Reward Learning as Doubly Nonparametric Bandits: Optimal Design and Scaling Laws,K. Bhatia,2023,02,0
3ac2d89388a816786234aa9f8ef2de9a635b0a69,"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",Yilun Du,2023,02,24
2cf43a61d0937ad25f23eaef7c90253ab799b3c7,Poisoning Web-Scale Training Datasets is Practical,Nicholas Carlini,2023,02,31
25d4ffc9fb1b320137ea51612ad4fdb1fdfcee19,Harms from Increasingly Agentic Algorithmic Systems,Alan Chan,2023,02,14
139ebdcaa3d5f60a3c5289bb61d5865141886d51,Optical Transformers,Maxwell G. Anderson,2023,02,3
2d227f3a0203c2491bcf7c77ae594cea3a7caf89,Cluster-Guided Label Generation in Extreme Multi-Label Classification,Taehee Jung,2023,02,1
89184ab496b2a1ae31e068e628479b4cd8f4b9d2,Do We Still Need Clinical Language Models?,Eric P. Lehman,2023,02,32
eebe95c31aba4b862e1bc3d312ca60094bd01f61,"With Shared Microexponents, A Little Shifting Goes a Long Way",Bita Darvish Rouhani,2023,02,2
22e2f488ecd88bd2adf79092d0d390d8f7b06a0f,Auditing large language models: a three-layered approach,Jakob Mokander,2023,02,36
6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552,The Capacity for Moral Self-Correction in Large Language Models,Deep Ganguli,2023,02,42
288c66ee0afaaaa44831bdbce2893453ef74ceda,Over-parametrization via Lifting for Low-rank Matrix Sensing: Conversion of Spurious Solutions to Strict Saddle Points,Ziye Ma,2023,02,1
629bc57782bb4326a3eb5f89314e350729c5f417,AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models,Alexandra Chronopoulou,2023,02,12
05b9bbd578c3f1d52af0fb3c9ac355541ca1c3e8,In Search for a Generalizable Method for Source Free Domain Adaptation,Malik Boudiaf,2023,02,6
5b3bbac233a8fd08675fdd511c5cd461ecb47736,ASR Bundestag: A Large-Scale political debate dataset in German,Johannes Wirth,2023,02,0
71ef07a18d1e2127bbe0c3e4db1d16bf386ade9a,Level Generation Through Large Language Models,G. Todd,2023,02,10
101958920c32d67b6509cd849e6644b28d7e473f,Binarized Neural Machine Translation,Yichi Zhang,2023,02,1
feea0452e03b78f7c85f40e5daa1bd08b61bb44b,Revisiting Offline Compression: Going Beyond Factorization-based Methods for Transformer Language Models,M. Banaei,2023,02,0
a19c67d9d1d416731930dff76d8c79ff5b8247c9,Population-size-Aware Policy Optimization for Mean-Field Games,Pengdeng Li,2023,02,1
e0401ca2d4fd6d0ed55130a4a24b33ed90111479,Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories,Suyu Ge,2023,02,3
0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3,Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning,Thomas Carta,2023,02,27
7a2a5b8c63bb537678c8be2d654132f52f09398a,Toward Large Kernel Models,Amirhesam Abedsoltan,2023,02,1
7080caab6265c6432a3eeb1328ab64a6e3bd4a08,Multipath agents for modular multitask ML systems,Andrea Gesmundo,2023,02,1
e962f95e03a50ff2f3a0fe7840daebac04578c46,Structure-informed Language Models Are Protein Designers,Zaixiang Zheng,2023,02,12
a8f9be0f611af3aea17baa639d556d601be80477,Exploratory Inference Chain: Exploratorily Chaining Multi-hop Inferences with Large Language Models for Question-Answering,Shosuke Haji,2023,,0
43cefce076df7ee54505fd78a8a97129c0f6d36b,MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism,Quan Zhou,2023,,2
94d84d1403a23a8a8486a151f52a126beb16875c,Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks,Christopher W. F. Parsonson,2023,01,0
b369b05f5386ce22dc7fa33c6f912d5c1cd27f14,The Power of External Memory in Increasing Predictive Model Capacity,Cenk Baykal,2023,02,0
24576dcca716c82f66b8cc3c85ecfae18be41edd,Adaptive Computation with Elastic Input Sequence,Fuzhao Xue,2023,01,2
fbd49b25bdab98c171af49962a41139c73dacbde,Specializing Smaller Language Models towards Multi-Step Reasoning,Yao Fu,2023,01,47
c9d46cfcf0211d11356c295ecd9584c84c19c8f8,Alternating Updates for Efficient Transformers,Cenk Baykal,2023,01,0
528bfc811a3b4213566134afe2c880f867be5065,"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",Terry Yue Zhuo,2023,01,13
43016361a50fc150696a0d73928b5cde1443ae38,Policy-Value Alignment and Robustness in Search-based Multi-Agent Learning,Niko A. Grupen,2023,01,0
5882dd04d95c9c88cdec389059fcf44d56cbb789,Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation,Jessica Huynh,2023,01,2
58d6ec0dec4952e93be7cf72c1cebb0216eac9df,Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers,Yangyang Feng,2023,,2
e1c9da44c7a5d66c802c0f42816a19720842e357,An Experimental Study on Pretraining Transformers from Scratch for IR,Carlos Lassance,2023,01,5
abba9a6f99d877fdd1b8412ddfcc26fdac6163dc,SMART: Self-supervised Multi-task pretrAining with contRol Transformers,Yanchao Sun,2023,01,8
7ed237af793f43c442b3e8e1bc9ace906a276b2a,Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?,Jielin Qiu,2023,01,4
bfe6fd05f09647b001c7eb6e333a95c881c88344,Human-Timescale Adaptation in an Open-Ended Task Space,Adaptive Agent Team,2023,01,32
f39d601fdcfc4c029bb33be2898856e6d4673bdb,AutoDDL: Automatic Distributed Deep Learning with Asymptotically Optimal Communication,Jinfan Chen,2023,01,0
c879413103f8950bdd414c7f60a39bd7748c9be8,Prompting Large Language Model for Machine Translation: A Case Study,Biao Zhang,2023,01,35
9a9e68d400069f023f7dc9b982226c95159a509d,Dissociating language and thought in large language models: a cognitive perspective,Kyle Mahowald,2023,01,91
7c0013dff50fe4dbc6af3677324a5852716f121a,"Mephisto: A Framework for Portable, Reproducible, and Iterative Crowdsourcing",Jack Urbanek,2023,01,3
1673b72a4ebb56ae3775131937c40afae3853e9b,Data Distillation: A Survey,Noveen Sachdeva,2023,01,9
c7fc4c09a18bf0c26d04fc69f5567bfd3ac0c8f6,Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language Understanding,Yunchang Zhu,2023,01,0
3a82ca5b41eb1e50e78d48c1cf229d2053e774df,A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems,Megan M. Baker,2023,01,8
6975638e6a7ee1e403b3d76242b3ec7f127d91f6,Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data,Vladislav Lialin,2023,04,0
776f3b645743e6daee06f18078598b974812c497,Bayesian Interpolation with Deep Linear Networks,B. Hanin,2022,12,6
4b308ba40e67b0b4b25c6fde17195d5a456a2f41,Cramming: Training a Language Model on a Single GPU in One Day,Jonas Geiping,2022,12,19
6052486bc9144dc1730c12bf35323af3792a1fd0,Large language models encode clinical knowledge,K. Singhal,2022,12,254
b118283afc5d8652de52cd13a5e287d76c5ec91f,Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text,Liam Dugan,2022,12,13
8ca527976a1e803e10fae88a7ef37c4254af1d0b,Finetuning for Sarcasm Detection with a Pruned Dataset,Ishita Goyal,2022,12,0
77d3fb31d3d0a8929aa5e46aac9910e793b94557,The role of machine learning in HIV risk prediction,J. Fieggen,2022,,0
658a8195ca7dc839bf254d4b2eb67c50384c5c6e,Language models generalize beyond natural proteins,Robert Verkuil,2022,,31
54a4517022703a27e1670d3b84214521882f0108,Contrastive Distillation Is a Sample-Efficient Self-Supervised Loss Policy for Transfer Learning,Christopher T. Lengerich,2022,12,1
7a5cd8a1bf99c9cc58bd7a818be446c29e9e1cbb,SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning,M Saiful Bari,2022,12,4
8bf77f3f14d20b36a0a4b96693e0a6480f17aac1,HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation,Hamish Ivison,2022,12,7
44c376d9195c10bafd052835c8cc1508b5a8a96a,Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation,Yaoming Zhu,2022,12,1
1e03fe3f5d1c1f0e413e368fe42109774ac975e4,Evaluation for Change,Rishi Bommasani,2022,12,0
f3b15f3399d26409ce519e522cb0e07d1651731b,Multilingual Sequence-to-Sequence Models for Hebrew NLP,Matan Eyal,2022,12,1
5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e,Training Trajectories of Language Models Across Scales,M. Xia,2022,12,12
83562af413d730b9321efe8bea24058514ac940b,I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation,Chandra Bhagavatula,2022,12,12
30f60e8a9b2186bf278c9e62299b9d8b3cef44e4,Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?,Shuheng Liu,2022,12,2
3692f4df9d11af68f9b9c9a526667db3f99e552c,Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models,Shibo Wang,2022,,10
34bc28087e1d6f047e2736791f79d769293f447c,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,Hritik Bansal,2022,12,12
f1b475c6bde80880dd8edfc223c1f20c087201b7,Ethical Issues in Automatic Dialogue Generation for Non-Player Characters in Digital Games,Yusuke Mori,2022,,0
c77404122600594ff2f45ec50d3e36a7eacd30a3,Offline Reinforcement Learning for Visual Navigation,Dhruv Shah,2022,12,8
857ba15bba68934b8a23de8c2c3564bb86a59f22,Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation,Maha Elbayad,2022,12,1
7d5175db1b99552491063d2d9581b0b51e1d2932,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",Joshua Albrecht,2022,12,6
8255b71fae79c92d1b3d72caa1e563c80dc36a0b,Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging,Peng Lu,2022,12,2
be157d55b4afd5be9c81619d75aa4897f5e201e4,Elixir: Train a Large Language Model on a Small GPU Cluster,Haichen Huang,2022,12,2
93fdf5cf598aefb0335f001039e83494dc721c3a,General-Purpose In-Context Learning by Meta-Learning Transformers,Louis Kirsch,2022,12,25
dcc062eca9f7665100ac8389258c6975bdde8a27,DP-RAFT: A Differentially Private Recipe for Accelerated Fine-Tuning,Ashwinee Panda,2022,12,8
d2edf22af2239f754ea7fa0e044be254161eee70,Deep Incubation: Training Large Models by Divide-and-Conquering,Z. Ni,2022,12,3
7557105c9aa6a26db4f8e73fabb25e8134013fb5,Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning,Kyuyong Shin,2022,12,2
17a272c5ac1ef01126b2848ae05d0c06162d212b,Pretrained Diffusion Models for Unified Human Motion Synthesis,Jianxin Ma,2022,12,11
a2935aa093b7a8c5a562bf406e0cd673aa8135fa,An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws,Hong Jun Jeon,2022,12,0
7821e7639ffaeea175422f35fae2eb1c095ed1a6,Protein Language Models and Structure Prediction: Connection and Progression,Bozhen Hu,2022,11,15
2cadb8141d86cacde9b4fc5ca72cc82874022eeb,RAMP: A Flat Nanosecond Optical Network and MPI Operations for Distributed Deep Learning Systems,Alessandro Ottino,2022,11,2
9bdad7f737c1303f199384c5b65dc67da7e5c4d8,Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes,Aviral Kumar,2022,11,15
d7a9b3c750c7e5f0c9af3864a796b7fcdc07f030,GPT-Neo for commonsense reasoning-a theoretical and practical lens,Rohan Kashyap,2022,11,2
08c53d66c03b2c00a70fcc2952f90b08cbe94d3b,Receptive Field Refinement for Convolutional Neural Networks Reliably Improves Predictive Performance,Mats L. Richter,2022,11,2
e2df6ae1b3485449364ce2a5356ab09600fc3632,Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism,Xupeng Miao,2022,11,10
4eef626112610d8767d5ded9b288ce0752bdd7aa,Powderworld: A Platform for Understanding Generalization via Rich Task Distributions,Kevin Frans,2022,11,1
62a45ab7b676f3877d41f66f6c9ddf1ec44a1c5f,GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,Max Zvyagin,2022,,11
9f4351600c72d5dac0251cd49984f691dca2fcd1,Word-Level Representation From Bytes For Language Modeling,Chul Lee,2022,11,0
0b2c1f698b8e7b2d1b15945a53e4245833a048d6,Using Focal Loss to Fight Shallow Heuristics: An Empirical Analysis of Modulated Cross-Entropy in Natural Language Inference,Frano Rajic,2022,11,3
be25cd72db91d40d305d9bf870e23a16fda813be,Coreference Resolution through a seq2seq Transition-Based System,Bernd Bohnet,2022,11,3
598d9b235f5ab148fc757240d9bc39a47b8eaf72,Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks,Wenhu Chen,2022,11,135
438e9fece0f177cd5655e490c580ae70c757b09b,Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models,Christian W. F. Mayer,2022,,4
4f4e98cc9133e1814ac2eee9fc4693bf80d1d0d4,Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback,Josh Abramson,2022,11,12
637337f4a84156ea9eee0ca69a4604e5aa43afee,A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach,Yew Ken Chia,2022,11,4
e9f5a58209b34c5c1b1063bd7c7fecea4864334b,Metadata Might Make Language Models Better,K. Beelen,2022,11,0
5e52d654fd31f04c1bd884cd5480e6af8c95ad50,Efficient Transformers with Dynamic Token Pooling,Piotr Nawrot,2022,11,8
7d645a3fd276918374fd9483fd675c28e46506d1,Galactica: A Large Language Model for Science,Ross Taylor,2022,11,221
074f9767bf81f04448ce6d997b7e29dc92b10fe7,General intelligence requires rethinking exploration,Minqi Jiang,2022,11,4
4d17732d90440682b0500f4e209c6cc4fac20e0e,Teaching Algorithmic Reasoning via In-context Learning,Hattie Zhou,2022,11,43
78281482c1fdad8e167bab39cc9955c73d58ae8f,EVA: Exploring the Limits of Masked Visual Representation Learning at Scale,Yuxin Fang,2022,11,126
e51578b433b7cf7cb92febd4fcd8b78035f08ff9,Breadth-First Pipeline Parallelism,J. Lamy-Poirier,2022,11,0
679d0eede253ae27e40ea1fff5eac592ff7a9562,Striving for data-model efficiency: Identifying data externalities on group performance,Esther Rolf,2022,11,1
26c80bd65baa90f5b18157de4951f4eb0b62ab69,InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions,Wenhai Wang,2022,11,110
379e42895f6d40ab9e9559609f505aba89145a5d,Efficiently Scaling Transformer Inference,Reiner Pope,2022,11,45
c90a33f1f0049d524e9b5b3174d35611fd9a8096,Pretraining in Deep Reinforcement Learning: A Survey,Zhihui Xie,2022,11,11
bcec7d17e68aceb91d020dd796ece075694f77c6,COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,Hao Peng,2022,11,10
d2995cbd50528beac1419e5943c8fd6e4c91ddb3,Harmonizing the object recognition strategies of deep neural networks with humans,Thomas Fel,2022,11,22
b364def1f651dc7a5d312c6edd5d79b0de8197bb,"Astronomia ex machina: a history, primer and outlook on neural networks in astronomy",Michael J. Smith,2022,11,3
4610ffb1b016acaa82a2065ffd1a3adbae1ce722,Large Language Models Are Human-Level Prompt Engineers,Yongchao Zhou,2022,11,160
761699845aa27675c39f027462ef6b03658be2da,Fine-Tuning Language Models via Epistemic Neural Networks,Ian Osband,2022,11,2
4809452eb4ed547adaf44a004d47ee910265ba34,Inverse scaling can become U-shaped,Jason Wei,2022,11,26
d17bddf9dc329bb9ff7883642699b84055db06fc,PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation,Siqi Bao,2022,11,4
7df49275071c0ea7b2e7a30df6bd0edd26e6edec,Cloud-Intelligenz und kollektives Lernen für das automatisierte und vernetzte Fahren,B. Lampe,2022,,0
5a5e9bc060c4b6e3aa600b0bea5c5427ab5c94e0,Cloud Intelligence and Collective Learning for Automated and Connected Driving,Bastian Lampe,2022,,1
de1c66454220eeb93f92b66bacad9d4f7dc8b2ab,Developing a GPT-3-Based Automated Victim for Advance Fee Fraud Disruption,J. Hewett,2022,,2
1a6a7fe065e42365515ccf7d0b5d1225b8088464,STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training,Xiaoyang Sun,2022,,3
7ffb3a27a2a4da5c35472bd3a3a4dee8d40a6d86,Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models,Xiaoman Pan,2022,10,11
e6afdf59d67c84ec1902f709b78e87d4aa8ad189,COCO-DR: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning,Yue Yu,2022,10,18
f143659c51896efe0aa59c339e591a7bf4bd5e53,"Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks",Edwin Zhang,2022,10,0
519b4ff448ee48ba7e08d146a29ade6f019a98d4,Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning,Pablo Villalobos,2022,11,26
8a4e2828777c9b3703e8e2b68ac27d9af496261a,"Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models",Hong Liu,2022,10,6
f031ba42cf82f106200bb03fbb91dd5671a59b9c,Practical Program Repair in the Era of Large Pre-trained Language Models,Chun Xia,2022,10,32
12c20b14f0652c2953dee2e4fc7da59e46744ff4,Learning Better Intent Representations for Financial Open Intent Classification,Xianzhi Li,2022,10,2
8b1aa3a20638363be3a6d941e907df704c738dfc,The Robustness Limits of SoTA Vision Models to Natural Variation,Mark Ibrahim,2022,10,5
311fd5f6f114ae51f8cbd95a0da69d7b556d25f1,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,Maarten Sap,2022,10,56
4583738cfaf1fd90f33de6cfb0f4bb2693f54fa6,Characterizing Verbatim Short-Term Memory in Neural Language Models,K. Armeni,2022,10,2
8f309ff0faa96376253722f9a9cbbb0b7a3f1891,Performance-Efficiency Trade-Offs in Adapting Language Models to Text Classification Tasks,Laura Aina,2022,10,0
c10195a628f31315ec56c219e219b25d81bb2b7f,Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale,Ran Tian,2022,10,2
c2a2b801e9091c33d50efd8758d3bcc2b05368ff,Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination,Yue Yang,2022,10,5
1bb6d5761903c7ac978188ae36e2648905e95dc5,Transcending Scaling Laws with 0.1% Extra Compute,Yi Tay,2022,10,31
4bef9d46209ac8988ea5ab83547149760d4af65e,Automatic Document Selection for Efficient Encoder Pretraining,Yukun Feng,2022,10,2
5484d228bfc50efbac6e86677bc2ec2ee4ede1a6,Scaling Instruction-Finetuned Language Models,Hyung Won Chung,2022,10,685
18f7ff46b033abd09a7f3f81237500514cc56556,Measuring the dimensionality of behavior,S. Ganguli,2022,,1
4431a39f677fe59b07b3f0cfde7b10f7208cf46c,N-Best Hypotheses Reranking for Text-to-SQL Systems,Lu Zeng,2022,10,3
5c02d55fe14e2baf4b6b59a476ee6a20698397ef,"Language Models Understand Us, Poorly",Jared Moore,2022,10,2
05bb7f4e44131a1f6a1c3d6b3cefc385a04204cc,A baseline revisited: Pushing the limits of multi-segment models for context-aware translation,Suvodeep Majumde,2022,10,7
61b54e8702a9e204c722653bdd43de56df92ca13,Optimisation & Generalisation in Networks of Neurons,Jeremy Bernstein,2022,10,1
663a41c866d49ce052801fbc88947d39764cad29,Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,Mirac Suzgun,2022,10,158
711d5e8ddbb840ad31a9ffa3d38590603ba69a92,Prompting GPT-3 To Be Reliable,Chenglei Si,2022,10,57
82cd40e926300b6b18c34ced2edeb07e84d9d6c7,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,Yuxian Gu,2022,10,8
e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9,LAION-5B: An open large-scale dataset for training next generation image-text models,Christoph Schuhmann,2022,10,686
0cb346ccbd338517789db9abafe9fd93549f7893,A Simple and Strong Baseline for End-to-End Neural RST-style Discourse Parsing,Naoki Kobayashi,2022,10,4
e02dce6ee032a13b1653f69b034a35676e7d4dc2,Can language representation models think in bets?,Zhi–Bin Tang,2022,10,4
30d0a5a5f8e776c844a37afe3b1ace0b21b24859,Retrospectives on the Embodied AI Workshop,Matt Deitke,2022,10,25
84e693cfde91eb7acf598b223313365c8dd89485,Spontaneous Emerging Preference in Two-tower Language Model,Zhengqi He,2022,10,1
9166caa474031b62bacad8a920db8308e6a15120,An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification,Ilias Chalkidis,2022,10,11
e086d318769c3be0173b572d5e070c226bfb3443,ASRS-CMFS vs. RoBERTa: Comparing Two Pre-Trained Language Models to Predict Anomalies in Aviation Occurrence Reports with a Low Volume of In-Domain Data Available,Samuel Kierszbaum,2022,,4
229efae9f027e39249ca013eba5580dc620e953d,FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings,Jean Ogier du Terrail,2022,10,36
6dd4743ee5430157c981a8dfe9a7434d99be2e8b,Understanding HTML with Large Language Models,Izzeddin Gur,2022,10,17
f158e70e9ead719e8a524eaf8ec79270574f2eda,How Large Language Models are Transforming Machine-Paraphrase Plagiarism,Jan Philip Wahle,2022,10,6
bf0d43bf6aac1eed8abe56f0b26d902a1f259fc0,Automatic Discovery of Composite SPMD Partitioning Strategies in PartIR,Sami Alabed,2022,10,0
3de645f0c1993cd3f3374ad747640a1aa6658a82,Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?,Mansheej Paul,2022,10,12
6a0321b05af7ed549309a05be5f9e335396d3a3d,Generalization Properties of Retrieval-based Models,S. Basu,2022,10,3
6d5555348f453bac901c5b57e8a4eeb3074b4071,Learning to Reason With Relational Abstractions,A. Nam,2022,10,2
fb49e88c6bd676516898e911e42b4f8479e6f1bf,Ask Me Anything: A simple strategy for prompting language models,Simran Arora,2022,10,60
257457233bd12a76096f1023935f08b6b0a14465,Privacy-Preserving Text Classification on BERT Embeddings with Homomorphic Encryption,Garam Lee,2022,10,5
9be68112ba98a733fb57f93e753cb3e0fd6f63d9,Fine-Tuning with Differential Privacy Necessitates an Additional Hyperparameter Search,Yannis Cattan,2022,10,13
36a4d5977b6eeb5656f7a8c185159c050411c958,Antibody Representation Learning for Drug Discovery,Lin Li,2022,10,5
55e3fe05598be7c3dd357d51166869f6571b824f,Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,Mohammad Reza Taesiri,2022,10,1
49b8782d0e715f50c3cd9718180abd2e43e80b7e,Memory in humans and deep language models: Linking hypotheses for model augmentation,Omri Raccah,2022,10,0
c88cafa3e980765a64febe369ceb7c2aa7261d2a,Complexity-Based Prompting for Multi-Step Reasoning,Yao Fu,2022,10,101
7698498dcb14db063154f4c955fc041114d1960d,Single-sequence protein structure prediction using a language model and deep learning,Ratul Chowdhury,2022,,102
461630c5126fb457b2396ed5238fff0977e94b5a,DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability,Cian Eastwood,2022,10,5
20b1b783d500d65505c87d7b72b84f2c2ae2ecf9,A Combinatorial Perspective on the Optimization of Shallow ReLU Networks,Michael Matena,2022,10,0
891edceb78a274b0c2494d8176bc4d6f6e3f9cbc,Calibrating Sequence likelihood Improves Conditional Language Generation,Yao Zhao,2022,10,19
8fbd7ddf1ea30c991f3b1152a245df77caa18e16,Learning by Distilling Context,Charles Burton Snell,2022,09,20
13d78bde4dc7059ab941871048ffa91d556584c8,Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods,Skanda Koppula,2022,09,5
b65b7f480a61d3dd31d8117b349cabc87c8ccf6c,Bidirectional Language Models Are Also Few-shot Learners,Ajay Patel,2022,09,14
34e1c62586f0c86af60a6ff2c3e1121c1ebd779a,Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging,Jean Kaddour,2022,09,14
8f61a198bf9f97e445267f8b48d459e4bac9a14c,A Multi-Agent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems,Andrea Gesmundo,2022,09,2
b6408c7dc8ce8c386197990d90ccb528419db25b,Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning,Manas Gupta,2022,09,2
d42c50b154dbc41cf869c702f9e4c40fd0cea4da,OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning,Youhe Jiang,2022,05,5
f67db2aa37b2093fe40c1bb48fbe3626e565c2f4,SpeedLimit: Neural Architecture Search for Quantized Transformer Models,Yuji Chai,2022,09,0
bf72cfa87a5c55e430abf6d2a3d9b66eb9e1a717,Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity,Gabriel Simmons,2022,09,9
25b6c9a11d9078d2cc45e02e284195320ce61f0f,Variational Open-Domain Question Answering,Valentin Li'evin,2022,10,2
e194f641c554cb00cdd4bd6993f14c2dff8c3c03,Efficient Few-Shot Learning Without Prompts,Lewis Tunstall,2022,09,61
b2542a738b75ee9b7ce1a13d8b78f9095d212412,Generate rather than Retrieve: Large Language Models are Strong Context Generators,W. Yu,2022,09,76
205eab69e430b4da93ecf3fd9115f0919b448040,WeLM: A Well-Read Pre-trained Language Model for Chinese,Hui Su,2022,09,6
cf49af82a5f1ba5f2beba9f290e684b7b51b64e6,Extremely Simple Activation Shaping for Out-of-Distribution Detection,Andrija Djurisic,2022,09,17
cbeb03802b00024bf3d43b63814241e5261abe93,Relaxed Attention for Transformer Models,Timo Lohrenz,2022,09,2
33fd110d1e4ca5f91d1b7ca7ff24ce1e9335359e,Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics,Shoaib Ahmed Siddiqui,2022,09,14
818861b82fae7e41a6e4d599c63ecc9a18822a7d,"Can we do that simpler? Simple, Efficient, High-Quality Evaluation Metrics for NLG",Jens Grunwald,2022,09,1
f5d73457d05d4b4da3905fde99d104e4b744a032,Textual Datasets For Portuguese-Brazilian Language Models,Matheus Ferraroni Sanches,2022,,1
ddf0de2a4e0f707446c7463ea63cfdc08d59d4ca,Rapid classification of local seismic events using machine learning,Luozhao Jia,2022,,0
ed9b3de9d3b99bcd004f853c3696dd4916bb794d,Current sequence-based models capture gene expression determinants in promoters but mostly ignore distal enhancers,Alexander Karollus,2022,,20
5860267c5b1761fa5ff8dbcde980edff4c3b2f2a,Part-Based Models Improve Adversarial Robustness,Chawin Sitawarin,2022,09,5
ceece2488eef8fa427cf4fee60e6cfb1155ff15e,A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems,Andrea Gesmundo,2022,09,6
28630034bb29760df01ab033b743e30b37f336ae,PaLI: A Jointly-Scaled Multilingual Language-Image Model,Xi Chen,2022,09,204
0d4af5dd9b1212c9f6a78b18935fc1c1187c2d73,Language Chameleon: Transformation analysis between languages using Cross-lingual Post-training based on Pre-trained language models,Suhyune Son,2022,09,0
f71778a1cb584cb1272c292a152d4bcd0a8ae6e3,PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization,S. Fadnavis,2022,09,0
9a5df5b7f2b42ffccb6b52c20547850cadfa0fe2,Enabling Connectivity for Automated Mobility: A Novel MQTT-based Interface Evaluated in a 5G Case Study on Edge-Cloud Lidar Object Detection,Lennart Reiher,2022,09,2
0b5f3259d393e0e25469c92d7fa44f8cec35d170,Mimose: An Input-Aware Checkpointing Planner for Efficient Training on GPU,Jian-He Liao,2022,09,3
0afc96eca8b94d19a4c98fdd78e5fd9c68f6859a,Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision,Lei Zhang,2022,09,2
6e02a7eedad079451b9a8dd358268727cf599c6e,Do Large Language Models know what humans know?,Sean Trott,2022,09,15
ca086f4c09cf8de705830ac2b70951737fab93ca,A Review of Sparse Expert Models in Deep Learning,W. Fedus,2022,09,41
1ad7a323cebbd186a6f3c9a1f0caf0af94ce91bd,HammingMesh: A Network Topology for Large-Scale Deep Learning,T. Hoefler,2022,09,6
dbeee6ac35ed73eb6ffb16c6201502369b6ae42b,Masked Sinogram Model with Transformer for ill-Posed Computed Tomography Reconstruction: a Preliminary Study,Zhengchun Liu,2022,09,0
6c673f92ac808d9ccbaf3fc35e8da9cd66caf847,Petals: Collaborative Inference and Fine-tuning of Large Models,Alexander Borzunov,2022,09,13
c2efc25fa1a2b0192b95dc92d9dccf90e602c74c,Efficient Methods for Natural Language Processing: A Survey,Marcos Vinícius Treviso,2022,09,30
18a831422a4b4c89bbf1cc4baaa2cfcbf29daaf1,Exploring and evaluating personalized models for code generation,Andrei Zlotchevski,2022,08,1
966ccb741d4e8d92db931852f2d9480fb5c497a0,Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment,Mustafa Shukor,2022,08,12
e66384bfc81d0c9357ce5e692a1b42a30bbe48e1,Adaptive and Efficient GPU Time Sharing for Hyperparameter Tuning in Cloud,L. Liu,2022,,1
cdbfd6201296db1f7e82421c0de34f12fd5294e9,On Reality and the Limits of Language Data: Aligning LLMs with Human Norms,Nigel Collier,2022,08,1
74d700b7df1f3fff7a145e47250310035d845c4d,Enhancements to Neural Language Model for Generating System Configuration Code: A Study with Maven Dependency,Chen Yang,2022,,0
acba7ea5905901944027b1bab14d5c6059edf062,Federated Select: A Primitive for Communication- and Memory-Efficient Federated Learning,Zachary B. Charles,2022,08,7
608547338e096bfdf1db170ada341c28f1b7462a,Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim Verification with Pattern Exploiting Training,Xia Zeng,2022,08,2
69e9fcc1b85732d822cc0f02fb2942c0fdd74dba,Self-Supervised Learning for Scene Classification in Remote Sensing: Current State of the Art and Perspectives,P. Berg,2022,,11
e221c769957a8eef2836a64cede88401129d86e9,Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP,Thao Nguyen,2022,08,30
2d3b6058370b804009fd7866098f4fca2d1894ca,Reducing Retraining by Recycling Parameter-Efficient Prompts,Brian Lester,2022,08,5
3b39efe6c91ae432dd35bb79431edb8a6719f906,Investigating Efficiently Extending Transformers for Long Input Summarization,Jason Phang,2022,08,20
ac774b892a67fd0eccdf92e94e2bcb1402cc037a,Now What Sequence? Pre-trained Ensembles for Bayesian Optimization of Protein Sequences,Ziyue Yang,2022,,6
398e4061dde8f5c80606869cebfa2031de7b5b74,Few-shot Learning with Retrieval Augmented Language Models,Gautier Izacard,2022,08,177
2a672342035defd8d75b54e08597ef124c6a0172,Fusing Sentence Embeddings Into LSTM-based Autoregressive Language Models,Vilém Zouhar,2022,08,1
05816d1476719e04084ee03f6a1bbfb68cdfa8e6,Joint Data Collection and Resource Allocation for Distributed Machine Learning at the Edge,Min Chen,2022,,4
076deb54a5d776cd21eabf2c40cdd839f53d6d77,giMLPs: Gate with Inhibition Mechanism in MLPs,C. Kang,2022,08,0
ec4c8d99eb1c028c43af6d8bbf727392d351cb59,Efficient Training of Language Models to Fill in the Middle,Mohammad Bavarian,2022,07,58
2c709ef6186bd607494a3344c903552ea500e449,Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks,Tilman Raukur,2022,07,38
0d975a8bbc1e0495cb95df8666d42111b546ab34,Retrieval-Augmented Transformer for Image Captioning,Sara Sarto,2022,07,20
39638f73ecd4080592d9fd9270fd7aa25df02f63,Dive into Big Model Training,Qinghua Liu,2022,07,1
cf25fc0d063eb653de708267e95e0f1d6f5ef4e0,Training philosopher engineers for better AI,Brian Ball,2022,,0
0369ea9a84d230007ccae2d772ee7b1e3d0f7c36,Comparative Study of Repertoire Classification Methods Reveals Data Efficiency of k -mer Feature Extraction,Yotaro Katayama,2022,,3
0de905634bd3d42a612ffab7ddd7a814e7e655bb,Benchmarking Transformers-based models on French Spoken Language Understanding tasks,Oralie Cattan,2022,07,1
3215fb4ca5ffa36333245c35b133d452824e7837,Towards Learning Causal Representations of Technical Word Embeddings for Smart Troubleshooting,A. Trilla,2022,,0
d697b440dd0e65a05fe027e4c0ea85f62dcba033,Can large language models reason about medical questions?,Valentin Li'evin,2022,07,81
00bd301992aaf5b27beb1f6a4edded32a3c635e8,Generative Models of Brain Dynamics,Mahta Ramezanian-Panahi,2022,,9
1d91bc3979e87380c31ac8aa152919228a639a04,Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,Takanori Ashihara,2022,07,11
8abc6b3246ea821efa862735359313bd220f39e8,u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,Wei-Ning Hsu,2022,07,11
44f6612c238297304331d6fe6aa4b4f909f1c6f0,Wayformer: Motion Forecasting via Simple & Efficient Attention Networks,Nigamaa Nayakanti,2022,07,52
f75425ef1884a7bc8d367788aef111b242cd540b,Embedding Recycling for Language Models,Jon Saad-Falcon,2022,07,3
5607f81dd171defdb8e76506fd3ea8869d92e415,Where is the mind within the brain? Transient selection of subnetworks by metabotropic receptors and G protein-gated ion channels,D. Nikoli'c,2022,07,2
5b984766416a4c881f7acdfcd3ddf983b6ff9333,"Synergy and Symmetry in Deep Learning: Interactions between the Data, Model, and Inference Algorithm",Lechao Xiao,2022,07,7
605ce1aa8886eecb53bdb29fca60f1fe5ff7fd8c,Mechanisms that Incentivize Data Sharing in Federated Learning,Sai Praneeth Karimireddy,2022,07,15
cb6ed5fe96f6c04ddaace1d141226091a8192601,Training Transformers Together,Alexander Borzunov,2022,07,4
c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617,Neural Networks and the Chomsky Hierarchy,Gr'egoire Del'etang,2022,07,35
041edc8b14bdd0e5627377956fd0e6c6c011146a,Machine Learning Model Sizes and the Parameter Gap,Pablo Villalobos,2022,07,9
4ce6d229d5f44239c948fd56ad744c012aae22e0,Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,Joel Dapello,2022,,10
14efaa989b8db89ad907e6a15253d482cbba77d9,Linguistically inspired roadmap for building biologically reliable protein language models,Mai Ha Vu,2022,07,8
051c2cfb399d71dfc91459cad5be404ee4568f9d,When Does Differentially Private Learning Not Suffer in High Dimensions?,Xuechen Li,2022,07,26
70e8b89f4dde78113a1c3f3a2359a6af702e38f0,Evaluation Methods for Representation Learning: A Survey,Kento Nozawa,2022,,2
2a1493eee51f07be435dd02d0de9ca48a57fa79f,Knowledge Distillation of Transformer-based Language Models Revisited,Chengqiang Lu,2022,06,3
8dd49f94b213834e580eeea18098f402b1be1226,Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers,Minjia Zhang,2022,,7
26133033149afb4b45e5d0a4bd1dc712a236810e,ProGen2: Exploring the Boundaries of Protein Language Models,Erik Nijkamp,2022,06,67
b9c004a61ba7db8e4e576b95048c16bbb61b103d,Bayesian Optimization Over Iterative Learners with Structured Responses: A Budget-aware Planning Approach,Syrine Belakaria,2022,06,1
65fc1f1c567801fee3788974e753cdbf934f07e9,Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos,Bowen Baker,2022,06,97
374bbb716aa007a65ac03f0220d3027fa724874d,AI Challenges for Society and Ethics,Jess Whittlestone,2022,06,3
5eeb828685e44ca5b8ebafb34a9fa4d51c9186df,LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models,Gunho Park,2022,06,8
dac3a172b504f4e33c029655e9befb3386e5f63a,Emergent Abilities of Large Language Models,Jason Wei,2022,06,751
daa3af99c6421a60e4dc06cb27fc97a60a1aa54b,Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems,Jack G. M. FitzGerald,2022,06,13
9b155c244ed21edc44594ffd97b4e06fc1d8908b,Etude comparative de modèles Transformers en compréhension de la parole en Français,Oralie Cattan,2022,,0
0edbcf40a5d8f001b1542c4adfaf3303c7216cb9,Mediators: Conversational Agents Explaining NLP Model Behavior,Nils Feldhus,2022,06,9
29acc890e521f7a6415666ab9eb3432c49b4587a,Self-critiquing models for assisting human evaluators,W. Saunders,2022,06,68
8409243143193a7533f1a6cef05181b1d1849d20,Ancestor-to-Creole Transfer is Not a Walk in the Park,Heather Lent,2022,06,3
2e700ff36108119f5ed19a53bd2eaa22b42ec3d8,Tutel: Adaptive Mixture-of-Experts at Scale,Changho Hwang,2022,06,21
59f91478c1f63b0fb5628bbd1af8267792708443,Intra-agent speech permits zero-shot task acquisition,Chen Yan,2022,06,8
65a4723b9980d99b1fdd1e03eba69953cd6af958,Graph Attention Neural Network Distributed Model Training,Armin Esmaeilzadeh,2022,,0
7574ac431d9b1f3ef5996746817a4ae7cad94ad3,Learning to Ask Like a Physician,Eric P. Lehman,2022,06,7
7ffb212356df9980347b3d3b9910dfba75a5d0c7,"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code",Patrick Bareiss,2022,06,31
511e6559df79b5b7cc3fa69ae31ef1c3badce048,When does return-conditioned supervised learning work for offline reinforcement learning?,David Brandfonbrener,2022,06,27
d852c5928774e110c8cb700fa2e457904eb4b0fe,Dynaformer: A Deep Learning Model for Ageing-aware Battery Discharge Prediction,L. Biggio,2022,06,6
9431759eb9845eab9731febf802754b3d4c6d381,Does Federated Dropout actually work?,Gary Cheng,2022,,8
1be5064847c7a5e9a7e516e4c21aa8f4f8ab2eb5,Multi-Game Decision Transformers,Kuang-Huei Lee,2022,05,89
1ab3a007cf4819cc740a21a3140cb0ef3d7c132c,Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression,Lechao Xiao,2022,05,9
962fad98095f5a5e34ba5354557337ec8637a10d,A Blessing of Dimensionality in Membership Inference through Regularization,Jasper Tan,2022,05,5
5c4f8de98525eebd762773093d149ba459cef290,Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation,Yixuan Wei,2022,05,77
ea9c21c66f970a6bcc6750fc7d6ff6fa3c7fd301,Can Foundation Models Help Us Achieve Perfect Secrecy?,Simran Arora,2022,05,3
832aa77be74d7d3a8b33b9abc0f48b8b7babac12,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,Pascal Notin,2022,05,60
c9866e126cb70229e2017087f84b5db446f25a20,An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems,Andrea Gesmundo,2022,05,15
4a4581003f56e8cb581ad6f383c037964765d3d5,Active Programming by Example with a Natural Language Prior,Ruiqi Zhong,2022,05,2
011095a0082e5e301f9bf30267b193c1c9e7e370,Perturbation Augmentation for Fairer NLP,Rebecca Qian,2022,05,31
744e68099831f743aec5e0224a79fec784a61ff1,Semi-Parametric Inducing Point Networks and Neural Processes,R. Rastogi,2022,05,0
7618f17179bb316002cb6cc472d61382776af6b7,Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT,J. Lee-Thorp,2022,05,3
9ebd20a621ea9f458d4919c8e247932ddc7664fd,The Diminishing Returns of Masked Language Models to Science,Zhi Hong,2022,05,4
bc981df87edaae95104007f48872b30d16d07d60,CNNs Avoid the Curse of Dimensionality by Learning on Patches,Vamshi C. Madala,2022,05,2
4b516216d7d150a081fd74993bddf36b6b22c118,Chain of Thought Imitation with Procedure Cloning,Mengjiao Yang,2022,05,8
a005a6160efa2fd055581b8222b41f71f966ea50,Life after BERT: What do Other Muppets Understand about Language?,Vladislav Lialin,2022,05,3
58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b,Can Foundation Models Wrangle Your Data?,A. Narayan,2022,05,46
ffabceeeab1e6298b3633ce5a0117fe4f4fd0270,SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System,Liang Shen,2022,05,8
50090a9c51e276e8c6331e508f161a2516224f65,Single-shot optical neural network,Liane Bernstein,2022,05,11
4853183081cb85284859ff0cc97273116a57a345,The games we play: critical complexity improves machine learning,A. Birhane,2022,05,5
5922f437512158970c417f4413bface021df5f78,A Generalist Agent,S. Reed,2022,05,361
7cdaa08890895e1ad92afb5fad429690ad7b1dac,Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning,Haokun Liu,2022,05,175
6c790c1a4233f5201b052b149a3c6c76261aee68,RITA: a Study on Scaling Up Generative Protein Sequence Models,Daniel Hesslow,2022,05,32
b21670e8061a06ab97e7d6052c9345a326e84ff8,UL2: Unifying Language Learning Paradigms,Yi Tay,2022,05,93
4ce22cb5747db637e242e2dc74536a13b185e4aa,Optimizing Mixture of Experts using Dynamic Recompilations,Ferdinand Kossmann,2022,05,2
c7d7113e0d7d306953fbda7409a21e5948ee6fbf,An Exploration of Consistency Learning with Data Augmentation,Connor Shorten,2022,,1
90c7ea8273612b2fa12a294f171863e30d879505,Adaptable Adapters,N. Moosavi,2022,05,9
7ab0d76a2a0ba7491f06a6cd2d264b8f50037305,Jack and Masters of All Trades: One-Pass Learning of a Set of Model Sets from Foundation Models,Han Xiang Choong,2022,05,2
561c74d1b27efc2fbfccfae6985ae70e97c68687,Triangular Dropout: Variable Network Width without Retraining,Edward W. Staley,2022,05,1
e47da75675b9a3fe02ef1efadca39bc8cdfcdc17,Designing Effective Sparse Expert Models,Barret Zoph,2022,,54
ee18ec08a0cdfd23e8cb85bb9c2421b88453e314,Artificial Intelligence Based on Machine Learning in Pharmacovigilance: A Scoping Review,Benjamin Kompa,2022,,14
6a684417089eb6f07ccb1ac5391c63ebacef821d,Can OpenAI's Codex Fix Bugs?: An evaluation on QuixBugs,Julian Aron Prenner,2022,,38
ba377c911b1d601cbe6a7f10f89e5b6175adcd98,TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting,Xiaonan Nie,2022,,10
5cbdaa4a011044fa9d341f8b743d301dcc84ceec,Continual Learning with Foundation Models: An Empirical Study of Latent Replay,O. Ostapenko,2022,05,16
26218bdcc3945c7edae7aa2adbfba4cd820a2df3,Flamingo: a Visual Language Model for Few-Shot Learning,Jean-Baptiste Alayrac,2022,04,883
1fafaccebc4a74898a74c606f846318c4c2c7536,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,Seongjin Shin,2022,04,35
9f96d8b71551e735171283976ce2160e0b88e824,Can deep learning match the efficiency of human visual long-term memory in storing object details?,Emin Orhan,2022,04,0
3b9b1aba877ecd3f7e508cbc78a41b623349902b,Translation between Molecules and Natural Language,Carl N. Edwards,2022,04,39
0221d3f45899e226ba7840ca9b19117de3a394bc,Semi-Parametric Neural Image Synthesis,A. Blattmann,2022,04,15
e33dbc93f454b856100332b2f5427e027b356bc4,DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation,Cheonbok Park,2022,04,2
0f071ec7a34b25d60b5f122e5e755213771bf7f9,EHR foundation models improve robustness in the presence of temporal distribution shift,L. Guo,2022,,9
b6ec1e8f18185b4b3d46201359a440404575460c,METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals,Payal Bajaj,2022,04,21
408efdd599b2b27ecb95a4d799869c9ff568fb31,ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension,Sanjay Subramanian,2022,04,44
c69f9a5185b4c29525bedb2dcc79d20b42c14cc6,TRUE: Re-evaluating Factual Consistency Evaluation,Or Honovich,2022,04,66
094ff971d6a8b8ff870946c9b3ce5aa173617bfb,PaLM: Scaling Language Modeling with Pathways,Aakanksha Chowdhery,2022,04,2051
b83eb34b088b31118974f33109ce53a32209d73a,"Theory of Mind and Preference Learning at the Interface of Cognitive Science, Neuroscience, and AI: A Review",C. Langley,2022,,12
341bdbcfc3febef7691a97c216ad394653211095,Can language models learn from explanations in context?,Andrew Kyle Lampinen,2022,04,129
8326dba15f6b8ee6e43c23eea3265a05e59e8135,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,Tri Dao,2022,04,35
02be347cd94a5c3596003d98ccaf66831b048df5,Scaling Language Model Size in Cross-Device Federated Learning,Jae Hun Ro,2022,04,11
8342b592fe238f3d230e4959b06fd10153c45db1,Training Compute-Optimal Large Language Models,Jordan Hoffmann,2022,03,685
54b8bc5be8bbffae333dd73f2cb9d93a492d438e,HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System,Xiaonan Nie,2022,03,11
43fee02f5e63dcc85c58d74966951e89b2331489,"Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking",Iñigo Urteaga,2022,03,0
2c6230fd6c474790d3a3199b8491bef0d54f8fd3,WOODS: Benchmarks for Out-of-Distribution Generalization in Time Series Tasks,Jean-Christophe Gagnon-Audet,2022,03,14
3cc1ba2e3cd14857f843d9c3b955fa1fb333527a,EVA2.0: Investigating Open-domain Chinese Dialogue Systems with Large-scale Pre-training,Yuxian Gu,2022,03,34
03013e291fb3192b286147f5bdb5770e434f91b2,Do Language Models Plagiarize?,Jooyoung Lee,2022,03,16
3938e017cf711cec6f91d072c5f7a0795c3f11c0,Prospectively validated disease-agnostic predictive medicine with hybrid AI,B. Lovetrue,2022,,0
daade6d1d7aa018e42d3acabff1e83380d5d0f07,Does Corpus Quality Really Matter for Low-Resource Languages?,Mikel Artetxe,2022,03,9
86febec5ac647fe4e5aab1284f913684ccf70845,"Improving Macroeconomic Model Validity and Forecasting Performance with Pooled Country Data using Structural, Reduced Form, and Neural Network Model",Cameron Fen,2022,03,0
3330175709ceb69cb2358053e72b9afc49723657,Towards Personalized Intelligence at Scale,Yiping Kang,2022,03,0
bc7984bfcfae537dbe633eeeb8d69c42a994c724,ELLE: Efficient Lifelong Pre-training for Emerging Data,Yujia Qin,2022,03,29
1098ca3dbda5778c2bf6c9e8cbb9bc7a02249e10,Staged Training for Transformer Language Models,Sheng Shen,2022,03,12
ad9effaa6c12330f24e34bd3fa5d1496acc113af,More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize,Alexander Wei,2022,03,26
436278ce3b85265dc5cface29e63c714fe979d23,LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models,Mojan Javaheripi,2022,03,2
7fad3f050e8a3fdb00213c919b433b904fff8dfe,Distributed Methods with Absolute Compression and Error Compensation,Marina Danilova,2022,03,3
024dbd9cf7f9c605cc3a99b35b578ce24993d32c,Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale,Laurent Sartran,2022,03,24
68de81265a2ef91f838b00a9bd39e2ab53cb63fb,COMPASS: Contrastive Multimodal Pretraining for Autonomous Systems,Shuang Ma,2022,03,6
a86d04a60bfdd196f217b2f4a01ef08a8c83896d,DataMUX: Data Multiplexing for Neural Networks,Vishvak Murahari,2022,02,5
6bcdf260d7927d8fe4ff030c20ee1db974d0c969,Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation?,Beren Millidge,2022,02,17
bbc57e1b3cf90e09b64377f13de455793bc81ad5,Mixture-of-Experts with Expert Choice Routing,Yan-Quan Zhou,2022,02,48
a45582e75f326fdb668e164d6d06a365e4bcc6b5,Geometric Regularization from Overparameterization,Nicholas J. Teague,2022,02,0
1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3,ST-MoE: Designing Stable and Transferable Sparse Expert Models,Barret Zoph,2022,02,32
92f7056e93c4a78eeaaa6c644ebd1edacdff1ee4,MUCE: a multilingual use case model extractor using GPT-3,Deepali Bajaj,2022,,4
e8f170e3eee1fce6c9e9d6ca7b7f56f97a40e02b,CAREER: Transfer Learning for Economic Prediction of Labor Sequence Data,Keyon Vafa,2022,02,0
44d6c201a4056e260e9844bdbb01461ea9b1a011,A data-driven approach for learning to control computers,P. Humphreys,2022,02,25
802a5d24c78f713e282b003d99b4afd924bd7568,A Survey on Dynamic Neural Networks for Natural Language Processing,Canwen Xu,2022,02,13
ebd042387a2dde05ab18b23820030841bb671966,Lie Point Symmetry Data Augmentation for Neural PDE Solvers,Johannes Brandstetter,2022,02,21
9d40837175577bb0009b138269b422f6d5820d00,Transformer Memory as a Differentiable Search Index,Yi Tay,2022,02,90
a8d671773daa3bf07329ef76f0be110d6b7a646d,Pruning Adapters with Lottery Ticket,Jiarun Wu,2022,,1
62d17b6f6ad77fd71ef9954c7784700d5e316f1f,What Does it Mean for a Language Model to Preserve Privacy?,Hannah Brown,2022,02,56
e468f74ebffa8bbdd99bf8d0233822a1d2a9b430,Coarsening the Granularity: Towards Structurally Sparse Lottery Tickets,Tianlong Chen,2022,02,20
5cbe278b65a81602a864184bbca37de91448a5f5,Competition-level code generation with AlphaCode,Yujia Li,2022,03,442
627fa6970101e2b532745dc9e11cf1850f9fc205,Adaptive Fine-Tuning of Transformer-Based Language Models for Named Entity Recognition,F. Stollenwerk,2022,02,0
916a06a6d51aa93de27aac2f3e14faed08dd6706,Formal Mathematics Statement Curriculum Learning,Stanislas Polu,2022,02,38
f74b387136d1d57a791c6b7c9d823577a47516aa,Harmony: Overcoming the hurdles of GPU memory capacity to train massive DNN models on commodity servers,Youjie Li,2022,02,5
55ad5e818cfed72317576027fb33a9609210d592,Training and Evaluating a Jupyter Notebook Data Science Assistant,Shubham Chandel,2022,01,18
24efea3318ec208063ede7f8422696743b2f924d,ScaLA: Accelerating Adaptation of Pre-Trained Transformer-Based Language Models via Efficient Large-Batch Adversarial Noise,Minjia Zhang,2022,01,0
1b6e810ce0afd0dd093f789d2b2742d047e316d5,Chain of Thought Prompting Elicits Reasoning in Large Language Models,Jason Wei,2022,01,1849
aff09088b8953a35d20938556eb9070e4d693d39,A Large and Diverse Arabic Corpus for Language Modeling,Abbas Raza Ali,2022,01,0
b3848d32f7294ec708627897833c4097eb4d8778,LaMDA: Language Models for Dialog Applications,Romal Thoppilan,2022,01,770
c783e1fb3ce8514f981925ee590c00884660ee4e,CM3: A Causal Masked Multimodal Model of the Internet,Armen Aghajanyan,2022,01,86
7d1e512888a2fa4e838c12a02ae7fce867d322a8,DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,Samyam Rajbhandari,2022,01,81
0c181f508ec9de8e48f62523ba8a9bcb1f51f83a,Pretrained Language Models for Text Generation: A Survey,Junyi Li,2022,01,0
10b9ca173c665e3f2c322c2d5ce9b9d433fe4629,The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,Alexander Pan,2022,01,38
400d619cbabeb669115bb7281a889ab869829ef5,MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound,Rowan Zellers,2022,01,100
6dabb2efbe33ef7f7bfa39be57496a0a7e80568e,Complexity from Adaptive-Symmetries Breaking: Global Minima in the Statistical Mechanics of Deep Neural Networks,Shaun Li,2022,01,0
cf4f4b69b76dc58dc8c0d443ab88ceb461eec719,EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate,Xiaonan Nie,2021,12,6
b6270cc18c5a992c30275364b3ebf2f5f6643d12,Generative Models of Brain Dynamics -- A review,Mahta Ramezanian Panahi,2021,12,4
42eddd30bad406ffbd7472bbc467e1446c65b3d6,"Toward a Computational Neuroethology of Vocal Communication: From Bioacoustics to Neurophysiology, Emerging Tools and Future Directions",Tim Sainburg,2021,,9
c0a98fca99e5f2d34330a15ab8ad4b9d692146d0,The growing cost of deep learning for source code,V. Hellendoorn,2021,,16
0b483b550b21ec42d693fc04a372dbb10dd07019,Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge,Ian Porada,2021,12,3
4c70bdfe7bbdf4899a50e25f6fdaff0d7ee97ac1,Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer,Yanpeng Zhao,2021,12,13
5b0cfef3ebb8f709a4cf2e4718e8440cc5890bab,Efficient Hierarchical Domain Adaptation for Pretrained Language Models,Alexandra Chronopoulou,2021,12,25
5ebc677cb284606192b827ffa7a8578542896686,Can Multilinguality benefit Non-autoregressive Machine Translation?,Sweta Agrawal,2021,12,1
3dfb1f50f2a34a699c339dabaa6f9b3a977973de,LongT5: Efficient Text-To-Text Transformer for Long Sequences,Mandy Guo,2021,12,119
32881086e4f0d97a81b0f9c89b59384040b1d02d,LMTurk: Few-Shot Learners as Crowdsourcing Workers,Mengjie Zhao,2021,12,8
503fc0edd883e8ddaa6c542eb47c45669c276864,Epigenomic language models powered by Cerebras,Meredith V. Trotter,2021,12,5
80d3f14c95a1c187b5ba17b3a326a611874a3996,Computational bioacoustics with deep learning: a review and roadmap,D. Stowell,2021,12,83
f57bdceddfffd82951f7942f8f936bf8c3ead446,Eigenspace Restructuring: a Principle of Space and Frequency in Neural Networks,Lechao Xiao,2021,12,16
002c256d30d6be4b23d365a8de8ae0e67e4c9641,Improving language models by retrieving from trillions of tokens,Sebastian Borgeaud,2021,12,359
5a980aa843e40de5f91a243cbf680af273c797ba,Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning,DeepMind Interactive Agents Team Josh Abramson,2021,12,35
64c8350a66ada14ecc0baa3446c75884cc27e3dd,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,Yichong Xu,2021,12,39
ee042a3e299a32c413532e64603de8d3ddb6aa87,Automap: Towards Ergonomic Automated Parallelism for ML Models,Michael Schaarschmidt,2021,12,9
e5e8d196e1b857bd1b23ace648e695e41a45863b,Co-domain Symmetry for Complex-Valued Deep Learning,Utkarsh Singhal,2021,12,7
82edbb92d0f6952224c5aa0aff264b44b8fb4e98,Toward Foundation Models for Earth Monitoring: Proposal for a Climate Change Benchmark,Alexandre Lacoste,2021,12,13
90b21dbad8969b74d704eed15a3d98722a88e464,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,Beidi Chen,2021,12,39
ce608300992225fd1dba6229adbee51341dd2332,Emotions as Abstract Evaluation Criteria in Biological and Artificial Intelligences,C. Gros,2021,11,0
da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed,Sparse is Enough in Scaling Transformers,Sebastian Jaszczur,2021,11,48
cbf98ebe967e0f3f3236e7932f37013b98244e94,ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning,V. Aribandi,2021,11,137
04db9b694280134f09af5fa787a306907edba29d,How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,R. Thomas McCoy,2021,11,40
06b20a1c6883464fcb2855adc146874fe7937c41,Merging Models with Fisher-Weighted Averaging,Michael Matena,2021,11,67
be0fbb810583930c071d0b9b2c5187fe260783f5,Swin Transformer V2: Scaling Up Capacity and Resolution,Ze Liu,2021,11,593
9a258f42e333ed5ff79037724eb01747ede0bb49,Few-Shot Self-Rationalization with Natural Language Prompts,Ana Marasović,2021,11,54
7567744a0e23174166575e8d98590967684696b4,Scaling Law for Recommendation Models: Towards General-purpose User Representations,Kyuyong Shin,2021,11,16
21e76679cc64c119347997f21c70e33b540d9d3e,Tensor network to learn the wavefunction of data,A. Dymarsky,2021,11,5
9e4f5629355d2fe5febae6ff78d239cff053ea02,Information geometry for multiparameter models: new perspectives on the origin of simplicity,Katherine N. Quinn,2021,11,16
6c15605b4b77f970975757a875d349ba240f4caf,Scaling ASR Improves Zero and Few Shot Learning,Alex Xiao,2021,11,15
1444536496d8064f33e10b38b5820fecfab5b367,Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs,Julian Aron Prenner,2021,11,29
f74ec348bcfb0c3cf65a1e8ddb3e64374a8c7e9b,Learning Size and Shape of Calabi-Yau Spaces,M. Larfors,2021,11,26
c23d9d44e8bc68408cea9f305d1f24d915bc0d0d,Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey,Bonan Min,2021,11,122
96716a1fe0815b3c3f05c9421dc39515b44e8dd6,Comprehensive analysis of embeddings and pre-training in NLP,Jatin Karthik Tripathy,2021,,11
c09ebcb1ca6ad1eced57340f3e81e456416ed185,A Systematic Investigation of Commonsense Knowledge in Large Language Models,Xiang Lorraine Li,2021,11,16
e88dcda819baa14d7392c0d5d1b9d6996e87016b,Towards Language Modelling in the Speech Domain Using Sub-word Linguistic Units,Anurag Katakkar,2021,11,1
3186b9dd1331b647cf3304d185c248ea7ec9ad1b,OneFlow: Redesign the Distributed Deep Learning Framework from Scratch,J. Yuan,2021,10,18
d6045d2ccc9c09ca1671348de86d07da6bc28eea,Training Verifiers to Solve Math Word Problems,Karl Cobbe,2021,10,520
66d735987a31d666a6459566ae026c40ab9a1c3a,The Efficiency Misnomer,Mostafa Dehghani,2021,10,66
aace5f303546288dc1c5c3dbaadc12a92d78e891,Practical Galaxy Morphology Tools from Deep Supervised Representation Learning,Mike Walmsley,2021,10,11
9019f5da4af44b66b7e0d19e6177baf6d5071dc8,No News is Good News: A Critique of the One Billion Word Benchmark,Helen Ngo,2021,10,3
f309fd989473d73ff9a4d7b3baf20148214b4e53,Identifying and Benchmarking Natural Out-of-Context Prediction Problems,David Madras,2021,10,2
d3ac65b10af091759863b8e2c488036bf52a2ce6,DAG Card is the new Model Card,Jacopo Tagliabue,2021,10,5
9cb47b4063de9ebba86881fbe8774362e7580623,"Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model",A. Bodin,2021,10,7
74ee70e3ecbb2a7123a14de75bee7d3d8514f1cb,Wide Neural Networks Forget Less Catastrophically,Seyed Iman Mirzadeh,2021,10,30
1520b70861275c13331069acbae555984950acac,Intelligent host engineering for metabolic flux optimisation in biotechnology,L. J. Munro,2021,,6
f18ace4af609cf284325b65735aa97e8e0499172,Behavioral Experiments for Understanding Catastrophic Forgetting,Samuel J Bell,2021,10,3
a0b777b25cdf0fc992568ca52a5c7bebf1ee987f,Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research,Ross Gruetzemacher,2021,10,7
e968a3c9590481cd13f2f86a7ac8839e3cf3455f,Improving Compositional Generalization with Self-Training for Data-to-Text Generation,Sanket Vaibhav Mehta,2021,10,13
11ccb9b509d84e845901ee097e7d0a6419fdc182,EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models,Frederick Liu,2021,10,3
6bd91a3183ddb844641acb9f3fe9faec6a9ff617,Meta-learning via Language Model In-context Tuning,Yanda Chen,2021,10,61
d83c7aa12420d5d035f43d7737bfa6893e9e2c61,Exploring Universal Intrinsic Task Subspace via Prompt Tuning,Yujia Qin,2021,10,6
683de10cda9cd3b709983db2da67f1f059fa516a,Towards More Effective and Economic Sparsely-Activated Model,Hao Jiang,2021,10,5
f9c0f78521e5bf4a38f2fd9c417dfc9c54950af1,Automated Essay Scoring Using Transformer Models,Sabrina Ludwig,2021,10,13
6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd,Large Language Models Can Be Strong Differentially Private Learners,Xuechen Li,2021,10,155
e588a46a86e6548f4bb57b0ce2b53b98889759bb,The Evolutionary Dynamics of the Artificial Intelligence Ecosystem,M. Jacobides,2021,,24
101904869d6f2a4fe56838b3eb96f0fb4c4f9ca6,Temporal Convolutions for Multi-Step Quadrotor Motion Prediction,Sam Looper,2021,10,1
24e775b20adf21e9b5b95c6a9b7a5c164d055849,M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining,Junyang Lin,2021,10,26
bfa385a782d7fce8b1951afa8fdee56d070e8453,A General Method for Transferring Explicit Knowledge into Language Model Pretraining,Rui Yan,2021,,1
79f95d13d8430b2e31cb8a8104c0455c6995a259,The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks,James B. Simon,2021,10,10
c206a6e7f51f5e1b6bfc479a174b66ad88ada2db,Exploring the Limits of Large Scale Pre-training,Samira Abnar,2021,10,71
dd0e499557c933477e9de29151370d874e88b4b6,Language Modeling using LMUs: 10x Better Data Efficiency or Improved Scaling Compared to Transformers,Narsimha Chilkuri,2021,10,4
fd33e77884e69f6bc099990fc2790248af2749d9,LexGLUE: A Benchmark Dataset for Legal Language Understanding in English,Ilias Chalkidis,2021,10,92
0ad6c5ac3cc46b2caa3a1927ce70d9c31b05800e,Combating Fake News with Transformers: A Comparative Analysis of Stance Detection and Subjectivity Analysis,P. Kasnesis,2021,,5
90148343ba48aed5eb363337888fa981ef39a489,MotifBoost: k-mer based data-efficient immune repertoire classification method,Yotaro Katayama,2021,,1
31000c79187b9159c53aa53e3a73a008ca3844b4,Using Named Entity Recognition to Identify Substances Used in the Self-medication of Opioid Withdrawal: Natural Language Processing Study of Reddit Data,Alexander J Preiss,2021,,4
08460ecff91b8a54358b9c1709d7dc6a77417f62,Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing,Haoyu He,2021,09,4
6ca9b34fbabbdb33288508906b79beabd0b2ef3e,Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules,Steve Kommrusch,2021,09,0
d52977458284acce166bb1291a9d79143aa59070,Scalable and Efficient MoE Training for Multitask Multilingual Models,Young Jin Kim,2021,09,47
4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1,The Trade-offs of Domain Adaptation for Neural Language Models,Dan Iter,2021,09,10
992129aa96c97fa3b2ced0ddbc4c7d07bfaaf821,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,Siqi Bao,2021,09,43
4a8964ea0de47010fb458021b68fa3ef5c4b77b2,Primer: Searching for Efficient Transformers for Language Modeling,David R. So,2021,09,52
4e0709e83f62f7011ec98dd9182aa3cde81aa66d,Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering,Ander Salaberria,2021,09,20
586cafd248d01c1a1c67a57b3cef9f807173110c,Performance-Efficiency Trade-Offs in Unsupervised Pre-Training for Speech Recognition,Felix Wu,2021,09,46
9cac09098aa611bd9a94d080d2401840632ab16f,LM-Critic: Language Models for Unsupervised Grammatical Error Correction,Michihiro Yasunaga,2021,09,21
a6d8d04962f84ae6225e72723869a002b9fc8036,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,Boseop Kim,2021,09,68
73ebd89b20bb1c1a93e79e4b0175e0825b0aecda,Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach,Koren Lazar,2021,09,7
77d956cdab4508d569ae5741549b78e715fd0749,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Stephanie C. Lin,2021,09,273
732402ff28d1b814ef24dd49d428be333b187d8c,De Novo Prediction of RNA 3D Structures with Deep Learning,Julius Ramakers,2021,,0
752985595ce02327b76bb11e0afafb9d31522215,Scaling Effect of Self-Supervised Speech Models,Jie Pu,2021,,4
5b13fae7218cf5e7f7b0aaf88ba366d35c9a39c6,Design and Scaffolded Training of an Efficient DNN Operator for Computer Vision on the Edge,Vinod Ganesan,2021,08,1
0622f6f8b0ea543671cc1bdb090353017f3558c8,Learning C to x86 Translation: An Experiment in Neural Compilation,Jordi Armengol-Estap'e,2021,08,4
5046dfc4a3b77b120c1ccfc7ada2eba63fe5e1d9,Comparative analysis of molecular fingerprints in prediction of drug combination effects,B. Zagidullin,2021,,30
6d178f85c8ca9698b02d0f4f1f4a8f2fc4895411,The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models,Conglong Li,2021,08,8
4183d028c7b7e54f55e63698232e4d0a6df535bc,Towards Structured Dynamic Sparse Pre-Training of BERT,A. Dietrich,2021,08,12
6c761cfdb031701072582e434d8f64d436255da6,AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing,Katikapalli Subramanyam Kalyan,2021,08,112
917c63f2186119166b3379f5d2816bb1a2f39b09,DEMix Layers: Disentangling Domains for Modular Language Modeling,Suchin Gururangan,2021,08,67
898b14509593d235414df054527b7702e35c3099,Post-hoc Interpretability for Neural NLP: A Survey,Andreas Madsen,2021,08,76
1081df12b1500f48718ad9cdf3b0b90f44a31fc9,Curriculum learning for language modeling,Daniel Fernando Campos,2021,08,10
4c67b079e9154b85dd8486050ef2d98fd8c626d7,Single-sequence protein structure prediction using language models from deep learning,Ratul Chowdhury,2021,,47
2bc9e9b5161df6ea2015b209c8586f5220fd78d0,Generalization Bounds using Lower Tail Exponents in Stochastic Optimizers,Liam Hodgkinson,2021,08,8
c1b219918c2ee00fca9a1628ecea8a5c7f9d15b2,"Simple, fast, and flexible framework for matrix completion with infinite width neural networks",Adityanarayanan Radhakrishnan,2021,08,15
9933a5af7895354087baf6c96b64dc8a8973eaed,Perceiver IO: A General Architecture for Structured Inputs & Outputs,Andrew Jaegle,2021,07,314
528d1f017cf87144aecb43834b2712a2300a9190,Over-Parameterization and Generalization in Audio Classification,Khaled Koutini,2021,07,1
1106f88502c8681c774a63fd1553fb98525fe2fa,Epistemic Neural Networks,Ian Osband,2021,07,49
58350665d7880fa1c85f6889b3a44cc96fa67d56,Codified audio language modeling learns useful representations for music information retrieval,Rodrigo Castellon,2021,07,40
acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269,Evaluating Large Language Models Trained on Code,Mark Chen,2021,07,1494
44ca883b0f1385d4cf832fe3ed65a0f00b40bb5f,"Prioritized training on points that are learnable, worth learning, and not yet learned",S. Mindermann,2021,07,0
319b84be7a843250bc81d7086f79a4126d550277,ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,Yu Sun,2021,07,179
7220984a462b650a0f1e12a0954cf7cd273434aa,A Generalized Lottery Ticket Hypothesis,Ibrahim M. Alabdulmohsin,2021,07,4
20a01009d38d083a49e01ef46005363135453661,The great Transformer: Examining the role of large language models in the political economy of AI,D.M.E. Luitse,2021,,24
03c06a7e960311758e05f89a3a5e5c3913971704,JUWELS Booster - A Supercomputer for Large-Scale AI Research,Stefan Kesselheim,2021,08,7
4b88931fe3f022f508ba4d01aa7994603a3ee359,Adversarial examples within the training distribution: A widespread challenge,Spandan Madan,2021,06,1
cc33ddc931cb3f709f7f6e9d4165cf8d98874f67,Towards universal neural network potential for material discovery applicable to arbitrary combination of 45 elements,So Takamoto,2021,06,47
8125a482edcff959e7245a1da0da9ff6d5ed9742,Deep limitations? Examining expert disagreement over deep learning,Carla Zoe Cremer,2021,,22
f6c4d57c710b461ece658a0b8e7427e862fef116,NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs,Mikhail Galkin,2021,06,41
64902a5077ee68011cd467398dbb66511e8e891a,It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning,Alexey Tikhonov,2021,06,17
f76a85b34dee7d3a20937a9d016bcbed2e947f2f,Distributed Deep Learning in Open Collaborations,Michael Diskin,2021,06,19
90e99c33ce72b81445dad9b10cff931201a9d5c7,On Anytime Learning at Macroscale,Lucas Caccia,2021,06,15
67bbcc013a70b09652032623787dba6ab80740a7,"Learning the protein language: Evolution, structure, and function.",Tristan Bepler,2021,,134
feba0c47bf12a02c3a725174bb53df78658a72a8,"Pre-Trained Models: Past, Present and Future",Xu Han,2021,06,316
41fe7f4b3ebf9616419101faa8c5f2ee43a118b4,Revisiting Model Stitching to Compare Neural Representations,Yamini Bansal,2021,06,51
d103f20f0a30eb055d0ae44b3b5b8ab44b0c7913,Neural Symbolic Regression that Scales,L. Biggio,2021,06,64
883fd1cad73f37a41196f6eeb92a07e85af8ec8c,"Disentangling the Roles of Curation, Data-Augmentation and the Prior in the Cold Posterior Effect",Lorenzo Noci,2021,06,13
9f9e6b4731d3cf467bf2bfab4ce42bbc6d4afd73,GroupBERT: Enhanced Transformer Architecture with Efficient Grouped Structures,Ivan Chelombiev,2021,06,3
a6337d9ebb0b7de84588806110157806f9c0383b,GraphiT: Encoding Graph Structure in Transformers,Grégoire Mialon,2021,06,70
0f1382cb004b4834cc3ca7824a61d0d6b86a5763,Pretraining Representations for Data-Efficient Reinforcement Learning,Max Schwarzer,2021,06,66
0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe,Hash Layers For Large Sparse Models,Stephen Roller,2021,06,90
5aab57cc0530560d82c74c055f664280619d7e81,PROST: Physical Reasoning about Objects through Space and Time,Stephane T Aroca-Ouellette,2021,06,22
9c4dd36ad206ca8be96ae4000568e899f4acfa91,Top-KAST: Top-K Always Sparse Training,Siddhant M. Jayakumar,2021,06,68
70db308d3635bf20bfbc66e177e6744362e4a9cb,ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression,Weiyue Su,2021,06,8
3127973b8ca73c67c3ba207c4a2b59dba8e5d258,Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models,J. Lamy-Poirier,2021,06,0
aee7fd72f33bc8060ff32eb88f88b904802a9243,Lower Perplexity is Not Always Human-Like,Tatsuki Kuribayashi,2021,06,30
c0f77574d63a1d608009ad096e8f0fc791eb1297,A Generalizable Approach to Learning Optimizers,Diogo Almeida,2021,06,18
10e37aea3266cfa33849a530db7eec4d4b52966b,What Matters for Adversarial Imitation Learning?,Manu Orsini,2021,06,46
32feca141fce06c6588b4014d27953a3fc25f19b,PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World,Rowan Zellers,2021,06,42
ec5798563138f80de1a4ab2f54ceddd894d7522f,Doing more with less: training large DNN models on commodity servers for the masses,Youjie Li,2021,,4
7777a31341fa4bfbd25b96a5320681af8dccf3af,Exploring Sparse Expert Models and Beyond,An Yang,2021,05,22
981995fd64611f475179b280f4e9c241051ac185,Knowledge Inheritance for Pre-trained Language Models,Yujia Qin,2021,05,25
1006d191e9eb5b4dbc35fc0bb389328ddc75cba7,ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models,Linting Xue,2021,05,208
b9dd0fec4df68bec7673c260f34083a1bec5895b,One4all User Representation for Recommender Systems in E-commerce,Kyuyong Shin,2021,06,22
28459083ba624020c8f1c1ed7c3a075f48b4e709,KLUE: Korean Language Understanding Evaluation,Sungjoon Park,2021,05,93
e3a3e85c5a32af29e13b3561f6cf070de70651de,Pay Attention to MLPs,Hanxiao Liu,2021,05,344
bfe6d34ceece0a79bee5791b35e1314018b62201,Compressed Communication for Distributed Training: Adaptive Methods and System,Yuchen Zhong,2021,05,6
8307e0a2c64092fce2d44e73c6b865afe4264d55,Investigation of text data augmentation for transformer training via translation technique,Dominykas Seputis,2021,,1
5b1641b7661b4d9ec2826e847ebf1b36f2d5bdec,What’s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus,A. Luccioni,2021,05,51
dc70b180329a6ffead5e48093fb5a551955047c4,HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish,Robert Mroczkowski,2021,05,37
a65b7ff72deb358bef29900a53c814771275c37c,Scaling End-to-End Models for Large-Scale Multilingual ASR,Bo Li,2021,04,48
ba68248488a92114d5136263d284e109395abb5e,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,GongZheng Li,2021,04,4
79b8ef3905a42b771248719495a2117271906445,Carbon Emissions and Large Neural Network Training,David A. Patterson,2021,04,303
478bc0a2b9e5230c1f97eb90934122b682095b11,"Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?",T. Isbister,2021,04,12
3a863672b072a37ac8d2eb6b780234f6b748a2d9,Generating bug-fixes using pretrained transformers,Dawn Drain,2021,04,36
01df9e724fb0149ad350ff6870b6110a587cb626,Comparative analysis of molecular representations in prediction of drug combination effects,B. Zagidullin,2021,,1
6263f69b0154fb090fc76f792c1a4aa182e24294,Language Models are Few-Shot Butlers,Vincent Micheli,2021,04,16
4506c9cd34a0b6f09fe23a3d9be0bc0962d14540,ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning,Samyam Rajbhandari,2021,04,144
d8ea98c57f1f4cf4d42b26fd8e3694772e9ca495,Robust Optimization for Multilingual Translation with Imbalanced Data,Xian Li,2021,04,13
7694aae9766d5f1fe74d900cd82aee898cb6e8e9,How to Train BERT with an Academic Budget,Peter Izsak,2021,04,61
da8a4416db1da4887572ed47240a72b6f177a472,Online and Offline Reinforcement Learning by Planning with a Learned Model,Julian Schrittwieser,2021,04,70
4b734d4fb14acbc5135eb382e1388840df84e9f2,Lookup-Table Recurrent Language Models for Long Tail Speech Recognition,W. R. Huang,2021,04,4
981dbdf6f87f13f3f3047a925c519fc39a35202b,Revisiting Simple Neural Probabilistic Language Models,Simeng Sun,2021,04,6
52c06a43eeddd2a0666b7cd3ab9fd41686bc7db4,GPU Domain Specialization via Composable On-Package Architecture,Yaosheng Fu,2021,04,5
2a8fa407e074bebeaf1e254be37fae7fc54610e3,SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,William Chan,2021,04,86
614e53f957525fa652df64b9c45731a8b66cd73f,Why is AI hard and Physics simple?,Daniel A. Roberts,2021,04,6
d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f,Understanding Robustness of Transformers for Image Classification,Srinadh Bhojanapalli,2021,03,223
65ba326ca90f6c6f204fad91db76af1076aeac30,Active multi-fidelity Bayesian online changepoint detection,Gregory W. Gundersen,2021,03,8
7198c33fcbd3561f9491c73529eb19a45ac298cc,Visual Grounding Strategies for Text-Only Natural Language Processing,Damien Sileo,2021,03,8
2642ab2e7f570891c06768c6a3bedb6033ba83b3,K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining,Rui Yan,2021,04,1
b9ce9fea4634d6bfed5af2f4de410822295b3630,Can Vision Transformers Learn without Natural Images?,Kodai Nakashima,2021,03,18
238eb420c472bfdb1b4d34f9f53abec51f307a6b,FastMoE: A Fast Mixture-of-Expert Training System,Jiaao He,2021,03,41
f111b5e43118891c517e9f47a36418870dc39533,How to decay your learning rate,Aitor Lewkowycz,2021,03,9
2972bd6cb49883a5c75e26f8f7266dc91e1af25a,Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles,Jevgenij Gamper,2021,03,25
a4bb144fc0e7b6d25ff770c556745d9149c04147,Training a First-Order Theorem Prover from Synthetic Data,Vlad Firoiu,2021,03,9
2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6,Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices,Max Ryabinin,2021,03,12
63a9daf15ae2d4c1a7859d3105c9e6710903e072,Perceiver: General Perception with Iterative Attention,Andrew Jaegle,2021,03,489
a194297f3703bae8040ddfd0c08a60cdcaea2aeb,Transformers with Competitive Ensembles of Independent Mechanisms,Alex Lamb,2021,03,16
453fc588d97958c6fefad96e79edd896873b3e09,Chess as a Testbed for Language Model State Tracking,Shubham Toshniwal,2021,02,11
288cb169619bde78604450adc8cb5df536ef20f1,Learning Chess Blindfolded: Evaluating Language Models on State Tracking,Shubham Toshniwal,2021,,5
46130875c8c2d89ea23dfb29c3784a6e5e510e54,Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,Víctor Campos,2021,02,17
19537be34dbadbcaa4fffcf028a8ada5095b1b5c,COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining,Yu Meng,2021,02,144
c16835c8e535ebd9c10a550ca9455fe384a14449,High-Performance Large-Scale Image Recognition Without Normalization,Andrew Brock,2021,02,351
ec05bd6725ac6a5217021881cac8553581b3e313,Measuring Progress in Deep Reinforcement Learning Sample Efficiency,Florian E. Dorner,2021,02,7
be09ed6cd73654a23f78416433a1b23ea623ea79,Symbolic Behaviour in Artificial Intelligence,Adam Santoro,2021,02,26
ba233d75aa403092bda0bffc026be7913673ad69,Mind the Gap: Assessing Temporal Generalization in Neural Language Models,Angeliki Lazaridou,2021,02,98
232d0c42a26a54f52266a09e0c7117291402d33c,Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research,A. Feder Cooper,2021,02,35
12b71736392209b4292471b7da0aed71ba2aa545,ZeRO-Offload: Democratizing Billion-Scale Model Training,Jie Ren,2021,01,156
fdacf2a732f55befdc410ea927091cad3b791f13,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,W. Fedus,2021,01,885
0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36,Deep Learning applications for COVID-19,Connor Shorten,2021,,174
4b30dd65a26e573df9796beb582ba1e1a69f23f7,Reservoir Transformers,Sheng Shen,2020,12,7
98cadf04e1f4a65552debc0376471f7370f2e8da,BERT Goes Shopping: Comparing Distributional Models for Product Representations,Federico Bianchi,2020,12,11
5d76c2591334f56dc9155568f793b013df0f6613,Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training,Chen Xing,2020,,0
eb2fc03b8865b8e1b4cb933d917ea269ebe14584,Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training,Chen Xing,2020,12,0
043e5e0cf3129284683260976c10d98c7f121f35,Transformer protein language models are unsupervised structure learners,Roshan Rao,2020,,159
62d1a3137b01a69443bebf4d92c1990ec512a6a1,Extracting Training Data from Large Language Models,Nicholas Carlini,2020,12,745
4c5d4601a3a19c31da6588d2a34adfb161f68c0e,Imitating Interactive Intelligence,Josh Abramson,2020,12,51
64e1b6bae8de89d49b1b8f52d0ae09f8b48cbcc3,Parallel Training of Deep Networks with Local Updates,M. Laskin,2020,12,15
33422275fbb9958f55419620697faf531482699b,How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering,Zhengbao Jiang,2020,12,138
2f8f049cbaded475ed10808b60388ce7dc1374e1,Real-Time Social Media Analytics with Deep Transformer Language Models: A Big Data Approach,Ahmed Ahmet,2020,,3
4d9709e366afccd097129e80c92006189fc901ed,Bounds for Algorithmic Mutual Information and a Unifilar Order Estimator,L. Debowski,2020,11,0
f0dad13022ae7c236517a88b6a151b304aa04963,Whale: Efficient Giant Model Training over Heterogeneous GPUs,Xianyan Jia,2020,11,18
b5613da1f643159c97cbf8555d6f5c4f05b36a9a,High Performance Natural Language Processing,Gabriel Ilharco,2020,,4
9dbb94bdf18a90a04697b19975b6ada4d257cc0a,Stochastic Optimization with Laggard Data Pipelines,Naman Agarwal,2020,10,10
bc87279d4b32a425377ff18ab63f7ecf95ff228c,Rethinking embedding coupling in pre-trained language models,Hyung Won Chung,2020,10,100
4d3f6673009589971973a81a097441e7c78a265e,Improving Multilingual Models with Language-Clustered Vocabularies,Hyung Won Chung,2020,10,43
687b13c44f849d23c2496996b5da83e706094db9,Beyond English-Centric Multilingual Machine Translation,Angela Fan,2020,10,458
1a6c5f6ce26914c2a7af0217e8cd3e844f2b2f37,"Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",Gyuwan Kim,2020,10,38
ccad27088b9098de4eaca8dc449b18766db4b3ab,Reformulating Unsupervised Style Transfer as Paraphrase Generation,Kalpesh Krishna,2020,10,139
4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c,Contrastive Representation Learning: A Framework and Review,Phúc H. Lê Khắc,2020,10,334
7e55df7baa707cb9bd32c0f72e9882038f4fa5b8,Using Support Vector Regression in multi-target prediction of drug toxicity,F. Adilova,2020,,1
3d2660faebfcc1b7c7777f48b32a2eebec346bab,Guiding Attention for Self-Supervised Learning with Transformers,A. Deshpande,2020,10,13
b090cfc2179a789e7aeb684ee8c85e71bcbd4aa0,A Closer Look at Codistillation for Distributed Training,Shagun Sodhani,2020,10,6
dae4641eed6ddbe0a781ab5e78daf8204e60f397,Which *BERT? A Survey Organizing Contextualized Encoders,Patrick Xia,2020,10,37
ee5fff85d3ec62698eddba162f054b7e73670b2a,Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics,Swabha Swayamdipta,2020,09,236
f89e2f4fb44e30b1adb08d153bf22b063597f896,Current Limitations of Language Models: What You Need is Retrieval,Aran Komatsuzaki,2020,09,2
15aa86556be1579eadf8fbb0bc486f9427e681f0,Evaluating representations by the complexity of learning low-loss predictors,William F. Whitney,2020,09,21
486d580110a6780012b25c9783299e31f173ecf1,Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent,Matthew Brennan,2020,09,35
07794f74ebe869e64ea6d9f63fe71abf5c25196b,Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain,Beren Millidge,2020,09,15
10bb7e2c54b947fa50e7bb65b0b5c700fe998044,Measuring Massive Multitask Language Understanding,Dan Hendrycks,2020,09,404
51ae2c451a1a05293334a509b71c9c9e0377d35c,"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",Benjamin Heinzerling,2020,08,65
e64154041938fdee4dd6855cc79bfb3393eee23e,Roadmap to a Roadmap: How Could We Tell When AGI is a 'Manhattan Project' Away?,John-Clark Levin,2020,08,4
5a98bcb1082fc50d31ddb59671d3026f16a0dfc6,Forecasting AI Progress: A Research Agenda,Ross Gruetzemacher,2020,08,11
64da659c0687762359226b4cf455520c78acd165,"Neural Language Generation: Formulation, Methods, and Evaluation",Cristina Garbacea,2020,07,19
063f8b1ecf2394ca776ac61869734de9c1953808,AdapterHub: A Framework for Adapting Transformers,Jonas Pfeiffer,2020,07,360
1f8c1b8943466e2698dca2f6335081c5b46506ec,Language Modeling with Reduced Densities,T. Bradley,2020,07,6
7267c8343bd28b472e5b0684e98f067dfd70d5d4,Accuracy-Efficiency Trade-Offs and Accountability in Distributed ML Systems,A. Feder Cooper,2020,07,7
1882f194cb43828852cc052887671e55a80f945a,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,Dmitry Lepikhin,2020,06,487
8668fd1cb5cab820f8b2a136b2ef4adfad6c4dc1,Knowledge-Aware Language Model Pretraining,Corby Rosset,2020,07,51
c00ba15810496669d47d2ed5b627e6c7d2b1f6aa,Pre-training via Paraphrasing,M. Lewis,2020,06,127
0ece9e13768dd818c677254d1185775c70eaa6cc,Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks,Abdulkadir Canatar,2020,06,104
5844cfc72c1b75c544ff487a3737de6305df8935,Statistical Mechanics of Generalization in Kernel Regression,Abdulkadir Canatar,2020,,7
dc0fb0c81f87859a04314c94d74fc7d9c909393d,Online Handbook of Argumentation for AI: Volume 3,Ohaai Collaboration Federico Castagna,2020,06,1
c014f8bc3b521453a93a13bb2c90700fcf462738,Limits to Depth Efficiencies of Self-Attention,Yoav Levine,2020,06,32
cbe9be3a9731c13dd18fc7bdfaf8dcedfe7a5544,What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,Marcin Andrychowicz,2020,06,133
1426c97cc8f8f23a6cb1cfa4d4e10f27b0e2a3a7,Predictive Coding Approximates Backprop Along Arbitrary Computation Graphs,Beren Millidge,2020,06,80
f2fc9ef411846dd577c26225ce93f50bb1fa760b,Multi-scale Transformer Language Models,Sandeep Subramanian,2020,05,8
d97e7561fa7710213ccd4f8128044ea6849be377,XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,E. Ponti,2020,05,130
0771a9c573de33306b68d7ffe5564e285e4c74bb,Breaking (Global) Barriers in Parallel Stochastic Optimization With Wait-Avoiding Group Averaging,Shigang Li,2020,05,9
9b539d413393047b28bb7be9b195f142aaf7a80e,Recipes for Building an Open-Domain Chatbot,Stephen Roller,2020,04,703
f6e0164466e827112fd415afdc28ddf8e0eb1ba3,Document Ranking with a Pretrained Sequence-to-Sequence Model,Rodrigo Nogueira,2020,03,269
cf59f5248aa6f64150a6a6f668579a3e04e6f64a,On a Class of Markov Order Estimators Based on PPM and Other Universal Codes,L. Debowski,2020,03,2
2356781b8a98bf94e6fc73798c6cb65ac35e5f97,"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",Zhuohan Li,2020,02,193
f375915dddc706d2df04ec103eb004fd71ec1a8f,Learning@home: Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,Max Ryabinin,2020,,5
19eaa4ac17550fab2917d3f6121ed25e6d857a58,Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts,Max Ryabinin,2020,02,25
0b838b84c8b8610b869f26eebec37228ebf347dd,The Transformative Potential of Artificial Intelligence,Ross Gruetzemacher,2019,12,23
00c957711b12468cb38424caccdf5291bb354033,ZeRO: Memory optimizations Toward Training Trillion Parameter Models,Samyam Rajbhandari,2019,10,485
72ff8fa78d2a669389041c1b5ebe015c43448e93,Meta-Learning Initializations for Image Segmentation,S. Hendryx,2019,12,17
f64a4256a8e79c8553e2e8eb701a7b0a9c306236,Text Implicates Prosodic Ambiguity: A Corpus for Intention Identification of the Korean Spoken Language,Won Ik Cho,2018,11,6
da7627dd7769210fc6ec917d13b00365d0989c97,"Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",Sunayana Sitaram,2023,,1
789ccb1c15919fee66f1b2b67dc40365cb9996dd,HyperT5: Towards Compute-Efficient Korean Language Modeling,Dongju Park,2023,,0
1862ff140e0169c6b367dc3fd9e7c714dae38026,N EURAL N ETWORKS AND THE C HOMSKY H IERARCHY,Grégoire Delétang,2023,,0
75c08892179fc478f87d7020b5daff9fca4f3389,Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models,Chen Ling,2023,,7
33c61fcec57f5b412439e5a882b1a2a1e8a1de5a,MEMORY-EFFICIENT TRAINING,Sunghyeon Woo,2023,,0
df239785e6d26a45e9c8e06551cfecba92d1ecad,Exploring AI Ethics of ChatGPT: A Diagnostic Analysis,Terry Yue Zhuo,2023,,104
809f3198289554fce309795219cdb42befede20e,Almanac: Knowledge-Grounded Language Models for Clinical Medicine,C. Zakka,2023,,2
e8f060d1947b2deedffd6cf7d7bf8e327b333236,A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training,Siddharth Singh,2023,,0
4deafcd9db4687904f068c14ecd0f1a7a8103eea,Experimental Case Study of Self-Supervised Learning for Voice Spoofing Detection,Yerin Lee,2023,,1
4e8d03e525ea66bebdbc2769c5f60f40ad16fd05,What is the State of Memory Saving for Model Training?,Xiaoxuan Liu,2023,,0
82dd7bd1b722803306b0ab93aa93afcfbcaaac0b,mSilent: Towards General Corpus Silent Speech Recognition Using COTS mmWave Radar,Shangcui Zeng,2023,,0
70583309207f006020e514b7925103944029a1c6,BOLT: An Automated Deep Learning Framework for Training and Deploying Large-Scale Neural Networks on Commodity CPU Hardware,Nicholas Meisburger,2023,,1
4e2ff3308fe59a682071bf1869b979eb62a6f4fc,Deep learning of genomic contexts predicts protein co-regulation and function,Yunha Hwang,2023,,0
499075670464a7426ea5d12171cc55e892fad1d1,"Why try to build try to build a co-creative poetry system that makes people feel that they have ""creative superpowers""? 67-80",Ibukun Olatunji,2023,,0
6d172caf3723ef9faee956d535a96e91560638da,GPT-2 Metadata Pretraining Towards Instruction Finetuning for Ukrainian,Volodymyr Kyrylov,2023,,2
2d1842056c4311c886ed9bf634e310490d725b33,On the Concept of Resource-Efficiency in NLP,Luise Dürlich,2023,,0
7889cc4e9565c9e7bd725509d7da4597e6a9b576,CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks,Jue Wang,2023,,2
c148d80a82308d04e45ed480aaed1ff8fed0d5bc,Semantic-Augment: Augmenting the Semantic Space of Transformers Improves Generalization,Emirhan Kurtuluş,2023,,0
793f7284ccaa6c2cd530e6d405f5fa75bfd283e8,Scaling Infrastructure to Support Multi-Trillion Parameter LLM Training,Mikhail Isaev,2023,,0
9b6c38ef6eb937f4a802d75169bd18fba5394f71,Guideline for GPT-2: Investigating Model Variants For Different Computational Budgets,Ryan Kang,2023,,0
07ae5b2c07302281b1f0d17f01efeb88091a0930,Minimum Generative Pre-trained Transformer with Human Feedback,Yanjia Li,2023,,0
6ec6f56284232220e5c70e6138228fecf4576d10,GUAGE MODEL SELECTION,A. Owodunni,2023,,0
4762b614958013039ed14c59367eca05165b0c21,INSTRUCTION-FINETUNED FOUNDATION MODELS FOR MULTIMODAL WEB NAVIGATION,Hiroki Furuta,2023,,2
52344ebfe637d5d83e541185ae1e4753d572305f,Towards Disentangling the Roles of Vision & Language in Aesthetic Experience with Multimodal DNNs,C. Conwell,2023,,0
9801d47561825077621957cddf786cf1e26fbd3f,Road Barlow Twins: Redundancy Reduction for Motion Prediction,Royden Wagner,2023,,1
2dbc8228b86ff20612403e5aa7dd2984ea01ca7a,"Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding",Jonathan Hueser,2023,,0
63d8cd208d029d54a3c31f4c47d11ad9b2b16dee,Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times,Andrea Gregor de Varda,2023,,0
889a736ac167b5e465d0f0c83400244619efb566,RIGA at SemEval-2023 Task 2: NER Enhanced with GPT-3,Eduards Mukans,2023,,1
0db494283ace79ca1cabd169cc681ffccc0dcdc8,Corpus Complexity Matters in Pretraining Language Models,Ameeta Agrawal,2023,,0
899c7fd82d9afe38f76ffca105f2e60fe289ef09,U NMASKING THE L OTTERY T ICKET H YPOTHESIS : W HAT ’ S E NCODED IN A W INNING T ICKET ’ S M ASK ?,Brett W. Larsen,2023,,0
04262297b65a85476343f6483c09fc4f7f21eb59,G ENERATE RATHER THAN R ETRIEVE : L ARGE L ANGU - AGE M ODELS ARE S TRONG C ONTEXT G ENERATORS,Wenhao Yu,2023,,0
d790fe6ed7280f0596133aeb9d5be6bd4e8f54e9,DCI-ES: AN EXTENDED DISENTANGLEMENT FRAME-,Cian Eastwood,2023,,0
e4f0ab1408504c3873a849d341ec63fbca899534,Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent,Ping-yeh Chiang,2023,,5
2bd6afb11d81fb97bf6e2114043c35bd12c96ce9,P A LI: A J OINTLY -S CALED M ULTILINGUAL L ANGUAGE -I MAGE M ODEL,L. Beyer,2023,,0
a2a2d664b5ac7b3c35e4448203c519c4a26bf092,MUX-PLMs: Pre-training Language Models with Data Multiplexing,Vishvak Murahari,2023,,0
e50921bc8585bf78ef896e1f31718c5ca102aa67,Can Large Language Models Reason about Program Invariants?,Kexin Pei,2023,,6
a4d30f1d3c518310229af88ce85af9c6f02589b4,Efficient Fine-Tuning Large Language Models for Knowledge-Aware Response Planning,M. Nguyen,2023,,0
ecdbb4c306915685552e8cd752b8ab4c741ce59d,Not Enough Data to Pre-train Your Language Model? MT to the Rescue!,Gorka Urbizu,2023,,1
9249abc1ca05aeb9ffb45db638e4e8e49fc851dc,A Statistical Perspective on Retrieval-Based Models,Soumya Basu,2023,,0
8dc86c158696cf86b9bd51c85d9927e427bd63b4,On the role of resources in the age of large language models,Simon Dobnik,2023,,0
77b60fdaf00ba3287c07d5584df9be38bf8dcabc,MAP: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting,Wenyan Li,2023,,0
bb3f6ff6d4ae0e74a5f46a3cb3697446915980cb,Recent Trends in China’s Large Language Model Landscape,Jeffrey Ding¹,2023,,3
c05e852fa8fa8aa2493b9ace04320701102eb6cb,T RENDS AND CHALLENGES OF A RABIC C HATBOTS : L ITERATURE REVIEW,.. M. M. Gammoudi,2023,,0
75f43654ac8fe62095c84d8d201095e9959d6dcf,What Should Be Done About Google's Quasi-Monopoly in Search? Mandatory Data Sharing Versus AI-Driven Technological Competition,B. Martens,2023,,0
a6d6694bff3b4004c787e3e792211a17674dc009,Classification-Based Framework for Remaining Useful Life Prediction With Limited Images and Unequal Time Intervals,Xiaoyan Zhu,2023,,0
b28c36064b968e927c068edc8d96315862333e46,Uncovering the Risks and Drawbacks Associated With the Use of Synthetic Data for Grammatical Error Correction,Seonmin Koo,2023,,0
46299fee72ca833337b3882ae1d8316f44b32b3c,Reflexion: an autonomous agent with dynamic memory and self-reflection,Noah Shinn,2023,,88
1773be628c1ac76eb0e2d024a96627a98f27b62c,The argument for near-term human disempowerment through AI,Leonard Dung,2023,,0
75a259f3a61bf27c214d61cd2ef26f3291d77add,A trajectory is worth three sentences: multimodal transformer for offline reinforcement learning,Yiqi Wang,2023,,0
f8982fa1fa350b12b2fc8ba34b32af6896566652,Latency-Aware Short-Term Video Action Anticipation and its Application in Trajectory Prediction,Harshayu Girase,2023,,0
af1036818d8359b974f449fb5a501bc02e29f77d,Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey,Bonan Min,2023,,0
0801de6ecb137b1603772328c9791d69c1ba654e,Semantic data augmentation for meaning maintenance on Task-Oriented Conversation with Large-size Language Model,Jaehwan Lee,2023,,0
70f8e3c72e9178b408667e3619a87a153fd853e6,"Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned",Sameera Horawalavithana,2022,,8
9490d42c4869e6d6f3308c9813b1cfe31ff80137,E FFECT OF MODEL AND PRETRAINING SCALE ON CATASTROPHIC FORGETTING IN NEURAL NETWORKS,V. Ramasesh,2022,,1
cb9082315a417b301282c33d547ba13d76d80993,Towards Automatic Generation of Messages Countering Online Hate Speech and Microaggressions,Mana Ashida,2022,,9
66ba0765439c7f0189ace81cfd8626f88b260d3f,Optimisation & Generalisation in Networks of Neurons,J. Tropp,2022,,0
b42e3a759348f27cca2f918a6bd0b139a5312e44,A Survey of Pretrained Language Models Based Text Generation,Junyi Li,2022,,21
55b9a2ade0a49e9cf10b71528d69dfee4e826025,C EDILLE : A LARGE AUTOREGRESSIVE LANGUAGE MODEL IN F RENCH,Martin Müller,2022,,0
326f4a9625a1a3fdf696ce87cd82a6595e8311d5,I-Tuning: Tuning Language Models with Image for Caption Generation,Ziyang Luo,2022,,5
6b86a91737809b869ae7bf3d34c231b697728825,Do Language Models Learn Commonsense Knowledge?,Xiang Lorraine Li,2022,,6
57ceadbb37da24ce24b3ab8ff826ddcae717c0e2,Learning Transferrable Representations of Career Trajectories for Economic Prediction,Keyon Vafa,2022,,2
290ef971d936b9b727dc36dbf4f38be6c8d915c5,LiteTransformerSearch: Training-free On-device Search for Efficient Autoregressive Language Models,Mojan Javaheripi,2022,,11
ed0d9ef9891cf19c5c428e41effe5fedfdb5386e,Robust Question Answering: A study on Self-Attention Mechanism and Data Augmentation,Shicheng Xu,2022,,0
f15562af6b15c88f2b10a1d11aa26c15d2be3436,Summarizing Radiology Reports’ Findings into Impressions,Kathy Yu,2022,,0
6ee286a5a0eceae111ad579f17aedb115e4e8c6c,Improved QANet on SQuAD 2.0,Chenyang Dai,2022,,0
cbad1c5fdc942bb92b090741ba975a0fec91f97e,QAN-et al.: Exploring Extensions on QANet,Michelle Qin,2022,,0
83b80139de383c8aca991964d6248f066eef1426,Team Xuber: Question Answering via Modified R-NET Construction,Damon Zuber,2022,,0
29fb34f80b0e42da74e0d451b8fdaf2e4f4e8375,Classifying and Automatically Neutralizing Hate Speech with Deep Learning Ensembles and Dataset Ensembles,Ali Hindy,2022,,0
7ab536d567d60a9887c93adb562659be76138913,Prospectively validated disease-agnostic predictive medicine with augmented intelligence,B. Lovetrue,2022,,0
33f3f31f871070f19b0c3e967a24e322bfc178f2,Retrieval-Augmented Diffusion Models,A. Blattmann,2022,,44
c04f7b80487a7081e0e603a9ae36c5297f5742a5,Foundational Models for Continual Learning: An Empirical Study of Latent Replay,O. Ostapenko,2022,,11
f40aeae3e522ada1f6a9f326841b01ef5c8657b6,Unifying Language Learning Paradigms,Yi Tay,2022,,98
7450a612ef291216b0cd48e09b8879be4675c6eb,Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions,Haw-Shiuan Chang,2022,,5
84f78cc33b124434aa8e4b670686de6831be02ba,Distributional Semantics Still Can’t Account for Affordances,Cameron R. Jones,2022,,8
8b51019000acad8f96e1be8582a086d6f69f394d,Semi-Parametric Deep Neural Networks in Linear Time and Memory,R. Rastogi,2022,,0
a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f,P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR,Sophia Kolak,2022,,0
104f7a96eba307056e1038e183ee8c24d009ba13,nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models,Gunho Park,2022,,36
8f248b5476666e700390f7f8ff6ca923f97d726c,Advancing protein language models with linguistics: a roadmap for improved interpretability,Mai Ha Vu,2022,,6
d595e49ed44e28599b82c7a99ecf6794f50d41ce,Lightweight Transformers for Conversational AI,Daniel Pressel,2022,,1
26c7ed5aa1ef22dbd33358d12ef02b164e74c3ea,Multimodal large language models for inclusive collaboration learning tasks,Armanda Lewis,2022,,1
18ca53bc269b5d532256b5a7805f111427ef53cf,Evaluating Neural Network Pruning Techniques on Visual Transformers,Sarah Chen,2022,,0
be995829139bad9e91a0554220fd2d5faf9d3d3b,Diet Selective-Backprop: Accelerating Training in Deep Learning by Pruning Examples,Yuning Lu,2022,,1
c6368f4c480e4457dd40bee004bb46428283278d,A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer,Wei-Ning Hsu,2022,,4
9a5e9e08734fbe5d3b7a6ba18351cd0ba52ab396,Pruning Adatperfusion with Lottery Ticket Hypothesis,Jiarun Wu,2022,,0
bb3c188768fd4481bf83353caa94eb1f65f0d30a,Reject Before You Run: Small Assessors Anticipate Big Language Models,Lexin Zhou,2022,,1
39a3bfa818c00853103e7928f76612d3fe7ffe9d,Superior generalization of smaller models in the presence of significant label noise,Yihao Xue,2022,,2
9d7a75601e0e50dd68d40cfb8ef0e891dad797a6,Orca: A Distributed Serving System for Transformer-Based Generative Models,Gyeong-In Yu,2022,,21
4217467e747182b9ad8035e8a2d657d2ce80af07,On Reality and the Limits of Language Data,Nigel Collier,2022,,5
48c646ba65d40c0de759ea6d05edd3d7ef43311e,Improving Offline Handwritten Text Recognition Using Conditional Writer-Specific Knowledge,,2022,,0
2e6ca609b301ea6d62d7e2ae40b59064727c6614,Nepali Encoder Transformers: An Analysis of Auto Encoding Transformer Language Models for Nepali Text Classification,Utsav Maskey,2022,,0
ef74ec406378390cfaa2a939e5141ee943fba01b,Few-Shot Regularization to Tackle Catastrophic Forgetting in Multilingual Machine Translation,Salvador Carrión-Ponz,2022,,2
0e472830a6604c5ff54b9cc8837227b4bb68706d,ITAINNOVA@DA-VINCIS: A Tale of Transformers and Simple Optimization Techniques,R. Montañés-Salas,2022,,1
7f304f180c10715ab0e3624f06482a2b8e771a72,Bigger&Faster: Two-stage Neural Architecture Search for Quantized Transformer Models,Yuji Chai,2022,,0
d461aef562bd9c13dc0f972f147df8be1fcca63c,Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training,Eghbal A. Hosseini,2022,,9
c5862fd1bb07216a2cd7fcca5b2536b74eba0d44,A Closer Look at Parameter Contributions When Training Neural Language and Translation Models,Raúl Vázquez,2022,,1
3ec1723060c31489c5ab953b9402302d360eab83,Coordinating Heterogeneous Teams for Urban Search and Rescue,Zongyue Zhao,2022,,1
2f21201ac9fcb88a72c56471402388ec2fc365a8,Inferring Implicit Relations in Complex Questions with Language Models,Uri Katz,2022,04,7
0a9364ca06771d2b85da147a453bca0d02f8e248,Multimodal Machines from a perspective of humans Ph.D Thesis Proposal,Sunit Bhattacharya,2022,,0
38338207e9ee2591e67c926b7da2294318a0dec2,Models with Conditional Computation Learn Suboptimal Solutions,Mohammed Muqeeth,2022,,1
40accf927b75c49dad0f08199c390d1158ef202b,Contextualizing Language Models for Norms Diverging from Social Majority,Niklas Kiehne,2022,,1
e352f5753cbb4cee32f1d5f8d922d1ce415183ac,Unmasking the Lottery Ticket Hypothesis: Efficient Adaptive Pruning for Finding Winning Tickets,Mansheej Paul,2022,,0
29d0e22287b05661cc7d9c90eabadd6733ef6946,Semantic Shift Stability: Efficient Way to Detect Performance Degradation of Word Embeddings and Pre-trained Language Models,Shotaro Ishihara,2022,,2
1fcd87c6aed5f4510b6ff7dd65e35974093282a5,SMART: S ELF - SUPERVISED M ULTI - TASK PRETR A IN - ING WITH CONT R OL T RANSFORMERS,Yanchao Sun,2022,,0
02d07c0d4e93e6c84357c0360bfc6229afce445f,Adversarial Attacks on Feature Visualization Methods,Jonathan Marty,2022,,0
4b43bc712cab78ef05d40730ff2db203fdd6c153,Incremental Processing of Principle B: Mismatches Between Neural Models and Humans,Forrest Davis,2022,,0
f65e11bae859d318f286b36caaab8ff6ac11a771,PLM-based World Models for Text-based Games,Minsoo Kim,2022,,0
282000982c8f7752cbf9c4d226c0b22d2875ff9c,Multimodal Coreference Resolution,Alejandro Santorum Varela,2022,,0
9715be0a94c9b05bafe299cbfb4f846453bfd2ab,Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance,Yasaman Razeghi,2022,,2
178bbb0f9372e75d882419572360d693948e8472,Benchmarking data science: Twelve ways to lie with statistics and performance on parallel computers.,Torsten Hoeﬂer,2022,,0
70d89d380ca5d20564e1dd8ed2f4c59f5c7b3656,HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation,Hamish Ivison,2022,,2
e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da,Pitfalls of Static Language Modelling,Angeliki Lazaridou,2021,,46
d66e80224cda0c1d5a4c1be3798df6a6bfe3713c,GPT-3 for Few-Shot Dialogue State Tracking,Nicholas Pezzotti,2021,,1
e05efed686facda35aaac2fa9a69ba410e387ec0,ZeRO-Offload: Democratizing Billion-Scale Model Training,Jie Ren,2021,,0
80bbaf09b574ba42c8441e29ab476b50e13e996d,Emergent Unfairness: Normative Assumptions and Contradictions in Algorithmic Fairness-Accuracy Trade-Off Research,A. Feder Cooper,2021,,3
1f3d593ac53336a807e7381611f7fe6c7de42a8d,GSHARD: SCALING GIANT MODELS,Automatic Sharding,2021,,0
40c3327a6ddb0603b6892344509c7f428ab43d81,Documenting the English Colossal Clean Crawled Corpus,Jesse Dodge,2021,,35
f6264b11ec0dd9133f1d88a5288a3267a93182f8,What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study,Anton Raichuk,2021,,90
52555e649aa43a60554863af43d99fa0e3932566,Easy and Efficient Transformer : Scalable Inference Solution For large NLP mode,GongZheng Li,2021,,0
75aed4f0969fc801f39c4ed5e6305325947a142b,Very Deep Graph Neural Networks Via Noise Regularisation,Jonathan Godwin,2021,,16
c6499788267a24b8616f7ea444fc91577160bf25,C L ] 4 F eb 2 02 1 Knowledge-Aware Language Model Pretraining,Corby Rosset,2021,,0
98bee7c7b141084cd6668b47262befc81617d4c3,PFP: Universal Neural Network Potential for Material Discovery,So Takamoto,2021,,1
0c36366413f08c6f635af6897627a3f113f6115a,Catformer: Designing Stable Transformers via Sensitivity Analysis,Jared Davis,2021,,8
3357c0636ea7b1f3df990076ed6675c34000855d,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,Conglong Li,2021,,19
5861636a1c3c0e9ccf6f683fe70b3b5182ef1697,Whale: Scaling Deep Learning Model Training to the Trillions,Xianyan Jia,2021,,6
69c71029b898de7bc1ff7e9dab77d7fd8d3bb759,On the Role of Corpus Ordering in Language Modeling,Ameeta Agrawal,2021,,3
9bbe1ca7ff0e6c3f95f20063cc733b481445ef09,Self-Supervised Learning to Prove Equivalence Between Programs via Semantics-Preserving Rewrite Rules,Steven J Kommrusch,2021,,3
a0033c2b38d289fd71194eb830b14d0db8f5a18b,Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning,Yujia Qin,2021,,26
921c1216edbf6b2931b15874f24847ff1007ad8c,EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks,Frederick Liu,2021,,18
b2474a00d7de3373bab934c09acef1994fa82207,Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages,Kelechi Ogueji,2021,,92
d52302d302499cfd3430508b61b0f50fe7fce90c,A Systematic Investigation of Commonsense Understanding in Large Language Models,Xiang Li,2021,,6
0fd9c127463d62450337658fb59b96d0acbc5b41,F OR LARGE NLP MODEL,GongZheng Li,2021,,0
e58d8b717dfce4f37c5a4009c74c15915411442d,Dense-to-Sparse Gate for Mixture-of-Experts,Xiaonan Nie,2021,,21
3ede1108e6cbad247b875aa46b4541967a83b980,Limits to Depth Efficiencies of Self-Attention Supplementary Material,Yoav Levine,2021,,0
a0f482a50c5ead1bb054540aa48440fb92eaccf3,Empirics on the Expressiveness of Randomized Signature,Prafulla Dhariwal,2021,,0
d779908d8358febf9643e777fd72f4cb1df61620,Domain Randomization for Deep Reinforcement Learning in Self-Driving Environments,,2021,,0
8f3625dcb33f055be88c5542b8308b910b7dae31,Towards a Persona Aware Conversational Agent,Nuno Ventura de Melo,2021,,0
89d50d1f2b98fdee5ea7af86447c5bd39603a78d,Phase Transitions in Neural Networks,L. Carroll,2021,,0
eb870f3f203076f7769dedf6d3923f0a2cb5566e,O N E VALUATING AND I MPROVING THE E FFICIENCY OF D EEP N ETWORKS,Davis W. Blalock,2021,,0
40a8996eef0a4a17579f1798764632ad0ad3996f,Deep Transfer Learning & Beyond : Transformer Language Models in Information Systems,Ross Gruetzemacher,2021,,0
f3bb90070934727f5cad4fe8797bb9701fe1f010,Projecting Heterogeneous Annotations for Named Entity Recognition,Rodrigo Agerri,2020,,6
b5b2dad83e10c335e4c079e69b7a11fb79cc10b1,KungFu: Making Training in Distributed Machine Learning Adaptive,Luo Mai,2020,,43
be637822605968255e0157e856c5cd22949ac363,Reservoir Transformer,Sheng Shen,2020,,8
011cdf321549ec4775618357f907fa36902d455d,PRETRAIN KNOWLEDGE-AWARE LANGUAGE MODELS,Veselin Stoyanov,2020,,2
13a76224c65d60435006b3c58527cd5a7c4dabbc,LENGTH-ADAPTIVE TRANSFORMER: TRAIN ONCE,,2020,,0
acd9db8c22ddd6402bc21b48c9b154a78fb23f5e,Learning Robust Representations using a Change Point Framework,Ame Osotsi,,,0
bbd322611857a830b4c616ee609ea2aa65946e62,Communication-efﬁcient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence,Yuhao Zhou,,,0
b7fe45e6e0a973bc3c054a056fd96cdd3f96c1d9,Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension,Henry Kvinge,2023,02,0
72c4e3ea74da85a52e12bca37a505c4c0762809e,Topological obstructions to autoencoding,Joshua D. Batson,2021,02,27
075d94936309f7e631bc7fec36ef1436f8f071ab,Distributional Generalization: A New Kind of Generalization,Preetum Nakkiran,2020,09,33
7f09792bda83709373928fcaeb843472eecf386d,Partial local entropy and anisotropy in deep weight spaces,Daniele Musso,2020,07,3
7de55005c67dd71eba310b19952c7eb6f89e7886,Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?,Romain Egele,2023,07,0
50524ed9698016c96482a460fd2efc0a16908d2f,Mapping the evolution of regional brain network efficiency and its association with cognitive abilities during the first twenty-eight months of life,Weixiong Jiang,2023,,0
906762e1c3f81555009f990a22ea65abf77ed3b7,Dataset Growth in Medical Image Analysis Research,Y. Landau,2019,08,15
bb4b48ed0a71e545d11fb42c2e6bf7dbf62b7197,The Trend and Outcomes of Laparoscopic Appendectomy for Complicated Appendicitis in Nepal: A retrospective study from 2014 to 2018 in a University Hospital.,Y. Shakya,2019,,1
10227381a2220bbf5d9e8c028077adc99fad2a01,Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms,Elvis Dohmatob,2023,08,0
178c36de459f5f46284ba0aaf71736ce62be353b,Utilizing Unsupervised Learning for Improving ML Channel State Feedback in Cellular Networks,Bryse Flowers,2023,,0
fb6ecf67c275fe775a12ad4ddf2c8dc7cfad1348,Unifying Grokking and Double Descent,P. Battaglia,2023,03,4
0963909891d794d76a4bf623dba049097f33b328,Meta-Learning the Inductive Bias of Simple Neural Circuits,W. Dorrell,2022,11,0
40b672e817624a2357f279cd3d92aa0ae8ddd330,Data Feedback Loops: Model-driven Amplification of Dataset Biases,Rohan Taori,2022,09,9
7bc39510b01e867769d0cec8e7f5a160391ff949,How critical is brain criticality?,J. O’Byrne,2022,,52
290adead4fbf89559fc79464b814d47a0d0720a5,"Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting",Neil Rohit Mallinar,2022,07,19
7f4b73d667077ef8e7438f3bb0d7f9c9aa79fe4b,"A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta",Maksim Velikanov,2022,06,3
b069ff1ac62b1c64d2a0c56812ebc798c98b296c,Fast Finite Width Neural Tangent Kernel,Roman Novak,2022,06,26
bebc4b22262c5f6c550af6e9c144a48b32c5e4aa,Understanding out-of-distribution accuracies through quantifying difficulty of test samples,Berfin Simsek,2022,03,2
006092fa578c4ec9fa1403c115825515c6720c3e,Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions,Maksim Velikanov,2022,02,3
24af6f13f5dad9304120d1803da8304a982241ca,Causal BERT: Improving object detection by searching for challenging groups,Cinjon Resnick,2021,,6
7d403df62361cecbf0124cc633f19259a7fe2cfd,The External Validity of Combinatorial Samples and Populations,Andre F. Ribeiro,2021,08,5
4c03ff54f0aaadcc95385dcda49923180759c697,Deep multi-task mining Calabi–Yau four-folds,Harold Erbin,2021,08,8
23217d826dbe0c37976d32a73344a43c975ea627,Learning Curves for SGD on Structured Features,Blake Bordelon,2021,06,8
e3a1de3ffcd3c936da3c2e4c8c1478c725503ec7,A Theory of Neural Tangent Kernel Alignment and Its Influence on Training,H. Shan,2021,05,6
363aed360efd12d03e38f567312bbea8e49da491,Universal scaling laws in the gradient descent training of neural networks,Maksim Velikanov,2021,05,7
732597e57b3de0a2884a2a6b39156c55c1897b42,M ETA -L EARNING THE I NDUCTIVE B IASES OF S IMPLE N EURAL C IRCUITS,W. Dorrell,2023,,0
b60e981fd5268a290dbf4e7d8346036396ecaf7d,M ETA -L EARNING THE I NDUCTIVE B IASES OF S IMPLE N EURAL C IRCUITS,Maria Yuffa,2022,,0
368cfecf18230ce8de069f4dd4af55c2f458566a,Rapid Feature Evolution Accelerates Learning in Neural Networks,H. Shan,2021,,6
b23d463fa147fe7322171c9c233cfbb8cba80fec,Explicit loss asymptotics in the gradient descent training of neural networks,Maksim Velikanov,2021,,8
a8f103c9af08697d7a5bf509325f96c1e15f490b,L EARNING C URVES FOR S TOCHASTIC G RADIENT D E - SCENT ON S TRUCTURED F EATURES,Blake Bordelon,,,0
a8b5a20e3a983d96f9dea6fc38e77b155e7bd94f,Language Versatilists vs. Specialists: An Empirical Revisiting on Multilingual Transfer Ability,Jiacheng Ye,2023,06,2
604b4b4d55587ab6cdc7a6a70e66a193439f4445,On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training,Jieyu Zhang,2023,05,2
1ade52a56ef4e836c445446f74b15e413e53e271,Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models,Junmo Kang,2023,05,6
4220bffb01045faede80cf8eae76605658ccdcc8,A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text,Jessica Nayeli López Espejel,2023,06,2
eef5b8f3c4e60d596a04101d8261c222ab739861,Fine-Tashkeel: Fine-Tuning Byte-Level Models for Accurate Arabic Text Diacritization,Bashar Al-Rfooh,2023,03,1
3b16a709a5b18e52b0b6741cbc3c0e68a03ecd8e,The unreasonable effectiveness of few-shot learning for machine translation,Xavier García,2023,02,24
3aa2c10dd6c72267ea8a622c8f30b3c9240d5fab,Frozen Pretrained Transformers as Universal Computation Engines,Kevin Lu,2022,,21
f097fb79d7ab0743d96167d419788d4aad8197d7,Empirical Evaluation and Theoretical Analysis for Representation Learning: A Survey,Kento Nozawa,2022,04,3
3ebdb4df7667ea89dbee3dfe9b9ace3a2afed3f0,Revealing General Patterns of Microbiomes That Transcend Systems: Potential and Challenges of Deep Transfer Learning,M. David,2022,,4
697dac5b931b903c263c06688756a2c5c54434d7,"Improving Protein Function Annotation via Unsupervised Pre-training: Robustness, Efficiency, and Insights",David Dohan,2021,,5
c1075fa775bb8c5de915dc9ab6dacc79706a9613,Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations,Josh Beal,2021,08,16
c9a079b09f4f27bbcbf5175a4d79c96f38b9be77,Disentangling Transfer and Interference in Multi-Domain Learning,Yipeng Zhang,2021,07,1
4b1db6ebbdfcfe8ef67c5db511b6ad169fcc8f7f,The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning,Anders Andreassen,2021,06,51
1cf50a2e906dc89463d7eab827de9a3c371e7c53,"Generate, Annotate, and Learn: NLP with Synthetic Text",Xuanli He,2021,06,14
bcb40a7f2009259e660e14d12ec75bae1c184b1e,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,Zhengxuan Wu,2021,04,4
3544650f12a05cf4ed3bf2f7e22fc5c02fcabf50,Pretrained Transformers as Universal Computation Engines,Kevin Lu,2021,03,172
489666f4c11787b679b36238dee95b63248ed60a,Training Larger Networks for Deep Reinforcement Learning,Keita Ota,2021,02,23
021bbcefc993c389bad6c1daefd8ff92d0fc2441,Contrastive Code Representation Learning,Paras Jain,2020,07,93
98fef026effaf736eb93e7dcb06edf89d953c637,Transfer learning with weak labels from radiology reports: application to glioma change detection,Tommaso Di Noto,2022,,1
c7bef8d87a4ebdaf8ddedd91678efa789fd6f4b2,Attention for Compositional Modularity,O. Ostapenko,2022,,2
9dc031c2cd2f70dd0ede895c00fb89ebf10b501b,"Generate, Annotate, and Learn: Generative Models Advance Self-Training and Knowledge Distillation",Xuanli He,2021,,12
3169e370ba3cbe0b8738f47c3e59539bdfd51ed5,Learning Self-Supervised Representations of Code Functionality,Paras Jain,2021,,0
a1cb8499465117e3ad7e248ae0a6a8107c2958aa,Quasi-synthetic data generation for camouflaged object detection at edge,Rohan Putatunda,2023,,0
12d362946dbe9bc26e1b6902cf96e354587644d3,PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data,Roei Herzig,2022,12,6
0a2d9ceddd03da335a2676b1c6b53803a1283a42,Scalable Modular Synthetic Data Generation for Advancing Aerial Autonomy,Mehrnaz Sabet,2022,11,0
ffdca4d1239955fafcda72e273611fb033b2c48f,Task2Sim: Towards Effective Pre-training and Transfer from Synthetic Data,Samarth Mishra,2021,12,12
ad7c907da312fc7b7c8974a8496471dac67beaf7,On Pre-Training for Federated Learning,Hong-You Chen,2022,,19
d4085ae0f004624a3141734d3a88a9ebbc803a55,Anchor Points: Benchmarking Models with Much Fewer Examples,Rajan Vivek,2023,09,0
094f5c1e0cd4a5d22a637c65cf6afd6c7313f124,Examining the Effect of Pre-training on Time Series Classification,Jiashu Pu,2023,09,0
58f581ea709591cf6a23f66bb9ea6f80f30da4e6,FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate Representations,Changan Niu,2023,09,1
89dbadac7e39460e9d5cc7cd9c996e8f31be0899,An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning,Grégoire Petit,2023,08,0
aba7ec78648218577284ac4f5addea66e525c3ec,Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?,Megan Richards,2023,07,0
918617dbc02fa4df1999599bcf967acd2ea84d71,"Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution",Mostafa Dehghani,2023,07,4
9d460930d9b5d12a65ff2b3efa23047ec75fbca1,The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter,Ajay Jaiswal,2023,06,2
483f9a982cca661eaf755f32e2c148d49907cede,Quantifying the Variability Collapse of Neural Networks,Jing-Xue Xu,2023,06,0
4d10d876ba202ac40748257fbb4cb19cfe47e203,Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How,Sebastian Pineda Arango,2023,06,0
afa7f4ccf05e4b7a0ddc977c739a6aa16094d0da,Continual Learning with Pretrained Backbones by Tuning in the Input Space,Simone Marullo,2023,06,0
41a940aa45b5ea5119bd1e2cf84a6dd34bcb93e9,No Free Lunch in Self Supervised Representation Learning,Ihab Bendidi,2023,04,1
6baabc2f8824a781ded3d620fb847b6d717f5b4d,Towards Efficient Task-Driven Model Reprogramming with Foundation Models,Shoukai Xu,2023,04,2
1d3e99550eae0eceef728f4750d5d413af7cce43,The effectiveness of MAE pre-pretraining for billion-scale pretraining,Mannat Singh,2023,03,8
0ae93d5c8528328c9e59fa92e86ea9c6de4a62d0,The Role of Pre-training Data in Transfer Learning,R. Entezari,2023,02,5
6f757cec2b878f13ad5fbd883380b77338c43903,Data efficiency and extrapolation trends in neural network interatomic potentials,Joshua A Vita,2023,02,1
61e721334296ebfbbf6443b5ed9eb8c83b708c95,Scaling Vision Transformers to 22 Billion Parameters,Mostafa Dehghani,2023,02,117
76586f7ed4303514e7dbd02042f945ac8049fb26,Does progress on ImageNet transfer to real-world datasets?,Alex Fang,2023,01,10
40cfe84ce5ce0d24f4ac189f8aac441992ab3233,Exploring Efficient Few-shot Adaptation for Vision Transformers,C. Xu,2023,01,6
ac667aa10db56b1483640aefeca05f2e25d8353b,Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization,Alexandre Ram'e,2022,12,16
9671054b27698aafa4ea680480dc2ecdd25a14d6,OAMixer: Object-aware Mixing Layer for Vision Transformers,H. Kang,2022,12,0
ecd701ca06ae6c013162acf54cecb3bbbfdbdc8e,Transferability Estimation Based On Principal Gradient Expectation,Huiyan Qi,2022,11,1
272e77e2d4d69db2ef0160ef44f5e0abde03eaa6,A Survey of Research Progress and Theory Foundation in Large Model,Dong Xiaofei,2022,,0
ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c,Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information,Weijie Su,2022,11,12
8267de899404bc42df3f9893856b97d44c54de32,Cross-Reality Re-Rendering: Manipulating between Digital and Physical Realities,Siddhartha Datta,2022,11,0
690e60432b2fa393089f6b0ef72d6c7c2d927bc0,Human alignment of neural network representations,Lukas Muttenthaler,2022,11,13
1b76caf03527cab16052db606455d9827a4883e3,Where to start? Analyzing the potential value of intermediate models,Leshem Choshen,2022,11,15
12941d1b35059a85b3a135a8aa8f6cc3dd630e39,Changes from Classical Statistics to Modern Statistics and Data Science,Kai Zhang,2022,11,0
9a1d94a930168918a1a1e1939b089d16d58d7865,A Kernel-Based View of Language Model Fine-Tuning,Sadhika Malladi,2022,10,14
30e35301e6ff50f414a58fda2be82574aee98f79,Multi-modal Learning Algorithms and Network Architectures for Information Extraction and Retrieval,Maurits J. R. Bleeker,2022,,1
a6e0a6d58783e93a5f2ecf1f3d29490d9ec1698a,Adversarial Lagrangian Integrated Contrastive Embedding for Limited Size Datasets,Amin Jalali,2022,10,3
b48ae204812de284ca2299c9e70ed144ae89a3a1,Under the Cover Infant Pose Estimation Using Multimodal Data,Daniel G. Kyrollos,2022,10,1
cc5ec980b7b730f9809d2bfdf24ff44e4855f255,Intersection of Parallels as an Early Stopping Criterion,Ali Vardasbi,2022,08,1
ecc6d11de4afa98687fb99bdd04b2fcda72805bd,UFO: Unified Feature Optimization,Teng Xi,2022,07,8
3a096a3d67348640084c209abb09f79447fbbc11,Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset,Peter Henderson,2022,07,32
b09420b30fc093d63fb2ee1aac26c71da81da437,LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks,Tuan Dinh,2022,06,24
9904ff3ba8df83f3e2377e45de7746aa1ddece7a,Deep transfer learning for image classification: a survey,J. Plested,2022,05,14
0e11eeafd77ebab9361cd5b59b0cf5e53488c8e1,GreaseVision: Rewriting the Rules of the Interface,Siddhartha Datta,2022,04,3
14a3aae8060338e3fbefc2af694890b019874d4f,Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations,P. Kirichenko,2022,04,107
aa8f3e081ad2869c9469e2726364bdae0d9bdc7f,Fusing finetuned models for better pretraining,Leshem Choshen,2022,04,25
28c07ddb22f305d8e97459ef7351cc3cb9126cc8,How stable are Transferability Metrics evaluations?,A. Agostinelli,2022,04,10
e64914f9d4d2eabf3fa1352e2d132364a5539aaa,Understanding Contrastive Learning Requires Incorporating Inductive Biases,Nikunj Saunshi,2022,02,50
b7dc007054cf17dea3b22a2d1e71ba4cc8606648,Revisiting Weakly Supervised Pre-Training of Visual Perception Models,Mannat Singh,2022,01,54
da18398274bec87d3568acfd6aba6977be7ba6b8,Transferability in Deep Learning: A Survey,Junguang Jiang,2022,01,46
cb7e9e6a68f64972cb42fc790c3a755175b80828,Weakly-Guided Self-Supervised Pretraining for Temporal Activity Detection,Kumara Kahatapitiya,2021,11,1
00d7dfde1cd69d2247e8c36d10807b0dee9656d7,"PolyViT: Co-training Vision Transformers on Images, Videos and Audio",Valerii Likhosherstov,2021,11,47
5bcc379da187b69d705a81e93bf5ddbb90cda1b1,No One Representation to Rule Them All: Overlapping Features of Training Methods,Raphael Gontijo Lopes,2021,10,40
b8cb9c0b02da96a9908665ae67692a6da4dd25a4,SCENIC: A JAX Library for Computer Vision Research and Beyond,Mostafa Dehghani,2021,10,45
7668b23aadf43bebe5e2d3abf37938b44bd16200,WebQA: Multihop and Multimodal QA,Yingshan Chang,2021,09,37
62bc27534e9915307acced33c1601b525af5281c,Why Do Better Loss Functions Lead to Less Transferable Features?,Simon Kornblith,2020,10,54
0e6f87710b90d080cd609ee52d7a6ebc729a1c06,THE ROLE OF LANGUAGE IMAGE PRE-TRAINING DATA IN TRANSFER LEARNING,R. Entezari,2023,,0
4131c966b3470847c3ab87babc8b7e521b8a0dce,GreaseVision: Rewriting the Rules of the Interface,Siddhartha Datta,2022,,0
0eb74aa2d64b99f0917697e18faee53daac417d0,How well do contrastively trained models transfer?,M. Shariatnia,2022,,1
fca2e82075b590011e597778b3054a5964af274e,"Pre-train, fine-tune, interpolate: a three-stage strategy for domain generalization",Alexandre Ramé,2022,,2
8305e6114d9c7d0394f09fadbb55a59136c48794,Exploring Spurious Learning in Self-Supervised Representations,Alican Bozkurt,2022,,0
88bc9880ba58929c33220e70b8b58b92c4c5fb94,Self-supervised Pretraining with Classification Labels for Temporal Activity Detection,Kumara Kahatapitiya,2021,,6
e754e647ce86774040b1f05706e9809f18929a6a,Agent Instructs Large Language Models to be General Zero-Shot Reasoners,Nicholas Crispino,2023,10,0
004699d8ed6d1929e484aba461391fadb1f5b0a7,"Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly",Herbert Woisetschlager,2023,10,0
e9ca67b67f4b43650baaef0d03013683eeb4528e,Large Language Models Can Be Good Privacy Protection Learners,Yijia Xiao,2023,10,0
1153b05709be3dd0dc5e5d4d4ee8f631995ad768,CAT-LM: Training Language Models on Aligned Code And Tests,Nikitha Rao,2023,10,0
a5d27bf7a2155d4ca016565a78b52ee90f81624c,Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning,Mustafa Shukor,2023,10,0
4a284a665acae79b8ccbfcf5819b1620eb62c9ae,Small Visual Language Models can also be Open-Ended Few-Shot Learners,Mohammad Mahdi Derakhshani,2023,10,0
aa311c18cf863af7e0c7784171d808dec8777ce9,"Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",Zhihan Liu,2023,09,0
c182bcd5f37f37fea9f3dad856dc381e0f19578a,Augmenting LLMs with Knowledge: A survey on hallucination prevention,Konstantinos Andriopoulos,2023,09,0
5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0,Qwen Technical Report,Jinze Bai,2023,09,0
0652b0dc23650948569f8cfe8562524951ccdf27,ComPile: A Large IR Dataset from Production Sources,Aiden Grossman,2023,09,1
6f75e8b61f13562237851d8119cb2f9d49e073fb,Can LLM-Generated Misinformation Be Detected?,Canyu Chen,2023,09,0
29f032fc875576b5c3c6b1c2d76af8639bacfb88,OpenChat: Advancing Open-source Language Models with Mixed-Quality Data,Guan Wang,2023,09,2
c6a12d9a16b8653032c2d549ec56d2c44413f821,LMDX: Language Model-based Document Information Extraction and Localization,Vincent Perot,2023,09,0
b06e1a2c84fb3bff03b10283bc863f007f5483b6,The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics,Hugo Dalla-Torre,2023,,18
8f088f535419ea054f9cc6d073e53eed9715fe5a,Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT),P. Kavehzadeh,2023,09,0
91b68391df0b16d22bffbbf4d0c09f13dee36561,GPT-Lab: Next Generation Of Optimal Chemistry Discovery By GPT Driven Robotic Lab,Xiaokai Qin,2023,09,0
2ff694e20f492a7acf7fd0646c5e1576f0b3c901,C-Pack: Packaged Resources To Advance General Chinese Embedding,Shitao Xiao,2023,09,0
34f721457b92155a5f5ccaf5201c1a9f0f6f99be,An Assessment of ChatGPT on Log Data,Priyanka Mudgal,2023,09,1
3ed8eddc1a611540a046c1569fd577d1f7919012,RAIN: Your Language Models Can Align Themselves without Finetuning,Yuhui Li,2023,09,2
22ab4219371366a4e890382bc0ca606130840ca7,Statistical Rejection Sampling Improves Preference Optimization,Tianqi Liu,2023,09,1
02838f3cd9c7bbb679968ba593699f920ef2f4fc,Balanced and Explainable Social Media Analysis for Public Health with Large Language Models,Yan Jiang,2023,09,0
e5d0a261a8e224ab4ed9fada3e6cbb88429a0a9e,Evaluating the Deductive Competence of Large Language Models,S. M. Seals,2023,09,0
93f55f461f2a368328a7380101f759790733804a,HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models,Guijin Son,2023,09,0
ee7b871213e1deafadea4b7752467b5c5ab1b9fb,GPT Can Solve Mathematical Problems Without a Calculator,Z. Yang,2023,09,1
e2f1f04f648a8863d11439aa4c80ee65d6caccda,ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models,Chenliang Li,2023,09,2
dca53a5e349d8853f21680ab3291f6fb9afe75fe,BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge,Xiangru Tang,2023,08,0
b931b242f40a032b9ae7dae9d9fc10c6ab90695e,Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models,Hritik Bansal,2023,08,1
6660a20e26e9e8c9916ffdc488e925e313605d8d,Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA,Yang Zhou,2023,08,0
1dca108e88dbf69859b329b9e5cb00f36851a738,ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding,Omer Veysel Cagatan,2023,08,0
c2cbb952e896cce72193326051c1400d82bc23a8,"Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",Tyler A. Chang,2023,08,0
894ed1aba8e42a4ec27ba53ecde383b14c5128ca,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",Kaiyuan Gao,2023,08,0
9bd608dc60c7d2adabd3a2f9c29d2751145aa511,MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records,S. Fleming,2023,08,1
5a4f1a9a604a5a930d47c49ea50ea3546980333d,AIGC for Various Data Modalities: A Survey,Lin Geng Foo,2023,08,0
0b0debb710366cdff461938c80763eace1651af6,Code Llama: Open Foundation Models for Code,Baptiste Rozière,2023,08,42
39f2636fee2545ac59d356e52c643fabdc65e4f2,kTrans: Knowledge-Aware Transformer for Binary Code Embedding,Wenyu Zhu,2023,08,0
e5daadcac08ebfc30759260cbd43812d4e3bc775,DLIP: Distilling Language-Image Pre-training,Huafeng Kuang,2023,08,0
28ba105e12eb1cdbf5d9423f105771de07037f8c,Prompt-Enhanced Software Vulnerability Detection Using ChatGPT,Chenyuan Zhang,2023,08,0
bde98ab6970961b6a1bde2ab4c5ce83f7060b3ef,Cabrita: closing the gap for foreign languages,Celio H. N. Larcher,2023,08,0
2f26d6935face2856d71cc1a9147255ea6bf74be,On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers,Laura Cabra-Acela,2023,08,0
9a4765547cb43ab221fe262df7405f6795557d8c,Efficient Benchmarking (of Language Models),Yotam Perlitz,2023,08,0
000f964393dafe113a8e66734d63b2a145844159,Large Language Models for Software Engineering: A Systematic Literature Review,Xinying Hou,2023,08,7
53adaabb301a91c606dfef8fdae97b8b7c031e7b,Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems,Zeinab Taghavi,2023,08,0
3b88526a0f0337e3a6b632b4af8fd0882eb4b470,FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,Liwen Zhang,2023,08,2
dd18782960f9ee4c66b79e1518b342ad3f8d19e7,WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct,Haipeng Luo,2023,08,13
e3fd89a7f6b28973cfc68bfc51caebd8fb93f0bc,BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine,Yi Luo,2023,08,2
9f859726b3d8dffd96a1f55de4122617751cc1b4,Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment,Rishabh Bhardwaj,2023,08,4
d5bf55f810cf66d3e70155bca12420d38d0601ad,Scope is all you need: Transforming LLMs for HPC Code,Tal Kadosh,2023,08,0
37d6e93ecc32c5f4e68a2273ce273c765c42805c,Detoxify Language Model Step-by-Step,Zecheng Tang,2023,08,2
7ac38c3398f2696754bec69f296468e7a8237a64,FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs,Young Jin Kim,2023,08,1
1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc,Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification,Aojun Zhou,2023,08,10
b946f3ee813aa671aff7db0a0c840049acb83662,"The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models",Abi Aryan,2023,08,0
a88c44a25088384d070bd6cba9048bbf896a4503,Optimizing transformer-based machine translation model for single GPU training: a hyperparameter ablation study,Luv Verma,2023,08,0
868b7ea007786ec07af2d8ae96f89e04508dd167,Thinking Like an Expert: Multimodal Hypergraph-of-Thought (HoT) Reasoning to boost Foundation Modals,Fanglong Yao,2023,08,2
507acddb0b7f36b83fd7c8bff2f121eb506ac8fb,Cumulative Reasoning with Large Language Models,Yifan Zhang,2023,08,4
193955704f66923ac20a664bd184ed4663b2bdf9,Continual Pre-Training of Large Language Models: How to (re)warm your model?,Kshitij Gupta,2023,08,1
5c6abca085e6a0047339d57d849886e693fe62bf,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,Weihao Yu,2023,08,6
53947b7f42b7bf6c59d2f72bab2829342154d2ac,A Survey of Spanish Clinical Language Models,Guillem García Subies,2023,08,0
8706782e08baa3dc4980f44f53b552937dbc7ea4,Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty,I. Timiryasov,2023,08,0
c65a64ea1be8dd654e2685f9bfea4c5118a98804,Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation,Zhiqiang Yuan,2023,08,3
1b5b701f658ef277fd9f37bf4b91d84d44d54c25,Bridging the data gap between children and large language models,Michael C. Frank,2023,,2
b2bcb59d75fd185a259d272e13005fd64bf9b213,DEBI-NN: Distance-encoding biomorphic-informational neural networks for minimizing the number of trainable parameters.,L. Papp,2023,,0
8cdc2970e62f50b114e7e2cbb13c1c06df8e974b,Onboard AI: Constraints and Limitations,Kyle Miller,2023,,0
0fe88452660cb8a0e37f54bcd44f3cd6504354b5,"Unified Model for Image, Video, Audio and Language Tasks",Mustafa Shukor,2023,07,6
a8e07e045408bba0027cc3f48005d9c31474cb03,The Hydra Effect: Emergent Self-repair in Language Model Computations,Tom McGrath,2023,07,4
45cc3ccc88b07bb7c25889c014806cc04ad73d9a,Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation,Zhiyuan Li,2023,07,0
58d1a002a0ff0aa40b6633f0a7073d48f1cdff53,Empower Your Model with Longer and Better Context Comprehension,Yifei Gao,2023,07,0
584ca135b61482fd89247113da87d784f738dbfa,Foundational Models Defining a New Era in Vision: A Survey and Outlook,Muhammad Awais,2023,07,6
fbdff3437393f724e5c8aac2340cfc7245b536f0,Opinion Mining Using Population-tuned Generative Language Models,Allmin Pradhap Singh Susaiyah,2023,07,0
2364675a0b5de9fdf8c01956b942dca83a399032,In-Context Learning Learns Label Relationships but Is Not Conventional Learning,Jannik Kossen,2023,07,2
75059feaaec7dd0a8810d8f4ec6985f5d00b73d5,A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks,Yanis Labrak,2023,07,1
03ecf5342f17e9d829d6cc5a05cadc5815691b05,Self-Attention and Transformers: Driving the Evolution of Large Language Models,Qing Luo,2023,,0
20d448a8712238ea34d9a18287e3bf05bc61dd2c,"Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa",Shriyash Upadhyay,2023,07,0
67188a50e1d8a601896f1217451b99f646af4ac8,Towards A Unified Agent with Foundation Models,Norman Di Palo,2023,07,9
1c0c13edd4442ceb7eac70bbcaebaf84512f9a3c,Overthinking the Truth: Understanding how Language Models Process False Demonstrations,Danny Halawi,2023,07,9
77f02ff24909896856fec410968aef7999c29440,Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla,Tom Lieberum,2023,07,2
5b0fe50dc6df8f4eba13f8177dcd4bbe5a2b0e23,A Survey of Techniques for Optimizing Transformer Inference,Krishna Teja Chitty-Venkata,2023,07,1
acb17a87b22dd61e9037e4633604d4f68a4020ef,A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge,Zhecheng Sheng,2023,07,0
7c2c35ad8ad3f146f07521aa00c2747c6560bffb,Generating Efficient Training Data via LLM-based Attribute Manipulation,Letian Peng,2023,07,0
b86a3606dd3eeabe6504a1d2d0d737ee2d6033c7,A Comprehensive Overview of Large Language Models,Humza Naveed,2023,07,6
d988eb48e8b4e471f5df9d081bfc32db0781e6bf,Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding,Seongjun Yang,2023,07,0
d84a9d48362cb69ef175cdfc9908a6ffbd1aa936,BLUEX: A benchmark based on Brazilian Leading Universities Entrance eXams,Thales Sales Almeida,2023,07,1
85d9151aa2efd0cbe822e403138cfe49f9536703,SITTA: A Semantic Image-Text Alignment for Image Captioning,Fabian Paischer,2023,07,0
08be17fe36c51ac84ab5cb87cf5805a93fb60341,Continual Learning as Computationally Constrained Reinforcement Learning,Saurabh Kumar,2023,07,1
8a9846283e556406b0e78b203c087c4821071137,Mass2SMILES: deep learning based fast prediction of structures and functional groups directly from high-resolution MS/MS spectra,David Elser,2023,,0
7ea9e912b5c9644f7a429b02839e2b9fb1b49235,Style Over Substance: Evaluation Biases for Large Language Models,Minghao Wu,2023,07,5
e89b4ef0f0282327085078058557c04812aa4d35,Scaling In-Context Demonstrations with Structured Attention,Tianle Cai,2023,07,0
407ebe0b9a1795b44cffcb434788ccba5e11a81b,Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks,Zhaofeng Wu,2023,07,11
1aeffef1fff311e8a3d87db2d6c59cdd13463d7b,Beatrice: A Chatbot for Collecting Psychoecological Data and Providing QA Capabilities,Yeming Ni,2023,,0
3e45b642e9854d63e5405b289dcb9148dea91414,The Inner Sentiments of a Thought,Christian Gagné,2023,07,0
16160a4bdd0f239e47f120547e6ecee44636d5e8,JourneyDB: A Benchmark for Generative Image Understanding,Junting Pan,2023,07,3
efc694164312006c543ef745611348ef64e68dda,Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language,William M. Berrios,2023,06,6
d14aa448b17fdc8d4ea12b43ee1a2b1254c38703,Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models,Zaid Alyafeai,2023,06,3
beeef5fee63595ad33f0ee7357ed44bfe92fc650,FLuRKA: Fast fused Low-Rank & Kernel Attention,Ahan Gupta,2023,06,0
36f7bc27c9a37eb337c35df4ae86f148e13d4e9a,Understanding In-Context Learning via Supportive Pretraining Data,Xiaochuang Han,2023,06,1
b69969bd91206a9f8f3d474df20caa500904143d,Fauno: The Italian Large Language Model that will leave you senza parole!,Andrea Bacciu,2023,06,2
7a6a298efb965ce9a351a3212f6f536e94dbbb03,Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step,Liunian Harold Li,2023,06,8
f9d29c08c4f4b9e098a655f6057e87d98bea49ab,Neural Algorithmic Reasoning Without Intermediate Supervision,Gleb Rodionov,2023,06,1
778bcdab223754054059f1c70d624c6907ed7c07,Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale,Matt Le,2023,06,16
f3b512db3759dfdabeabd37d55f27321feaf45db,Exploring the Landscape of Ubiquitous In-home Health Monitoring: A Comprehensive Survey,Farhad Pourpanah,2023,06,0
f0495105f3ba0ce45090d3fc1cf52b217794ab4d,AI could create a perfect storm of climate misinformation,V. Galaz,2023,06,1
b4043ebf549954522c80c47e35421f2b14ce03d7,The Cultivated Practices of Text-to-Image Generation,J. Oppenlaender,2023,06,0
24c4cbd0fa2d214e8ff914d92c9b37e45c4a6bbf,A Simple and Effective Pruning Approach for Large Language Models,Mingjie Sun,2023,06,11
ef5c3052dcff67abd9bbb699e637847d1bf2120c,SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling,Jesse Zhang,2023,06,1
0983883619a0ca597d055d0e58da2f514052913d,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",Chenyang Lyu,2023,06,14
863bde644a979f96914b28c24b01856b4fb479c9,Segment Any Point Cloud Sequences by Distilling Vision Foundation Models,You-Chen Liu,2023,06,7
ba3adfca2d111a5d61ad928a214c4715e3470d50,Anticipatory Music Transformer,John Thickstun,2023,06,0
e288a9ea03273578bb0ee893f4cc47f9094ca6f3,Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning,Nikhil Vyas,2023,06,0
454c8fef2957aa2fb13eb2c7a454393a2ee83805,WizardCoder: Empowering Code Large Language Models with Evol-Instruct,Ziyang Luo,2023,06,41
00557300321dc60998e0f42853f4bba52d6e53db,Revealing the structure of language model capabilities,Ryan Burnell,2023,06,1
529a0b3addc3805d69f870b46267b7c329d35a2f,Aisha: A Custom AI Library Chatbot Using the ChatGPT API,Yrjö Lappalainen,2023,,2
3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc,SqueezeLLM: Dense-and-Sparse Quantization,Sehoon Kim,2023,06,10
46ae37aac816ed375a8a6134c003dedd63bb82d4,Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,Yin Fang,2023,06,3
c581d2ad3b092a2cc152d0c6f55fd6320f78eb3a,A Survey of Vision-Language Pre-training from the Lens of Multimodal Machine Translation,Jeremy Gwinnup,2023,06,0
74538984e72a26254697d4e7eeeb169000cf762a,Valley: Video Assistant with Large Language model Enhanced abilitY,Ruipu Luo,2023,06,9
b8647eb447102c3df003a39b4768eacdc83d93d7,AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing,Asaad Alghamdi,2023,06,2
4bd35d344c635b05f97f4d749741d196ff541bf3,A Primer on Seq2Seq Models for Generative Chatbots,Vincenzo Scotti,2023,,0
7acde68b8c1e017035cea095b7ad1c84237624ee,Enclosed Loops: How open source communities become datasets,M. Choksi,2023,06,0
822f41fdb57c57db614a27936474644daf12b715,14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon,K. Jablonka,2023,06,3
ef933c2d89a0dd8130a2b066be48317f0bfb03bb,The Age of Synthetic Realities: Challenges and Opportunities,J. P. Cardenuto,2023,06,2
f4d543ff431359947bf41152ac01233b8062221f,In-Context Learning through the Bayesian Prism,Kabir Ahuja,2023,06,3
d8a083ec247aa30079ada320e30e8c1ce5f74ec2,PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,Kaijie Zhu,2023,06,31
0871fab693e77b8a4ae0d622fb046ba9f93a4f43,Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning,B. Sharma,2023,06,0
8236010c2ecc94d826be6010ff187fdc000e7df6,Deductive Verification of Chain-of-Thought Reasoning,Z. Ling,2023,06,8
2f130a320798dba1b13fa2e2822d42273e453038,An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models,Zhongbin Xie,2023,06,1
14347ae05bebfd9cb594ca2d2ebf07dd179962b7,Data-Efficient French Language Modeling with CamemBERTa,Wissam Antoun,2023,06,2
0f68281213b7f87494b1f79d4f50e37359692fb2,Towards In-context Scene Understanding,Ivana Balazevic,2023,06,1
3dcf2db20082b480c6c091eea025465cc4fe57a6,AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap,Q. Liao,2023,06,8
104c878d17a179e86ba094b221993cfdd3277943,Measuring the Robustness of Natural Language Processing Models to Domain Shifts,Nitay Calderon,2023,06,0
6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2,LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,Zechun Liu,2023,05,16
7ff2a68167be556de7eaa7da049d877a7fe05406,BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages,Wen Yang,2023,05,4
d3060876d9ad4e4e50e1c88a8c04186df00f24e2,A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets,Md Tahmid Rahman Laskar,2023,05,23
65fba6eb65b599f3465e1bf88b6507f7810beb09,Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective,Khanh Nguyen,2023,05,0
53cb191dc7a3b29d76022a7a1f718b40affd6bfb,Learning Capacity: A Measure of the Effective Dimensionality of a Model,Daiwei Chen,2023,05,2
f8f8267a2acd7598de6c15327f3953241901a62d,On Evaluating Adversarial Robustness of Large Vision-Language Models,Yunqing Zhao,2023,05,12
32dcd0887537cece54e214f531d2c384470b023f,Large Language Models as Tool Makers,Tianle Cai,2023,05,31
4e16bfc8ded08fbec67666869f39c043a6770946,Parameter-Efficient Fine-Tuning without Introducing New Latency,Baohao Liao,2023,05,0
f11044596cf2eaf59f83d82b8167b16ba6a08617,Emergent Agentic Transformer from Chain of Hindsight Experience,Hao Liu,2023,05,4
ea75117f34b168a20f2a4309ac2eb685ca6b1436,Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance,Yao Fu,2023,05,22
ce0154d9251f67c262512b6e598f3aa3ba9fe9a4,Passive learning of active causal strategies in agents and language models,Andrew Kyle Lampinen,2023,05,4
b6d1fdeda732f45f79c29f84b6ff2867371841b5,Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation,Haonan Li,2023,05,8
e6293458302583882c0633da040a83f216e4c0a8,Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers,Zixuan Jiang,2023,05,1
f45a3474bd38d65c1b2cc3342a64dacbf07f445a,Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models,Geewook Kim,2023,05,2
8f2f6d90de822888ec4e283788bc09917018e9d6,ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories,Heming Xia,2023,05,4
f3ca1504ab4cc14f491f07e5a8b38d93890551e1,Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering,Avi Caciularu,2023,05,2
fb0cd3f58d405b2d1511d6f6066affeb7d5a4902,Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs,Xiaoyang Song,2023,05,2
f834aed32f5531bfa426faab71878c549572500e,Large Language Models for User Interest Journeys,Konstantina Christakopoulou,2023,05,3
ac36b65a6fa4a9e84de051f0d3e9d50348fa4160,Lexinvariant Language Models,Qian Huang,2023,05,0
2ad8183c72a90511383a32ccaeea313eb85f4085,DetGPT: Detect What You Need via Reasoning,Renjie Pi,2023,05,9
ca210bdb512283c37deeab31396f6893cde269a0,Skill-Based Few-Shot Selection for In-Context Learning,Shengnan An,2023,05,1
19906b181ca01a8dbb9b19fff5f23f69b9675e5b,Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization,Jeonghoon Kim,2023,05,5
39a6842456e14bc3b116a40e9fb643cf3f7cc393,i-Code Studio: A Configurable and Composable Framework for Integrative AI,Yuwei Fang,2023,05,0
24082ddc3274124795d28f81a2f62b87932d63b5,Sequence Modeling is a Robust Contender for Offline Reinforcement Learning,Prajjwal Bhargava,2023,05,2
f9bfc6d9ba1665b73af3323d46c7642b852759ef,VideoLLM: Modeling Video Sequence with Large Language Models,Guo Chen,2023,05,11
506f571f4c3ef3c5c52761cd6b99400acd22ebd6,Observations on LLMs for Telecom Domain: Capabilities and Limitations,Sumit Soman,2023,05,1
57e90f4fae211f23e2c2d437e344a40ee9da87a0,Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline,Zangwei Zheng,2023,05,1
ff0a83c519068f867bb9e89200ff3317a7da5c39,Textually Pretrained Speech Language Models,Michael Hassid,2023,05,8
bc2e8b613335259598ea5c49aea270469e9a35ed,A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models,Oana Ignat,2023,05,5
a52dd1e900200e0733eea927edc7d6c27aeba187,TheoremQA: A Theorem-driven Question Answering dataset,Wenhu Chen,2023,05,11
68ab1c79158000db6deccb1509c58d2e1f1f8633,Evaluation of medium-large Language Models at zero-shot closed book generative question answering,R. Peinl,2023,05,2
017010b941d902a467f6d329ae5e74fd67e67912,LLM-Pruner: On the Structural Pruning of Large Language Models,Xinyin Ma,2023,05,19
33f9ddca2469bf4831dcab085e1620792b1a6a80,LLM Itself Can Read and Generate CXR Images,Suhyeon Lee,2023,05,1
a979975d1a0aea0e01423f092249cc3de575b6cd,X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models,Yixiong Chen,2023,05,2
2e6b6de08f459e2165b11ed8d2103916966b0fcf,Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback,Yao Fu,2023,05,15
cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e,Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare,Ding‐Qiao Wang,2023,,10
25f729d7773614846b412db3c6c2a3aab41ec409,M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models,Chuang Liu,2023,05,8
236c7dafea3df7ecffb5f18ec780d12f2f27d4b0,C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,Yuzhen Huang,2023,05,59
842da718e02adf973342c5e3aab354735b845bc9,Neural Boltzmann Machines,Alex H. Lang,2023,05,1
83a734dee0809a46bc7189c12cd9956927d14836,Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models,Zhimin Chen,2023,05,2
4683d3d6cb31111cf4499a199c0b036662b3eb32,Large Language Models are Zero-Shot Rankers for Recommender Systems,Yupeng Hou,2023,05,28
d6549ffc970d45cd29105ad9bacbaa61613f8b00,Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems,Lukas Stappen,2023,05,1
293221ad06a0256f820dd84a90b4a166378e2dac,Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation,Jizhi Zhang,2023,05,24
6a6cbcc596758dd214120a1d51528ce55daa333d,Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation,Zhen Guo,2023,05,0
0383e049e98c9eedbc61be728d4ef037300bbedf,Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach,Junjie Zhang,2023,05,22
ad95c3f95e44afbd4b97bf3261ac909f421c76e9,MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts,Xiaonan Li,2023,05,7
243ac5656c4f8ed6e1eb757b7145fb12b837c166,CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors,Peng-Hsuan Li,2023,05,6
3bee3bc3e94a01892cc87d9dcbc982d6860c6490,On the average-case complexity of learning output distributions of quantum circuits,A. Nietner,2023,05,6
390a17b29e7b68873eddce92a556df8d13b29f94,The Current State of Summarization,Fabian Retkowski,2023,05,0
5a900dbbb88c880a7cee7b6a2bf00a4c22da156f,Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness,Liangliang Cao,2023,05,2
0ae12d63f77f40b430f17c791a5191ff5fee5086,How Do In-Context Examples Affect Compositional Generalization?,Shengnan An,2023,05,4
f5d35779f90b8b8dcf798787bf68b4c294c6b5a4,Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era,Dong Zhang,2023,05,0
198320a8c26f01e1d48fb1cd385900a7f374d609,Faithful Question Answering with Monte-Carlo Planning,Ruixin Hong,2023,05,2
33c98573ea6f4f27e7b922c4a09d49a45901a641,GPT-RE: In-context Learning for Relation Extraction using Large Language Models,Zhen Wan,2023,05,9
aad167be3c902388ea625da4117fcae4325b8b7d,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,Cheng-Yu Hsieh,2023,05,50
2c8e27bc3c2a00fdc05bf0ebc1ee194d5f7b52ea,TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation,Keqin Bao,2023,05,22
d008049c8b2fe633188dc394d2ad967807b22b80,GPT-NER: Named Entity Recognition via Large Language Models,Shuhe Wang,2023,04,19
442ce411113cc6a91dd7e393b1f32a766ce466b0,The impact of the AI revolution on asset management,Michael Kopp,2023,04,0
3270c3054e6a14c5ee483f690ccda7367dd9a556,CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population,Tianqing Fang,2023,04,4
ca6a2bc279be5a3349a22bfd6866ed633d18734b,MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models,Deyao Zhu,2023,04,220
f60d3ce156dff3868cf923e03d94eef843eaa13a,How Secure is Code Generated by ChatGPT?,R. Khoury,2023,04,14
babbe71a2d3c57834cde889e24973e0a55183207,Enough With “Human-AI Collaboration”,Advait Sarkar,2023,06,2
ba184d335a9a08c52c5d25eabd7f4a8ea987918b,UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining,Hyung Won Chung,2023,04,5
78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c,The MiniPile Challenge for Data-Efficient Language Models,Jean Kaddour,2023,04,2
8d2c28fcb49de6357b7203e132ba0f247a353fd1,Learning to Compress Prompts with Gist Tokens,Jesse Mu,2023,04,16
15d12739b26783e2cf38bc0cbd81557fcc078eb0,Sabiá: Portuguese Large Language Models,Ramon Pires,2023,04,5
a8d740aff768210d21bf30cd83bab156d78a232a,Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation,Yunjie Ji,2023,04,4
5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891,DINOv2: Learning Robust Visual Features without Supervision,M. Oquab,2023,04,143
3ab661db57d924f4ff1706e05ac807873ca00e0a,RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment,Hanze Dong,2023,04,33
ae6a4cd221684be6ca3082b6f526a7901281490b,Emergent autonomous scientific research capabilities of large language models,Daniil A. Boiko,2023,04,34
cb58062526cd92e0cba7b6f50e9cb7bbec1f763a,Human-machine cooperation for semantic feature listing,Kushin Mukherjee,2023,04,0
1aeb3239735e28c7318af096044e48d919ea500b,Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study,Zengzhi Wang,2023,04,18
7a84a26647892daa9d10dbfc97c0382619ac2f4d,ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application,Naoki Wake,2023,04,19
737b5bbbc22913949c7d31157ac2dedfb039003a,Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling,Haotao Wang,2023,04,0
a3446f7a75acbb3929a33658cdb6aeaa0388c3c8,Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models,Siddharth Suresh,2023,04,1
7ca954844bc1dd405bc43445b1c990e42d865095,"CAMEL: Communicative Agents for ""Mind"" Exploration of Large Scale Language Model Society",G. Li,2023,03,53
01fac76c587a5a28c2646d100bc4bc1ae1048679,CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia,Md Tahmid Rahman Laskar,2023,05,3
70da4fb798a86cbe8cad96c27ced0415885bbd9d,AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators,Xingwei He,2023,03,24
3add55c068fe19bb2e5392cbe994602a91630ec1,Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams,Desnes Nunes,2023,03,7
ef6d50fec35ee818f6f42039714cc810f43d445a,Sequence-to-sequence pretraining for a less-resourced Slovenian language,Matej Ulčar,2023,,1
1c7402843d8b586d945b3b030e3edd93f0ae3959,On the Creativity of Large Language Models,Giorgio Franceschelli,2023,04,5
8fc90497d9043fdf35e71302b7c2e79bb907144f,Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases,Yunjie Ji,2023,03,35
82327cb1f2e1aa7fdb4e76935ebe203d5309341b,Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts,Kastan Day,2023,04,0
4f2ae5fa2dc74af9c36ee57b359a4b3241006a92,TRAK: Attributing Model Behavior at Scale,Sung Min Park,2023,03,11
464770587aece80cc9e3451050058e30c2aa6666,Scaling Expert Language Models with Unsupervised Domain Discovery,Suchin Gururangan,2023,03,5
c167340dbda3ed2ae7544d0688fbe9675f5e23b9,The reusability prior: comparing deep learning models without training,A. Polat,2023,,0
23684a07517870cffd1f97fafbaae16ba22bd2b7,"Large AI Models in Health Informatics: Applications, Challenges, and the Future",Jianing Qiu,2023,03,14
5aeef5fc2533f8deeefb73688040279acad67e96,Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale,Botao Hao,2023,03,0
3f82430e918525bc0517c5eb2b94e7d8e009b99a,eP-ALM: Efficient Perceptual Augmentation of Language Models,Mustafa Shukor,2023,03,6
5616967d176e0ebed60d77d931de431d7aefdccf,Towards a Foundation Model for Neural Network Wavefunctions,Michael Scherbela,2023,03,4
a713d9fb97c1e2b3982bc2f71128989c085d3f13,Trained on 100 million words and still in shape: BERT meets British National Corpus,David Samuel,2023,03,2
8c236be5cb8073cb3db317919ceb55130ab66dbe,CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos,Seungju Han,2023,03,1
cd17c3180fd13f9a370cdf9b68e8b8bdc3258120,"Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",Yubo Ma,2023,03,15
8f7ef5550cb40b63709c36313b8ce30ba6dc17a7,SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,Potsawee Manakul,2023,03,69
92f19090599910af1b1c9ed2b318abc0adea0527,Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences,Yunjie Ji,2023,03,16
b6499bcc10d4a70c3ca8b84995270cfd0d29de4c,Model-tuning Via Prompts Makes NLP Models Adversarially Robust,Mrigank Raman,2023,03,3
e5174aeda1baa67c17f4ac630ae2e44453954cc3,Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback,Hannah Rose Kirk,2023,03,19
2ebd5df74980a37370b0bcdf16deff958289c041,"Foundation Models for Decision Making: Problems, Methods, and Opportunities",Sherry Yang,2023,03,40
a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5,A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT,Yihan Cao,2023,03,84
79c5f79827c643c1e9464c2a5ff4d1326db16f20,AI chatbots not yet ready for clinical use,Joshua Au Yeung,2023,,10
0f19e94f30b99d6c4b349900057cdae9262034f9,The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges,Maria Lymperaiou,2023,03,2
ea2c0f739d13c6a00a847fc5c4771158c41a5726,Prompting Large Language Models with Answer Heuristics for Knowledge-Based Visual Question Answering,Zhenwei Shao,2023,03,15
6b423d3506f8e8c86903a274edb2b8de5277b4d3,Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning,Marc Lanctot,2023,03,2
c198c1193f924953b649002413979e7733c93a48,Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face,Christopher Akiki,2023,02,2
2ed0030d06ac2cc739c8460c102ae4713d10d1e1,The ROOTS Search Tool: Data Transparency for LLMs,Aleksandra Piktus,2023,02,8
0a6906bd6f026d3da3031c641ed03081bd0b574e,Full Stack Optimization of Transformer Inference: a Survey,Sehoon Kim,2023,02,10
754ed0deccc8c05e7dd05d5df376f205e569a841,An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP),P. Shakarian,2023,02,15
009fac3543208b2124ab82a25713953c2fb17af1,On Statistical Properties of Sharpness-Aware Minimization: Provable Guarantees,K. Behdin,2023,02,0
998ac3e945857cf2676ee7efdbaf443a0c6f820a,Hyena Hierarchy: Towards Larger Convolutional Language Models,Michael Poli,2023,02,33
36e21d8093361027088c1977ebaa2acf105c2b28,On Provable Copyright Protection for Generative Models,Nikhil Vyas,2023,02,20
b8c236dc5963dac36b0d8e419beb5876e3a18f96,Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation,Bobby He,2023,02,3
507465f8d46489a68a527cb5304d76bdb6c31ed9,Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,Lorenz Kuhn,2023,02,36
88a55a2748a7f34ae03e0e70573dae0fb1b5ee24,Optimising Human-Machine Collaboration for Efficient High-Precision Information Extraction from Text Documents,Bradley Butcher,2023,02,2
681cee58cf7e54199191cf9e0baf6851d8356704,"Complex QA and language models hybrid architectures, Survey",Xavier Daull,2023,02,4
d5bf40a7aff69dcdf9317c8374012c08fa66f87b,Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning,A. Luccioni,2023,02,4
2029349c55c1dba3493c5b3bd25152f18ba21ae2,Augmented Language Models: a Survey,Grégoire Mialon,2023,02,124
4cb3bef12a50dbcbbb5707b41b3a118048c717c3,Speculative Decoding with Big Little Decoder,Sehoon Kim,2023,02,5
85996f9fc312777f487dd51bf9e96bb3704c2fb7,On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark),Karthik Valmeekam,2023,02,20
ca75356503c540ec9207ba203abe5884564598af,Transformer models: an introduction and catalog,X. Amatriain,2023,02,11
873a581320d928249609d3c07229d5af182a379c,Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,Chengwei Qin,2023,02,199
bd166912fa16cec04e3ee044a0d0fb6e3ac6fe33,Regulating ChatGPT and other Large Generative AI Models,P. Hacker,2023,02,43
a1f8082505c7e90b0a033e1b9da0a97d67aad66c,Accelerating Large Language Model Decoding with Speculative Sampling,Charlie Chen,2023,02,24
fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0,A Survey on Efficient Training of Transformers,Bohan Zhuang,2023,02,7
d1101476c85ae324142440e9f568ecbf41625be5,Analyzing Leakage of Personally Identifiable Information in Language Models,Nils Lukas,2023,02,25
f2b0017ddd77fa38760a18145e63553105a1a236,The Flan Collection: Designing Data and Methods for Effective Instruction Tuning,S. Longpre,2023,01,147
da2fe6cd385194b0274d04d04ee72e8caf3854d4,Learning Universal Policies via Text-Guided Video Generation,Yilun Du,2023,02,24
6173520a1eb2814d067e8c5fd16212b7cbf6ee78,Grounding Language Models to Images for Multimodal Inputs and Outputs,Jing Yu Koh,2023,01,11
07b14c24833400b79978b0a5f084803337e30a15,REPLUG: Retrieval-Augmented Black-Box Language Models,Weijia Shi,2023,01,86
3f5b31c4f7350dc88002c121aecbdc82f86eb5bb,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models,Junnan Li,2023,01,528
af34181ae916e01c72513f915f984a1d2c7febab,"Augmented Behavioral Annotation Tools, with Application to Multimodal Datasets and Models: A Systematic Review",Eleanor Watson,2023,,1
345be3852f19b6d72d33de9d4a1165460e324505,Projected Subnetworks Scale Adaptation,Siddhartha Datta,2023,01,0
dea20a6930aea139d4e3628d473e3cdd57e9bc8b,Call for Papers - The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus,Alex Warstadt,2023,01,13
590b7a6da46d28fdd2b268d6cefcb09e7a70de5d,TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World,Hongpeng Lin,2023,01,0
45d26202fe0d91232dcb14937c4dd21a8737e99f,Factors associated with fall risk of community-dwelling older people: A decision tree analysis,K. Fong,2023,,0
2cd72e71299c5d62d5cdb1164df5236172d418c4,Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits,Ruibo Liu,2023,01,12
5a77b508302771fc083bf24e0bcda8553c9b5421,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,Tri Dao,2022,12,40
3f2561f5ad6947eef6526dfb5ebeb4a5d2c25506,Online dispute resolution: can we leave the initial decision to Large Language Models (LLM)?,Mario Ferrer-Benítez,2022,,2
901483fb655072e99773d3da4f91c9372d2c035e,The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes,Alexander Atanasov,2022,12,6
82e849a32601090fbf820f5d381dba43b52a8ed5,Do DALL-E and Flamingo Understand Each Other?,Hang Li,2022,12,6
bc36e46688725d574fb3e4e74808435c85bd61cd,JASMINE: Arabic GPT Models for Few-Shot Learning,El Moatez Billah Nagoudi,2022,12,3
70b98d90767345b15e0569082c0e4ac661279b5d,Is GPT-3 a Good Data Annotator?,Bosheng Ding,2022,12,42
a9e3e5dd7b30890553b7ae1c41f932e99192bb44,Large Language Models Are Reasoning Teachers,Namgyu Ho,2022,12,57
5c32c653735b43a0a8923ca65ac191bd4bf15311,Precise Zero-Shot Dense Retrieval without Relevance Labels,Luyu Gao,2022,12,37
e9d6cad994fd48567198ef260fbc5f5241aa9746,Task Ambiguity in Humans and Language Models,Alex Tamkin,2022,12,8
d651691e42d42423f0b13a0ee7dfc67087d87379,'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers,J. Michaelov,2022,12,5
34ecaea52a1d078f85e92629f2e5fd0f0e79154e,CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models,Lorenz Kuhn,2022,12,1
d8496775f90ca21735decc238855550c11efd85a,"Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language",Alexei Baevski,2022,12,12
bbca81491a5def081d9b668d105cbd4043206248,Gradient flow in the gaussian covariate model: exact solution of learning curves and multiple descent structures,Antione Bodin,2022,12,1
7932b714e2ae1def5828df52b97f1decb9bebd32,Considerations for Differentially Private Learning with Large-Scale Public Pretraining,Florian Tramèr,2022,12,22
60bb277cb1e5f9102238da45d8e904d4696f80f4,The development of an automatic speech recognition model using interview data from long-term care for older adults,C. Hacking,2022,,2
659ce1e0e2a32c4f1c4980be1cf1412b696bd06b,Algorithmic progress in computer vision,Ege Erdil,2022,12,5
b4c80344081d8c548fc4d68868b8262182a146fa,Structured information extraction from complex scientific text with fine-tuned large language models,Alex Dunn,2022,12,27
71ba5f845bd22d42003675b7cea970ca9e590bcc,Editing Models with Task Arithmetic,Gabriel Ilharco,2022,12,44
933b37b21e9d61139660088adb032ff3fdf56d86,Learning Video Representations from Large Language Models,Yue Zhao,2022,12,21
7928253927fc11ba7da64b428b6b97dce096672c,DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing,Conglong Li,2022,12,2
0ba0091c60c0346493b9ffb46ac682eee5453a53,Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping,Jiyan He,2022,12,12
656b5cb17d28c80fefb8c2f21e8f1f79b6dcd0dc,ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning,Shachar Don-Yehiya,2022,12,15
8fd462f6248d5e3f1b6602697c09489086b5655f,Distilling Reasoning Capabilities into Smaller Language Models,K. Shridhar,2022,12,8
37a2718297a62dc2d1f73f14763247597155f48f,"Challenges of Self-Supervised Learning for Unified, Multi-Modal, Multi-Task Transformer Models",Graham Annett,2022,,0
0a296e43474f5c1ef23c3c5a16555f38250069ef,Novel machine learning approaches revolutionize protein knowledge.,Nicola Bordin,2022,,11
d3c6f2c2efc715ec1a2cc192c4d52804ca0cb540,"A Pipeline for Generating, Annotating and Employing Synthetic Data for Real World Question Answering",Matthew Maufe,2022,11,0
7c6fcb3f1577b2385342d054b94df7924a5cdd13,ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information,Arnold Overwijk,2022,11,2
de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a,Fine-tuning language models to find agreement among humans with diverse preferences,Michiel A. Bakker,2022,11,39
22775e58932cdfbd273a2a835a22c5d86800a458,Continuous diffusion for categorical data,S. Dieleman,2022,11,31
5a8aa7bb52bc7240d1269981bd0eb046ea1c6346,Waveflow: Enforcing boundary conditions in smooth normalizing flows with application to fermionic wave functions,L. Thiede,2022,11,0
6d7b8a478801bd9d21df82d5f33ae6eced90da5e,Solving math word problems with process- and outcome-based feedback,J. Uesato,2022,11,42
e7dfa8ef33f961315837dcd808856a45fc9e97c1,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",Lukasz Augustyniak,2022,11,3
4b34d80f7486eb43a2c0cde55811a1e8603f7e8c,Protein Language Model Predicts Mutation Pathogenicity and Clinical Prognosis,Xiangling Liu,2022,,1
ca34bb5a158349937c0ad3f3031aad60d5dfbcd6,Structured Pruning Adapters,Lukas Hedegaard,2022,11,4
4c6d9161b9e7d7ce8b9ed7a45bb32ef2067f7b5c,GAMMT: Generative Ambiguity Modeling Using Multiple Transformers,Xingcheng Xu,2022,11,0
0c452403a79f774da360934716f63191270fe0dc,Large-Scale Bidirectional Training for Zero-Shot Image Captioning,Taehoon Kim,2022,11,1
2c6ac935c826002976722ca8d3319f691975687e,Self-conditioned Embedding Diffusion for Text Generation,Robin Strudel,2022,11,36
25012e794961facbe7a3da637bcaa57dde602fff,Multiclass voice commands classification with multiple binary convolution neural networks,Jaroslaw Szkola,2022,,0
ac71932c6af8823224a302907ed65e21145f5db5,Modeling structure-building in the brain with CCG parsing and large language models,Milovs Stanojevi'c,2022,10,1
2577d053f8aab912d29b424e1f09133d83740fd2,Multi-lingual Evaluation of Code Generation Models,Ben Athiwaratkun,2022,10,38
43d3dbabea106b59e1ec248457c88b19636e4f47,Can language models handle recursively nested grammatical structures? A case study on comparing models and humans,Andrew Kyle Lampinen,2022,10,13
810e7ff45d5cdb2318f88a2787a37b2fadd9bf82,Legal-Tech Open Diaries: Lesson learned on how to develop and deploy light-weight models in the era of humongous Language Models,Stelios Maroudas,2022,10,1
075f83cac2742dbb36ee49d30f7aee2a322f3127,Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models,Hao Liu,2022,10,2
5b745be118ae6e8678725698ffaa336ecffd31c0,Code4Struct: Code Generation for Few-Shot Event Structure Prediction,Xingyao Wang,2022,10,3
7cf4f8cb8b4a373d869e785b79160dda7a49a250,Exploring The Landscape of Distributional Robustness for Question Answering Models,Anas Awadalla,2022,10,7
f3a13abf23afecf534c955954d70c3b0fc41d334,Composing Ensembles of Pre-trained Models via Iterative Consensus,Shuang Li,2022,10,13
6494c6149e5036c09ee92da9fd67cbecc998a52f,PCFG-Based Natural Language Interface Improves Generalization for Controlled Text Generation,Jingyu Zhang,2022,10,0
59fed7ca092c7e83583906456756abba8ce9295a,Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities,Brian Bartoldson,2022,10,12
2231b5e00fa1f8f4b222089fe4bb64a95970b59a,You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models,Tomasz Limisiewicz,2022,10,0
42e1790c7979796634d15920b4a08990e847243e,Transformers generalize differently from information stored in context vs in weights,Stephanie C. Y. Chan,2022,10,19
ba07acaacfe9f67afef97278dd3914c93fce6b6b,Multi-step Planning for Automated Hyperparameter Optimization with OptFormer,L. Dery,2022,10,0
dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc,AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models,Se Jung Kwon,2022,10,10
f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab,Binding Language Models in Symbolic Languages,Zhoujun Cheng,2022,10,60
559bfba3bee31f6061a5d5c7061f22794de47e39,State-of-the-art generalisation research in NLP: a taxonomy and review,D. Hupkes,2022,10,26
1d26c947406173145a4665dd7ab255e03494ea28,GLM-130B: An Open Bilingual Pre-trained Model,Aohan Zeng,2022,10,267
ed99a2572fb5f4240aa6068e3bf274832e831306,Recitation-Augmented Language Models,Zhiqing Sun,2022,10,29
ec97c3248537bb0b455b3fe9bc341110cfceffde,Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks,Zhenhailong Wang,2022,10,3
f9b16559282e5bea0bb072f9e260a4f0af697f4a,Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals,Piotr Wojciech Mirowski,2022,09,45
74eae12620bd1c1393e268bddcb6f129a5025166,Improving alignment of dialogue agents via targeted human judgements,A. Glaese,2022,09,165
cf7a41fedd0dfe0bb016c67e53317a5130c45d2a,"Trends in Energy Estimates for Computing in AI/Machine Learning Accelerators, Supercomputers, and Compute-Intensive Applications",S. Shankar,2022,10,3
557a5147d88ad361b807d4c93decac6d57d2d5e9,Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans,John J. Nay,2022,09,8
3585118b0c75a45a3c7a00343e14336bd32d8e9f,Language Model for Statistics Domain,Young-Seob Jeong,2022,,0
8cf974fd3973900c0598730ee5d3617900ac8c3d,Transformers with Learnable Activation Functions,Haishuo Fang,2022,08,1
f0a0e8b6e84207f50db4d24cc4016e40601214ef,Faithful Reasoning Using Large Language Models,Antonia Creswell,2022,08,61
8bd182f01c99e643c9a5a96832dc1a16b9cd10d0,Domain-Specific Text Generation for Machine Translation,Yasmin Moslem,2022,08,4
914254fac74a2da051cccf6ca16afcaad416a079,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,Saleh Soltan,2022,08,50
63cb94e274b31c38c60644574967d14a472f7bc4,From BERT to GPT-3 codex,Immanuel Trummer,2022,06,4
c5b47a34fd7e0e595360683b4effa51c0261d1e3,Language Models Can Teach Themselves to Program Better,Patrick M. Haluptzok,2022,07,11
292da1c4640c105aa5d7919a1ded9a1225c07d4d,Large Language Models and the Reverse Turing Test,T. Sejnowski,2022,07,28
00d9a73c54053a32e2aba92b53fc3e6bc71d3238,Sequence to sequence pretraining for a less-resourced Slovenian language,Matej Ulvcar,2022,07,3
311f171287dba3823547c6fd703c1ab8a2d45995,BigIssue: A Realistic Bug Localization Benchmark,Paul Kassianik,2022,07,0
f5f5616f39493566a9d502f611adcc8f1ceb394e,Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit,B. Barak,2022,07,40
290732e9fb08a29af8892a7c1f73c9d2a1b9d7db,Language models show human-like content effects on reasoning,I. Dasgupta,2022,07,62
fc46bfc5a16442366af0b1dd9fdad9f5696884ac,A single T-gate makes distribution learning hard,M. Hinsche,2022,07,20
a2a278c7b1ea7d851b49a310cccad45df14082f3,ClueWeb22: 10 Billion Web Documents with Rich Information,Arnold Overwijk,2022,,10
6c46b7b401be5ada79f00b36cb8e5b41286ae2aa,Measuring Forgetting of Memorized Training Examples,Matthew Jagielski,2022,07,34
ab0e3d3e4d42369de5933a3b4c237780b41c0d77,Solving Quantitative Reasoning Problems with Language Models,Aitor Lewkowycz,2022,06,257
eaef083b9d661f42cc0d89d9d8156218f33a91d9,Long Range Language Modeling via Gated State Spaces,Harsh Mehta,2022,06,28
1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,Jiahui Yu,2022,06,436
6d147d1b7a283252052cda28a98ee6cc6379f932,Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change),Karthik Valmeekam,2022,06,69
f2c17758e74707d379b87372528221656d14b697,Taxonomy of Risks posed by Language Models,Laura Weidinger,2022,,132
8af5a1b58338fb16c69bf832299453af2d2bbd0d,Know your audience: specializing grounded language models with listener subtraction,Aaditya K Singh,2022,06,1
395d89ba308890901ad19905f7556cab0a1a6a27,Towards Understanding How Machines Can Learn Causal Overhypotheses,Eliza Kosoy,2022,06,7
a970c8fadef8497576660b288c52c0ec8eebdc12,Zero-Shot Video Question Answering via Frozen Bidirectional Language Models,Antoine Yang,2022,06,48
bbf3451c9c4eb6d8d84314b584afb0fadd01ffba,A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,Wonseok Hwang,2022,06,8
70cb319c2339d5adee742912a6f3ef88ae462bdd,Unveiling Transformers with LEGO: a synthetic reasoning task,Yi Zhang,2022,06,37
32f81c63e5392f468e4e7fe2435a6b8c450a5895,Fair Classification via Transformer Neural Networks: Case Study of an Educational Domain,Modar Sulaiman,2022,06,0
2a0336bc5887968bd5590ea6b2b1f953edd9dcbc,"On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation",Tiago Pimentel,2022,05,6
560edeb5108b884f24061dece50a30c05f1f858f,You Can't Count on Luck: Why Decision Transformers and RvS Fail in Stochastic Environments,Keiran Paster,2022,05,5
374dd173491a59a10bbb2b3519ebcfe3649f529d,Teaching Models to Express Their Uncertainty in Words,Stephanie C. Lin,2022,05,68
c28e95a06dfcf13fc65a1cac83722f53e34f12a5,Autoformalization with Large Language Models,Yuhuai Wu,2022,05,53
8ce9b1e527c4d9d15239621ec4e3ef3fbbe32202,On the Role of Bidirectionality in Language Model Pre-Training,Mikel Artetxe,2022,05,15
3ae3716f125d71e9daccac3dafa4fab7482fb16a,Data Governance in the Age of Large-Scale Data-Driven Language Technology,Yacine Jernite,2022,06,25
c710d9a8c30b3130caaf3ea45e786de6b5a3eed8,Adversarial Training for High-Stakes Reliability,Daniel M. Ziegler,2022,05,26
13a0d8bb38f739990c8cd65a44061c6534f17221,OPT: Open Pre-trained Transformer Language Models,Susan Zhang,2022,05,1142
fd5a9dfe39e1918b631e0519e272d2643d8e6bca,Impossible Triangle: What's Next for Pre-trained Language Models?,Chenguang Zhu,2022,04,1
15190e8b459bd85d546286f7d7da61b4f4f3f58a,What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?,Thomas Wang,2022,04,60
736eb449526fe7128917954ec5532b59e318ec78,Block-Recurrent Transformers,DeLesley S. Hutchins,2022,03,35
3d849136e0070f6d038dd96985ed67ead5aedb69,Locally Typical Sampling,Clara Meister,2022,02,34
4e7de32c8da8c910285acdaf397347dc94ca3594,Many Heads but One Brain: Fusion Brain -- a Competition and a Single Multimodal Multitask Architecture,Daria Bakshandaeva,2021,11,0
9140afbf83ac5fa07b3d4672e2329012812ca0fc,Systematic human learning and generalization from a brief tutorial with explanatory feedback,A. Nam,2021,07,0
09279dc8018a8131e11d527cebb06d0a43c67cff,Creativity and Machine Learning: A Survey,Giorgio Franceschelli,2021,04,22
a37153a5f42ee2951ad8a2c9ec86b52c4bf81c77,Retrieval-Augmented Multimodal Language Modeling,Michihiro Yasunaga,2023,11,24
2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75,Grounding Language Models to Images for Multimodal Generation,Jing Yu Koh,2023,,37
e587004535976e1b9b117297d0b40fe81259bf88,Sharpness-Aware Minimization: An Implicit Regularization Perspective,K. Behdin,2023,,2
f2b2981f7d9439d734acbd0edbd8ad6e7af77a2b,Mystique: Accurate and Scalable Production AI Benchmarks Generation,Mingyu Liang,2023,,0
76fbe3314cd7c36099a4df921f0d26214b0fcb30,Language Models are Pragmatic Speakers,Khanh Nguyen,2023,,0
d1923233d6ef08caa97f1609ac3474f4ddedc297,Language Models are Bounded Pragmatic Speakers,Khanh Nguyen,2023,,1
a597da9bae91aae9ecc292a6d613d134a6ddd8f2,Enhancing Multi-Task Text Classification with Contrastive Learning and Dataset Augmentation in BERT-like Models,Phillip Yao-Lakaschus,2023,,0
8aee5f2d872736b018b8af8c2af1556bcb7e9147,Data Augmentation for Low-resourced Language Modeling,Wanyue Zhai,2023,,0
15ecf4c5f17dc5fe5fe5c39b85620cb014fe9863,Default Final Project: Improving minBERT with Entailment Learning,Akash Chaurasia,2023,,0
0e4bd81558b764b757d76258d93d184ac3893e1f,CROSS-LINGUAL AND PROGRESSIVE TRANSFER LEARNING,Johannes Welbl,2023,,0
35f773098092189a42f26c9282c52afde277ec87,GATED STATE SPACES,Luke Zettlemoyer,2023,,0
5d9eede603ebe7c86882c38cc3f29acf1f4bb0cb,EXPLORING THE DESIGN SPACE OF AI BASED CODE COMPLETION ENGINES,Parth Thakkar,2023,,0
c0d7ba869d89effb06720fa48041f908e92509c0,Defining Deception in Structural Causal Games Extended,Francis Rhys Ward,2023,,0
9881a953fc613d836d23528ef227442a21e5b0e8,Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation,Zhen Guo,2023,,1
0a470fbfebb5515e8af8d9a09f603df4dfce40a0,A Systematic Evaluation of Large Language Models for Natural,Xuanfan Ni,2023,,0
50b7a04dc5df6b31f31f63c9ce0f907df76b1cc3,Inflection-1 Inflection AI,,2023,,0
978b6b17f19945cb19ef8b9f3db397ebafd95b98,Inflection-1 Inflection,,2023,,0
e20975524ace61c39bffc35db48c5a592eb9d769,Hardware Software Co-design and Architectural Optimization of Deep Learning Models for Natural Language Processing,Thanakul Wattanawong,2023,,0
569d6641ff9b67c2bd399ea0519940a171a48829,Honesty Is the Best Policy: Defining and Mitigating AI Deception,Francis Rhys Ward,2023,,4
c8e0076c38901382e27b4c843b8a763a310619e9,Curating Datasets for Better Performance with Example Training Dynamics,Aviad Sar-Shalom,2023,,0
e002bb8dae5a18a5ea1e7e1aafa16e19ad545662,Solving Math Word Problems with Process-based and Outcome-based Feedback,J. Uesato,2022,,1
c042d0169ea4f4e4490d406befb973ebab135e7e,Vision Encoders in Visual Question Answering,Ryan R. Anderson,2022,,1
9a0d617e1c4840b01cd8eb79d92ec8cc5047343a,Cluster-based Evaluation of Automatically Generated Text,Tiago Pimentel,2022,,2
8ad98d68bebab0861dcb1c4bdaf12c2f54fa6f4f,Know your audience: specializing grounded language models with the game of Dixit,Aaditya K Singh,2022,,1
ae10c4b220a0bc0999bf169d5c219086d1f1aeed,Edinburgh Research Explorer Taxonomy of risks posed by language models,Laura Weidinger,2022,,1
450b8dff662a5d41388d04d994e5117020777ce5,Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language,Xingyao Wang,2022,,26
3994eb8e237a94dae1efc6e767a09044b8550ace,FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners,Hao Liu,2022,,2
5840bf765be8c3bcedab63f43f5982ddba26eaf9,SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING,,2022,,0
4dec6c9295e24dc884991893e30dec664034b928,SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling,Jesse Zhang,2022,,1
72123a86eae2cb5c4eae8650f43524039d48875d,Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions,K. Shridhar,2022,,26
29c9a8be58d8b6fbb697fecfc0ddae965c4c8480,IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages,Sumanth Doddapaneni,2022,,10
4ca1057d642caf8a7b817c4aeb6198801bc69758,CLAM: Selective Clarification for Ambiguous Questions with Large Language Models,Lorenz Kuhn,2022,,4
3d3f8399d625238fddb366697acb73446129d65c,Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing,Abbas Ghaddar,2022,,2
4d3c61b81e3b06fe325b3903206163ded21c4858,NT5 at WMT 2022 General Translation Task,Makoto Morishita,2022,,1
58d19352e591d33dec061d12ea3e2a233ce45fd8,STen: An Interface for Efficient Sparsity in PyTorch,Andrei Ivanov,2022,,3
84fa5410ac28b8df6ddd91aca841069b0abbe852,Something for (almost) nothing: Improving deep ensemble calibration using unlabeled data,Konstantinos Pitas,2023,10,0
d15021d750dbe9cee120b562acea857ca02d9104,Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models,Tianjian Li,2023,10,0
3fa1090abbc155c5ac483e71e9f687c91e6c5509,Feedback-guided Data Synthesis for Imbalanced Classification,Reyhane Askari Hemmat,2023,10,0
1cc39f7691af0bc1061dda7897a3f62f098573d6,Demystifying CLIP Data,Hu Xu,2023,09,1
f12605f969c09417f310c88dee3c6b3b60960297,Towards a statistical theory of data selection under weak supervision,Germain Kolossov,2023,09,0
89ced808962da94b6880b2916bd2fd1b6bf05a5d,An Optimization Method of Deep Transfer Learning for Vegetation Segmentation under Rainy and Dry Season Differences in a Dry Thermal Valley,Yayong Chen,2023,,0
fe9b409cd4738251ae340612c1f7b5671848eb67,SlimPajama-DC: Understanding Data Combinations for LLM Training,Zhiqiang Shen,2023,09,0
ada6905ae4c4c50cd7fbaa75be47eff1cdba7302,Gradient Matching for Categorical Data Distillation in CTR Prediction,Cheng Wang,2023,,0
4b4a329e54325e80be50cdc77e274c6e9fd5ade4,Nougat: Neural Optical Understanding for Academic Documents,Lukas Blecher,2023,08,2
ce776050bbc4d0a2afba25a479c7c8226de16ce2,GRASP: A Rehearsal Policy for Efficient Online Continual Learning,Md Yousuf Harun,2023,08,0
c91fde2dc4a687b880ed761b82df8ab18fb81235,A Clustering-Based Data Reduction for the Large Automotive Datasets,Patryk Siwek,2023,,0
00e19f0636f49ea712087972b710b0ff467a4787,Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing,Abdul Hameed Azeemi,2023,,0
b4550a5121cfb67054d7164a7146815b6c48bb07,Data-Centric Diet: Effective Multi-center Dataset Pruning for Medical Image Segmentation,Yongkang He,2023,08,0
7777c7f878df788fab66e79be68421676bbb31fb,Pre-training Vision Transformers with Very Limited Synthesized Images,Ryo Nakamura1,2023,07,0
4b474c1f42eefbf14ca85c951f2a22ce031b6cb7,Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models,Mayee F. Chen,2023,07,0
70d3325d88387cc8c8568c72cba211a09626edd6,Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development,Danilo Brajovic,2023,07,0
244ab847aee80cc3d0ab96f836f9e83630f5537d,NeSSA: Near-Storage Data Selection for Accelerated Machine Learning Training,N. Prakriya,2023,,0
becc1e8c609988ba208d911214e79fbc0254b5cf,Enabling Multi-Part Plant Segmentation with Instance-Level Augmentation Using Weak Annotations,Semen Mukhamadiev,2023,,2
80dffba8f0f268777d67b3b698118b58ab9d8106,AdaSelection: Accelerating Deep Learning Training through Data Subsampling,Minghe Zhang,2023,06,0
53347ea2f3df92f585c0c305ba0a29a2200ee00a,Efficient Quantization-aware Training with Adaptive Coreset Selection,Xijie Huang,2023,06,0
0cb130c2d4312d72d27d45fcd81e6c5a0e4d7c94,Large-scale Dataset Pruning with Dynamic Uncertainty,Muyang He,2023,06,0
b26b07597645522782e919c32f3f84c54a4b7cbf,NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks,Jean-Michel Attendu,2023,06,0
56677526b80771dc553eebad3681ec2434cc9d93,Prune Then Distill: Dataset Distillation with Importance Sampling,Anirudh S. Sundar,2023,,1
6c57ce08546a5f996d9c487522bd9ba1d67c9eac,Dataset Efficient Training with Model Ensembling,Yeonju Ro,2023,,0
88a3d58a81203abb17907a921c6df07abc2b18c4,Analysis of Emotion Annotation Strength Improves Generalization in Speech Emotion Recognition Models,João Palotti,2023,,0
00d66c65fdf371dfebcc5db265aa3da8f0b75740,Decoupled Semantic Prototypes enable learning from diverse annotation types for semi-weakly segmentation in expert-driven domains,Simon Reiß,2023,,1
d1efa2cde9adc02169e73f07e82e06f0f7b2862b,Too Large; Data Reduction for Vision-Language Pre-Training,Alex Wang,2023,05,0
f924315bad64452dfd1edd80f932fc1dc871b176,CodeTF: One-stop Transformer Library for State-of-the-art Code LLM,Nghi D. Q. Bui,2023,06,6
1607ee857e7fd5bcd16581859f4049cd61fc909a,Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning,Patrik Okanovic,2023,05,0
ae3fe0f334a1dd969047be1a725cd85bfe20d3d0,A Three-regime Model of Network Pruning,Yefan Zhou,2023,05,0
28571a6b4761423a8b6acadfb56e2b3765cccf78,SelfClean: A Self-Supervised Data Cleaning Strategy,Fabian Groger,2023,05,2
fec35357525ae87f46067da841478f35e28ed8e9,Selective Pre-training for Private Fine-tuning,Da Yu,2023,05,1
3878862b0d8dcfdce2a8e66dbc7fc455824c7f49,DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining,Sang Michael Xie,2023,05,12
dbc51f2a4b0bbae6a701d37b7981c8edf6cadfd7,DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications,Cyril Picard,2023,05,1
26bd268e4950057317a24d2e9e593b4c2a2e32eb,MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization,Yunshan Zhong,2023,05,0
b5ab93517516a4758f866e7cac03ca378296c124,INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models,H. S. V. N. S. K. Renduchintala,2023,05,0
158bfeeef80b20943478c5a4b5f78bdd23a3d67e,Learning Trajectories are Generalization Indicators,Jingwen Fu,2023,04,1
cc31009d3a7ad3a799b93c717ab22338e1819598,On the redundancy in large material datasets: efficient and robust learning with less data,Kangming Li,2023,04,3
06d6398e9a771e41cc63bacc156c38238b8fc221,Learning Sample Difficulty from Pre-trained Models for Reliable Prediction,Peng Cui,2023,04,1
d3083d2e751601349b3fd884c79994e6ad1cd548,Neural networks: from the perceptron to deep nets,Marylou Gabri'e,2023,04,2
84b6fecf016d74512869c698c66c83729abdf359,Self-Supervised Multimodal Learning: A Survey,Yongshuo Zong,2023,04,7
a3ce06f894d56edb5e530b5812e36fbe7f5971a7,SIESTA: Efficient Online Continual Learning with Sleep,Md Yousuf Harun,2023,03,4
a3a5770f06a36e11bdc5cc28607da640b4451b59,A Challenging Benchmark for Low-Resource Learning,Yudong Wang,2023,03,0
5722b1bd272145feb4fa51683a32ffe7d3d8c211,Disambiguation of Company names via Deep Recurrent Networks,A. Basile,2023,03,0
97d680e7de233e4a3023415d675afb6d681768e1,DREAM: Efficient Dataset Distillation by Representative Matching,Yanqing Liu,2023,02,6
1b6bac60fe8f7eeb084019181f5b7475472b0e68,Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least,S. Joshi,2023,02,1
a5326c49270f26e13dc5f6d731e77c832a14af25,Analyzing the Performance of Deep Encoder-Decoder Networks as Surrogates for a Diffusion Equation,J. Q. Toledo-Marín,2023,02,0
a008cc894024329d832d2c9c489d57440e3fa234,Data Selection for Language Models via Importance Resampling,Sang Michael Xie,2023,02,15
4bf68a5358453b651474f3c154ded652e0084a4f,Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data,Alon Albalak,2023,02,1
498414ed65a0cdb5550d8f7e0ce666eb290c6517,Learning Large-scale Neural Fields via Context Pruned Meta-Learning,Jihoon Tack,2023,02,0
6819341b679eaef6040d37fab172ddbff333a5e3,No Matter How You Slice It: Machine Unlearning with SISA Comes at the Expense of Minority Classes,Korbinian Koch,2023,,8
360cd0837ebbad4a4b5d15d66491e97164207a38,MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning,Krishnateja Killamsetty,2023,01,0
e7ca53c22f02257ae3cfce00c27fe386b5c2a107,Data Valuation Without Training of a Model,Nohyun Ki,2023,01,3
0ee03841a5db4faae781ab1af65f20e738d4a082,Margin-based sampling in high dimensions: When being active is less efficient than staying passive,A. Tifrea,2022,12,0
5f6a5f319aca3f3d3f7a3c7c1ef0b8fc97ee1458,"Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation",Cody Blakeney,2022,11,2
7619ed35ac30712fefd8f3e7f5d921f209f1268c,Coverage-centric Coreset Selection for High Pruning Rates,Haizhong Zheng,2022,10,1
bccf87b61a9549fff4c79e17c43c6b9051e7186a,AVES: Animal Vocalization Encoder based on Self-Supervision,Masato Hagiwara,2022,10,3
dd36fa18e4d254e87db4e7a0581ca04206cfe220,Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees,M. Ponti,2022,10,0
a4af00f50f0b397b14ae5dc22e0e766c31adaaa8,Making Your First Choice: To Address Cold Start Problem in Vision Active Learning,Liangyu Chen,2022,10,7
fa9aac3c7aba3ad4b2162c7ec2e0ad8ef4668005,Identify ambiguous tasks combining crowdsourced labels by weighting Areas Under the Margin,Tanguy Lefort,2022,09,0
66dc6277bac98622c09fd19f0d20a68471568674,Can we achieve robustness from data alone?,Nikolaos Tsilivis,2022,07,6
bb064505dfd8a1dd582d428d19d7d7a956942a6a,Computable Artificial General Intelligence,Michael Timothy Bennett,2022,05,4
6f7faf8afa42a00ac0a31d98766c78b09d5010d6,Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition,Abdul Hameed Azeemi,2022,03,0
1f898d163501059345d8ce4f2d7d695bb548c03d,Fundamental problems in Statistical Physics XIV: Lecture on Machine Learning,A. Decelle,2022,02,3
e023593fa5ab8930e43bb4777dd017bfedf4ae54,M ODERATE C ORESET : A U NIVERSAL M ETHOD OF D ATA S ELECTION FOR R EAL - WORLD D ATA - EFFICIENT D EEP L EARNING,Xiaobo Xia,2023,,0
5b7890c39325061261de8ec34cc82eff1a885143,M ILO : M ODEL -A GNOSTIC S UBSET S ELECTION F RAMEWORK FOR E FFICIENT M ODEL T RAINING AND T UNING,Krishnateja Killamsetty,2023,,0
7af5b177cd63c2c62c294c131e76b3c3bfa36aa1,Efficient Meta-Learning via Error-based Context Pruning for Implicit Neural Representations,Jihoon Tack,2023,,1
21042fc104765bcaa4199d76390265ace4f57519,Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most,S. Joshi,2023,,1
251516c1549fc4566b801788c932ef1f18f343b3,Making Your First Choice: To Address Cold Start Problem in Medical Active Learning,Liangyu Chen,2023,,1
b0433cedc9812d5e79e656c096664d89d99788ff,Dataset Reduction Methods for Data and Energy Efficient Deep Learning,D. Krol',2023,,0
ad0c2852aa67dbcdc6a1fed4f1ba567c08cb4033,Knowledge Representation of Training Data With Adversarial Examples Supporting Decision Boundary,Zehao Tian,2023,,0
7931e9dbb5734124a83ce16fd55115e87ef869c4,Density-based one-shot active learning for image segmentation,Q. Jin,2023,,0
b07ef57a8a11fdb22755e632fa9c76e5d7381e4c,M ILO : M ODEL -A GNOSTIC S UBSET S ELECTION F RAMEWORK FOR E FFICIENT M ODEL T RAINING AND T UNING,Krishnateja Killamsetty,2023,,0
d5b27c795e367df0e8b4ed6d91f3f6231e40d812,Sample Relationships through the Lens of Learning Dynamics with Label Information,Shangmin Guo,2022,,0
2eea104cd90d42ed81217dcbf5fe44725f879b93,Improve learning combining crowdsourced labels by weighting Areas Under the Margin,Tanguy Lefort,2022,,1
be8933165282991265bb7d05a7e0a4701422a54a,Active Learning is a Strong Baseline for Data Subset Selection,Tom B. Brown,2022,,7
166d1e5361465f8e235747d14641249cbb3b6fd2,A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models,Haoran Xu,2023,09,0
c74a13b251b6af6dfce49eeb128b1c0e2ddf955d,A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment,Ying Zhao,2023,08,1
aea817017e18cc1cb34962ffd399f8a83ab7a076,Teaching Smaller Language Models To Generalise To Unseen Compositional Questions,Tim Hartill,2023,08,1
002cfed5d4d9bf2fdaddb11d32f14751f2250e0c,Teaching Arithmetic to Small Transformers,Nayoung Lee,2023,07,5
f2dbc96d7f8250b1c6e7ebcdf9a8093b9c286226,Improving Retrieval-Augmented Large Language Models via Data Importance Learning,Xiaozhong Lyu,2023,07,1
4614b2eb70409aca18b7fad197dde34c83a0ceb7,Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning,G. Chalvatzaki,2023,05,5
886e0962479ec6dac563666399ca4c96a468fcaa,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,Erik Nijkamp,2023,05,17
5dfd3cfc5111f90a20b1f1cd60587c6a0f80157a,On the Inconsistencies of Conditionals Learned by Masked Language Models,Tom Young,2022,01,2
56c3f1286585f14aa6b34c53f4c09c6e5e6e068a,A Study on Transformer Configuration and Training Objective,Fuzhao Xue,2022,05,0
dd22be17893d177cf3e6c1213bfd447c7a29a24c,Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning,Mohammad Javad Darvishi Bayazi,2023,09,0
e1a280f346b575c85cde4ad4ba0e10dc6df93105,On Monitorability of AI,V. Yampolskiy,2023,,0
023d462ec6ff84cee0d0716a34d11efc7cde8534,Reward Model Ensembles Help Mitigate Overoptimization,Thomas Coste,2023,10,0
311b5c770738fabc940b3b630664d562916df83c,Tool-Augmented Reward Modeling,Lei Li,2023,10,0
68c437f0f157ea24772f9e678c7962a1cd767464,VAL: Interactive Task Learning with GPT Dialog Parsing,Lane Lawley,2023,10,0
2a7967ba1f44f8c5985bf01e2982ce511fec42fc,Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment,Tianhao Wu,2023,10,0
541b66bad4a9bf9b7fd97f13f94ab9061c7c0c47,The Trickle-down Impact of Reward (In-)consistency on RLHF,Lingfeng Shen,2023,09,0
860c8de4fdac38695ff6860dd15312f1079c6117,Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints,Chaoqi Wang,2023,09,0
c7ff2c35ab586b527a874dc370dc7f94269679d2,First Workshop on User Perspectives in Human-Centred Artificial Intelligence (HCAI4U),Ernesto William De Luca,2023,,0
e56dc21699e6283fce072ffc908cb9f66321760d,"Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF",Simeng Sun,2023,09,2
669441cb46666036f663f19def44bec2a838a518,Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models,Yue Zhang,2023,09,11
b9478ba0e4a55bcb642c3beb738788da8d04b33b,Reinforcement Learning with Human Feedback for Realistic Traffic Simulation,Yulong Cao,2023,09,0
182c7b40ff7560a5545764814338f55a2098e441,Reinforced Self-Training (ReST) for Language Modeling,Caglar Gulcehre,2023,08,11
6eb46737bf0ef916a7f906ec6a8da82a45ffb623,Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback,Stephen Casper,2023,07,21
bd040b065efd3cac12f3eb6832a8fc1e69da2a7e,Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions,Tongxin Li,2023,07,0
576ce0da147347053d7caf8d35e094cec6d3f7e0,Evaluating Capabilities of Large Language Models: Performance of GPT4 on Surgical Knowledge Assessments,B. Beaulieu-Jones,2023,,0
e730164e17975547564a1eaa70cea5884b16c89d,AlpaGasus: Training A Better Alpaca with Fewer Data,Lichang Chen,2023,07,10
19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e,Preference Ranking Optimization for Human Alignment,Feifan Song,2023,06,20
f320ad4e1a9afbe71625659568469c4d641ee395,On the Exploitability of Instruction Tuning,Manli Shu,2023,06,4
0b20389e0586fe00ee3c907e5e878bf2c27ddf41,Sistema inteligente conversacional aplicado a la gobernanza local,Franco Brandan,2023,,0
9178f53599dfef5597fa3b12dfca19db28f80771,Artificial Intelligence and Human-Induced Seismicity: Initial Observations of ChatGPT,M. Wilson,2023,,0
29240b90b0ce015aa0ad330e3680f3c69c64cb8c,Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards,Alexandre Ramé,2023,06,4
370e51386abb7b999728e08b74f0a77fbd064834,Fine-Tuning Language Models with Advantage-Induced Policy Alignment,Banghua Zhu,2023,06,3
be8db99310602d66bba64bcf41a572c45816fbfc,Let's Verify Step by Step,Hunter Lightman,2023,05,49
29c36b81c6b3d50e16b7f0531a9e8cb491dc460b,Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism,Zihao Li,2023,05,1
4052669120d78de842fde890851c64d306e6dec0,Training Socially Aligned Language Models in Simulated Human Society,Ruibo Liu,2023,05,19
65f604325b1403bc835691d905c2cd50bc04a309,Model evaluation for extreme risks,Toby Shevlane,2023,05,29
10d44cf3fee9b859cea327ea0c22ed03303605eb,"Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",Evangelos Pournaras,2023,05,0
8209f2db5b6ed3ec265f016dae7dc1b0213d2b2f,Training Priors Predict Text-To-Image Model Performance,Charles Lovering,2023,06,0
d8c78221e4366d6a72a6b3e41e35b706cc45c01d,Training Diffusion Models with Reinforcement Learning,Kevin Black,2023,05,10
cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa,AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback,Yann Dubois,2023,05,45
4a0423b059c5482fafe70202b2942e98f033336b,S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning,R. Dutta,2023,05,0
fa6d8c4c93eb35d9a2d9542133719dbad87faeb7,CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning,Weiqi Wang,2023,05,4
74b05bba46db21e589a2cc0f916f81069b0368ef,Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation,Patrick Fernandes,2023,05,13
fdaf607519da2a37d3653ec72b8085b157f20d1b,Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications through Visual Programming,Ruofei Du,2023,,5
354dcdebf3f8b5feeed5c62090e0bc1f0c28db06,ChemCrow: Augmenting large-language models with chemistry tools,A. Bran,2023,04,37
c11810fa8887b678facea62da4607c4898360308,Training Language Models with Language Feedback at Scale,J'er'emy Scheurer,2023,03,32
da872dfc0934f554311d5f91fa7e5ada44fb8155,Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT,Mostafa M. Amin,2023,03,18
2056f8fc406eabdeb40cd808c24f6d9b302519e6,Should AI Technologies Replace the Human Jobs?,Abhinav Trivedi,2023,,0
e3bc7f2bbe6e6c27be6132591d53df308f16ab97,Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT,Mostafa M. Amin,2023,,7
941f364cfc445b02884eae511ce5c35a85b0aba8,A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors,Shangding Gu,2023,02,2
b0bd491191e57bc4059e4fee4c2d4ed41a61f470,Conditioning Predictive Models: Risks and Strategies,Evan Hubinger,2023,02,0
4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4,Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons,Banghua Zhu,2023,01,25
3936fd3c6187f606c6e4e2e20b196dbc41cc4654,Constitutional AI: Harmlessness from AI Feedback,Yuntao Bai,2022,12,258
d40df59a1df8048a671bce60260b817e109a33f8,Reward Gaming in Conditional Text Generation,Richard Yuanzhe Pang,2022,11,5
0b572dd083ca8fbfe72a90c16b886e9370fc14d0,AlpaGasus: Training A Better Alpaca with Fewer Data,Lichang Chen,2023,,10
e38206adfdb82e7c100de23da0c4499ff1079b25,"Science in the Era of ChatGPT, Large Language Models and AI: Challenges for Research Ethics Review and How to Respond",Evangelos Pournaras,2023,,1
ecae41113bd7233603dd34545dc24b500e80b7bb,Learning Collective Behavior in an Experimental System of Feedback-Controlled Microswimmers,R. Löffler,2023,,0
